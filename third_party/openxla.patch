diff --git a/BUILD.bazel b/BUILD.bazel
index 23644329d2..44ffa9c943 100644
--- a/BUILD.bazel
+++ b/BUILD.bazel
@@ -7,8 +7,18 @@ package(
 
 licenses(["notice"])
 
+load("@python//:defs.bzl", "compile_pip_requirements")
+load("@python_version_repo//:py_version.bzl", "REQUIREMENTS")
+
 license(
     name = "license",
     package_name = "xla",
     license_kinds = ["@rules_license//licenses/spdx:Apache-2.0"],
 )
+
+compile_pip_requirements(
+    name = "requirements",
+    requirements_in = "requirements.in",
+    requirements_txt = REQUIREMENTS,
+    generate_hashes = True
+)
diff --git a/requirements.in b/requirements.in
new file mode 100644
index 0000000000..66dd73a662
--- /dev/null
+++ b/requirements.in
@@ -0,0 +1,10 @@
+#
+# deps
+#
+numpy~=1.22.0; python_version<="3.10"
+numpy>=1.24.3; python_version=="3.11"
+numpy>=1.26.0; python_version>="3.12"
+numpy>=1.26.0; python_version=="3.13"
+scipy<1.12.0
+ml_dtypes>=0.4.0
+lit
diff --git a/requirements_lock_3_10.txt b/requirements_lock_3_10.txt
new file mode 100644
index 0000000000..20bd3ed1de
--- /dev/null
+++ b/requirements_lock_3_10.txt
@@ -0,0 +1,73 @@
+lit==17.0.6 \
+    --hash=sha256:dfa9af9b55fc4509a56be7bf2346f079d7f4a242d583b9f2e0b078fd0abae31b
+ml-dtypes==0.5.0 \
+    --hash=sha256:099e09edd54e676903b4538f3815b5ab96f5b119690514602d96bfdb67172cbe \
+    --hash=sha256:2e7534392682c3098bc7341648c650864207169c654aed83143d7a19c67ae06f \
+    --hash=sha256:3e7d3a380fe73a63c884f06136f8baa7a5249cc8e9fdec677997dd78549f8128 \
+    --hash=sha256:54415257f00eb44fbcc807454efac3356f75644f1cbfc2d4e5522a72ae1dacab \
+    --hash=sha256:5f2b59233a0dbb6a560b3137ed6125433289ccba2f8d9c3695a52423a369ed15 \
+    --hash=sha256:60275f2b51b56834e840c4809fca840565f9bf8e9a73f6d8c94f5b5935701215 \
+    --hash=sha256:76942f6aeb5c40766d5ea62386daa4148e6a54322aaf5b53eae9e7553240222f \
+    --hash=sha256:7ee9c320bb0f9ffdf9f6fa6a696ef2e005d1f66438d6f1c1457338e00a02e8cf \
+    --hash=sha256:8c32138975797e681eb175996d64356bcfa124bdbb6a70460b9768c2b35a6fa4 \
+    --hash=sha256:968fede07d1f9b926a63df97d25ac656cac1a57ebd33701734eaf704bc55d8d8 \
+    --hash=sha256:a03fc861b86cc586728e3d093ba37f0cc05e65330c3ebd7688e7bae8290f8859 \
+    --hash=sha256:a38df8df61194aeaae1ab7579075779b4ad32cd1cffd012c28be227fa7f2a70a \
+    --hash=sha256:a988bac6572630e1e9c2edd9b1277b4eefd1c86209e52b0d061b775ac33902ff \
+    --hash=sha256:ab046f2ff789b1f11b2491909682c5d089934835f9a760fafc180e47dcb676b8 \
+    --hash=sha256:afa08343069874a30812871d639f9c02b4158ace065601406a493a8511180c02 \
+    --hash=sha256:c7a9152f5876fef565516aa5dd1dccd6fc298a5891b2467973905103eb5c7856 \
+    --hash=sha256:cb5cc7b25acabd384f75bbd78892d0c724943f3e2e1986254665a1aa10982e07 \
+    --hash=sha256:d3b3db9990c3840986a0e70524e122cfa32b91139c3653df76121ba7776e015f \
+    --hash=sha256:d4b1a70a3e5219790d6b55b9507606fc4e02911d1497d16c18dd721eb7efe7d0 \
+    --hash=sha256:dc74fd9995513d33eac63d64e436240f5494ec74d522a9f0920194942fc3d2d7 \
+    --hash=sha256:e04fde367b2fe901b1d47234426fe8819909bd1dd862a5adb630f27789c20599
+numpy==1.22.4 ; python_version <= "3.10" \
+    --hash=sha256:0791fbd1e43bf74b3502133207e378901272f3c156c4df4954cad833b1380207 \
+    --hash=sha256:1ce7ab2053e36c0a71e7a13a7475bd3b1f54750b4b433adc96313e127b870887 \
+    --hash=sha256:2d487e06ecbf1dc2f18e7efce82ded4f705f4bd0cd02677ffccfb39e5c284c7e \
+    --hash=sha256:37431a77ceb9307c28382c9773da9f306435135fae6b80b62a11c53cfedd8802 \
+    --hash=sha256:3e1ffa4748168e1cc8d3cde93f006fe92b5421396221a02f2274aab6ac83b077 \
+    --hash=sha256:425b390e4619f58d8526b3dcf656dde069133ae5c240229821f01b5f44ea07af \
+    --hash=sha256:43a8ca7391b626b4c4fe20aefe79fec683279e31e7c79716863b4b25021e0e74 \
+    --hash=sha256:4c6036521f11a731ce0648f10c18ae66d7143865f19f7299943c985cdc95afb5 \
+    --hash=sha256:59d55e634968b8f77d3fd674a3cf0b96e85147cd6556ec64ade018f27e9479e1 \
+    --hash=sha256:64f56fc53a2d18b1924abd15745e30d82a5782b2cab3429aceecc6875bd5add0 \
+    --hash=sha256:7228ad13744f63575b3a972d7ee4fd61815b2879998e70930d4ccf9ec721dce0 \
+    --hash=sha256:9ce7df0abeabe7fbd8ccbf343dc0db72f68549856b863ae3dd580255d009648e \
+    --hash=sha256:a911e317e8c826ea632205e63ed8507e0dc877dcdc49744584dfc363df9ca08c \
+    --hash=sha256:b89bf9b94b3d624e7bb480344e91f68c1c6c75f026ed6755955117de00917a7c \
+    --hash=sha256:ba9ead61dfb5d971d77b6c131a9dbee62294a932bf6a356e48c75ae684e635b3 \
+    --hash=sha256:c1d937820db6e43bec43e8d016b9b3165dcb42892ea9f106c70fb13d430ffe72 \
+    --hash=sha256:cc7f00008eb7d3f2489fca6f334ec19ca63e31371be28fd5dad955b16ec285bd \
+    --hash=sha256:d4c5d5eb2ec8da0b4f50c9a843393971f31f1d60be87e0fb0917a49133d257d6 \
+    --hash=sha256:e96d7f3096a36c8754207ab89d4b3282ba7b49ea140e4973591852c77d09eb76 \
+    --hash=sha256:f0725df166cf4785c0bc4cbfb320203182b1ecd30fee6e541c8752a92df6aa32 \
+    --hash=sha256:f3eb268dbd5cfaffd9448113539e44e2dd1c5ca9ce25576f7c04a5453edc26fa \
+    --hash=sha256:fb7a980c81dd932381f8228a426df8aeb70d59bbcda2af075b627bbc50207cba
+scipy==1.11.4 \
+    --hash=sha256:00150c5eae7b610c32589dda259eacc7c4f1665aedf25d921907f4d08a951b1c \
+    --hash=sha256:028eccd22e654b3ea01ee63705681ee79933652b2d8f873e7949898dda6d11b6 \
+    --hash=sha256:1b7c3dca977f30a739e0409fb001056484661cb2541a01aba0bb0029f7b68db8 \
+    --hash=sha256:2c6ff6ef9cc27f9b3db93a6f8b38f97387e6e0591600369a297a50a8e96e835d \
+    --hash=sha256:36750b7733d960d7994888f0d148d31ea3017ac15eef664194b4ef68d36a4a97 \
+    --hash=sha256:530f9ad26440e85766509dbf78edcfe13ffd0ab7fec2560ee5c36ff74d6269ff \
+    --hash=sha256:5e347b14fe01003d3b78e196e84bd3f48ffe4c8a7b8a1afbcb8f5505cb710993 \
+    --hash=sha256:6550466fbeec7453d7465e74d4f4b19f905642c89a7525571ee91dd7adabb5a3 \
+    --hash=sha256:6df1468153a31cf55ed5ed39647279beb9cfb5d3f84369453b49e4b8502394fd \
+    --hash=sha256:6e619aba2df228a9b34718efb023966da781e89dd3d21637b27f2e54db0410d7 \
+    --hash=sha256:8fce70f39076a5aa62e92e69a7f62349f9574d8405c0a5de6ed3ef72de07f446 \
+    --hash=sha256:90a2b78e7f5733b9de748f589f09225013685f9b218275257f8a8168ededaeaa \
+    --hash=sha256:91af76a68eeae0064887a48e25c4e616fa519fa0d38602eda7e0f97d65d57937 \
+    --hash=sha256:933baf588daa8dc9a92c20a0be32f56d43faf3d1a60ab11b3f08c356430f6e56 \
+    --hash=sha256:acf8ed278cc03f5aff035e69cb511741e0418681d25fbbb86ca65429c4f4d9cd \
+    --hash=sha256:ad669df80528aeca5f557712102538f4f37e503f0c5b9541655016dd0932ca79 \
+    --hash=sha256:b030c6674b9230d37c5c60ab456e2cf12f6784596d15ce8da9365e70896effc4 \
+    --hash=sha256:b9999c008ccf00e8fbcce1236f85ade5c569d13144f77a1946bef8863e8f6eb4 \
+    --hash=sha256:bc9a714581f561af0848e6b69947fda0614915f072dfd14142ed1bfe1b806710 \
+    --hash=sha256:ce7fff2e23ab2cc81ff452a9444c215c28e6305f396b2ba88343a567feec9660 \
+    --hash=sha256:cf00bd2b1b0211888d4dc75656c0412213a8b25e80d73898083f402b50f47e41 \
+    --hash=sha256:d10e45a6c50211fe256da61a11c34927c68f277e03138777bdebedd933712fea \
+    --hash=sha256:ee410e6de8f88fd5cf6eadd73c135020bfbbbdfcd0f6162c36a7638a1ea8cc65 \
+    --hash=sha256:f313b39a7e94f296025e3cffc2c567618174c0b1dde173960cf23808f9fae4be \
+    --hash=sha256:f3cd9e7b3c2c1ec26364856f9fbe78695fe631150f94cd1c22228456404cf1ec
diff --git a/requirements_lock_3_11.txt b/requirements_lock_3_11.txt
index 5c4bb687df..9d3bcae09a 100644
--- a/requirements_lock_3_11.txt
+++ b/requirements_lock_3_11.txt
@@ -1,4 +1,28 @@
-numpy==1.24.3 \
+lit==17.0.6 \
+    --hash=sha256:dfa9af9b55fc4509a56be7bf2346f079d7f4a242d583b9f2e0b078fd0abae31b
+ml-dtypes==0.5.0 \
+    --hash=sha256:099e09edd54e676903b4538f3815b5ab96f5b119690514602d96bfdb67172cbe \
+    --hash=sha256:2e7534392682c3098bc7341648c650864207169c654aed83143d7a19c67ae06f \
+    --hash=sha256:3e7d3a380fe73a63c884f06136f8baa7a5249cc8e9fdec677997dd78549f8128 \
+    --hash=sha256:54415257f00eb44fbcc807454efac3356f75644f1cbfc2d4e5522a72ae1dacab \
+    --hash=sha256:5f2b59233a0dbb6a560b3137ed6125433289ccba2f8d9c3695a52423a369ed15 \
+    --hash=sha256:60275f2b51b56834e840c4809fca840565f9bf8e9a73f6d8c94f5b5935701215 \
+    --hash=sha256:76942f6aeb5c40766d5ea62386daa4148e6a54322aaf5b53eae9e7553240222f \
+    --hash=sha256:7ee9c320bb0f9ffdf9f6fa6a696ef2e005d1f66438d6f1c1457338e00a02e8cf \
+    --hash=sha256:8c32138975797e681eb175996d64356bcfa124bdbb6a70460b9768c2b35a6fa4 \
+    --hash=sha256:968fede07d1f9b926a63df97d25ac656cac1a57ebd33701734eaf704bc55d8d8 \
+    --hash=sha256:a03fc861b86cc586728e3d093ba37f0cc05e65330c3ebd7688e7bae8290f8859 \
+    --hash=sha256:a38df8df61194aeaae1ab7579075779b4ad32cd1cffd012c28be227fa7f2a70a \
+    --hash=sha256:a988bac6572630e1e9c2edd9b1277b4eefd1c86209e52b0d061b775ac33902ff \
+    --hash=sha256:ab046f2ff789b1f11b2491909682c5d089934835f9a760fafc180e47dcb676b8 \
+    --hash=sha256:afa08343069874a30812871d639f9c02b4158ace065601406a493a8511180c02 \
+    --hash=sha256:c7a9152f5876fef565516aa5dd1dccd6fc298a5891b2467973905103eb5c7856 \
+    --hash=sha256:cb5cc7b25acabd384f75bbd78892d0c724943f3e2e1986254665a1aa10982e07 \
+    --hash=sha256:d3b3db9990c3840986a0e70524e122cfa32b91139c3653df76121ba7776e015f \
+    --hash=sha256:d4b1a70a3e5219790d6b55b9507606fc4e02911d1497d16c18dd721eb7efe7d0 \
+    --hash=sha256:dc74fd9995513d33eac63d64e436240f5494ec74d522a9f0920194942fc3d2d7 \
+    --hash=sha256:e04fde367b2fe901b1d47234426fe8819909bd1dd862a5adb630f27789c20599
+numpy==1.24.3 ; python_version == "3.11" \
     --hash=sha256:0ec87a7084caa559c36e0a2309e4ecb1baa03b687201d0a847c8b0ed476a7187 \
     --hash=sha256:1a7d6acc2e7524c9955e5c903160aa4ea083736fde7e91276b0e5d98e6332812 \
     --hash=sha256:202de8f38fc4a45a3eea4b63e2f376e5f2dc64ef0fa692838e31a808520efaf7 \
@@ -27,23 +51,29 @@ numpy==1.24.3 \
     --hash=sha256:ea8282b9bcfe2b5e7d491d0bf7f3e2da29700cec05b49e64d6246923329f2b02 \
     --hash=sha256:ecde0f8adef7dfdec993fd54b0f78183051b6580f606111a6d789cd14c61ea0c \
     --hash=sha256:f21c442fdd2805e91799fbe044a7b999b8571bb0ab0f7850d0cb9641a687092b
-lit==17.0.6 \
-    --hash=sha256:dfa9af9b55fc4509a56be7bf2346f079d7f4a242d583b9f2e0b078fd0abae31b
-ml-dtypes==0.3.2 \
-    --hash=sha256:2c34f2ba9660b21fe1034b608308a01be82bbef2a92fb8199f24dc6bad0d5226 \
-    --hash=sha256:3a17ef2322e60858d93584e9c52a5be7dd6236b056b7fa1ec57f1bb6ba043e33 \
-    --hash=sha256:533059bc5f1764fac071ef54598db358c167c51a718f68f5bb55e3dee79d2967 \
-    --hash=sha256:6604877d567a29bfe7cc02969ae0f2425260e5335505cf5e7fefc3e5465f5655 \
-    --hash=sha256:6b35c4e8ca957c877ac35c79ffa77724ecc3702a1e4b18b08306c03feae597bb \
-    --hash=sha256:763697ab8a88d47443997a7cdf3aac7340049aed45f7521f6b0ec8a0594821fe \
-    --hash=sha256:7a4c3fcbf86fa52d0204f07cfd23947ef05b4ad743a1a988e163caa34a201e5e \
-    --hash=sha256:7afde548890a92b41c0fed3a6c525f1200a5727205f73dc21181a2726571bb53 \
-    --hash=sha256:7ba8e1fafc7fff3e643f453bffa7d082df1678a73286ce8187d3e825e776eb94 \
-    --hash=sha256:91f8783fd1f2c23fd3b9ee5ad66b785dafa58ba3cdb050c4458021fa4d1eb226 \
-    --hash=sha256:93b78f53431c93953f7850bb1b925a17f0ab5d97527e38a7e865b5b4bc5cfc18 \
-    --hash=sha256:961134ea44c7b8ca63eda902a44b58cd8bd670e21d62e255c81fba0a8e70d9b7 \
-    --hash=sha256:b89b194e9501a92d289c1ffd411380baf5daafb9818109a4f49b0a1b6dce4462 \
-    --hash=sha256:c7b3fb3d4f6b39bcd4f6c4b98f406291f0d681a895490ee29a0f95bab850d53c \
-    --hash=sha256:d1a746fe5fb9cd974a91070174258f0be129c592b93f9ce7df6cc336416c3fbd \
-    --hash=sha256:e8505946df1665db01332d885c2020b4cb9e84a8b1241eb4ba69d59591f65855 \
-    --hash=sha256:f47619d978ab1ae7dfdc4052ea97c636c6263e1f19bd1be0e42c346b98d15ff4
+scipy==1.11.4 \
+    --hash=sha256:00150c5eae7b610c32589dda259eacc7c4f1665aedf25d921907f4d08a951b1c \
+    --hash=sha256:028eccd22e654b3ea01ee63705681ee79933652b2d8f873e7949898dda6d11b6 \
+    --hash=sha256:1b7c3dca977f30a739e0409fb001056484661cb2541a01aba0bb0029f7b68db8 \
+    --hash=sha256:2c6ff6ef9cc27f9b3db93a6f8b38f97387e6e0591600369a297a50a8e96e835d \
+    --hash=sha256:36750b7733d960d7994888f0d148d31ea3017ac15eef664194b4ef68d36a4a97 \
+    --hash=sha256:530f9ad26440e85766509dbf78edcfe13ffd0ab7fec2560ee5c36ff74d6269ff \
+    --hash=sha256:5e347b14fe01003d3b78e196e84bd3f48ffe4c8a7b8a1afbcb8f5505cb710993 \
+    --hash=sha256:6550466fbeec7453d7465e74d4f4b19f905642c89a7525571ee91dd7adabb5a3 \
+    --hash=sha256:6df1468153a31cf55ed5ed39647279beb9cfb5d3f84369453b49e4b8502394fd \
+    --hash=sha256:6e619aba2df228a9b34718efb023966da781e89dd3d21637b27f2e54db0410d7 \
+    --hash=sha256:8fce70f39076a5aa62e92e69a7f62349f9574d8405c0a5de6ed3ef72de07f446 \
+    --hash=sha256:90a2b78e7f5733b9de748f589f09225013685f9b218275257f8a8168ededaeaa \
+    --hash=sha256:91af76a68eeae0064887a48e25c4e616fa519fa0d38602eda7e0f97d65d57937 \
+    --hash=sha256:933baf588daa8dc9a92c20a0be32f56d43faf3d1a60ab11b3f08c356430f6e56 \
+    --hash=sha256:acf8ed278cc03f5aff035e69cb511741e0418681d25fbbb86ca65429c4f4d9cd \
+    --hash=sha256:ad669df80528aeca5f557712102538f4f37e503f0c5b9541655016dd0932ca79 \
+    --hash=sha256:b030c6674b9230d37c5c60ab456e2cf12f6784596d15ce8da9365e70896effc4 \
+    --hash=sha256:b9999c008ccf00e8fbcce1236f85ade5c569d13144f77a1946bef8863e8f6eb4 \
+    --hash=sha256:bc9a714581f561af0848e6b69947fda0614915f072dfd14142ed1bfe1b806710 \
+    --hash=sha256:ce7fff2e23ab2cc81ff452a9444c215c28e6305f396b2ba88343a567feec9660 \
+    --hash=sha256:cf00bd2b1b0211888d4dc75656c0412213a8b25e80d73898083f402b50f47e41 \
+    --hash=sha256:d10e45a6c50211fe256da61a11c34927c68f277e03138777bdebedd933712fea \
+    --hash=sha256:ee410e6de8f88fd5cf6eadd73c135020bfbbbdfcd0f6162c36a7638a1ea8cc65 \
+    --hash=sha256:f313b39a7e94f296025e3cffc2c567618174c0b1dde173960cf23808f9fae4be \
+    --hash=sha256:f3cd9e7b3c2c1ec26364856f9fbe78695fe631150f94cd1c22228456404cf1ec
diff --git a/requirements_lock_3_12.txt b/requirements_lock_3_12.txt
new file mode 100644
index 0000000000..db485f3a7a
--- /dev/null
+++ b/requirements_lock_3_12.txt
@@ -0,0 +1,87 @@
+lit==17.0.6 \
+    --hash=sha256:dfa9af9b55fc4509a56be7bf2346f079d7f4a242d583b9f2e0b078fd0abae31b
+ml-dtypes==0.5.0 \
+    --hash=sha256:099e09edd54e676903b4538f3815b5ab96f5b119690514602d96bfdb67172cbe \
+    --hash=sha256:2e7534392682c3098bc7341648c650864207169c654aed83143d7a19c67ae06f \
+    --hash=sha256:3e7d3a380fe73a63c884f06136f8baa7a5249cc8e9fdec677997dd78549f8128 \
+    --hash=sha256:54415257f00eb44fbcc807454efac3356f75644f1cbfc2d4e5522a72ae1dacab \
+    --hash=sha256:5f2b59233a0dbb6a560b3137ed6125433289ccba2f8d9c3695a52423a369ed15 \
+    --hash=sha256:60275f2b51b56834e840c4809fca840565f9bf8e9a73f6d8c94f5b5935701215 \
+    --hash=sha256:76942f6aeb5c40766d5ea62386daa4148e6a54322aaf5b53eae9e7553240222f \
+    --hash=sha256:7ee9c320bb0f9ffdf9f6fa6a696ef2e005d1f66438d6f1c1457338e00a02e8cf \
+    --hash=sha256:8c32138975797e681eb175996d64356bcfa124bdbb6a70460b9768c2b35a6fa4 \
+    --hash=sha256:968fede07d1f9b926a63df97d25ac656cac1a57ebd33701734eaf704bc55d8d8 \
+    --hash=sha256:a03fc861b86cc586728e3d093ba37f0cc05e65330c3ebd7688e7bae8290f8859 \
+    --hash=sha256:a38df8df61194aeaae1ab7579075779b4ad32cd1cffd012c28be227fa7f2a70a \
+    --hash=sha256:a988bac6572630e1e9c2edd9b1277b4eefd1c86209e52b0d061b775ac33902ff \
+    --hash=sha256:ab046f2ff789b1f11b2491909682c5d089934835f9a760fafc180e47dcb676b8 \
+    --hash=sha256:afa08343069874a30812871d639f9c02b4158ace065601406a493a8511180c02 \
+    --hash=sha256:c7a9152f5876fef565516aa5dd1dccd6fc298a5891b2467973905103eb5c7856 \
+    --hash=sha256:cb5cc7b25acabd384f75bbd78892d0c724943f3e2e1986254665a1aa10982e07 \
+    --hash=sha256:d3b3db9990c3840986a0e70524e122cfa32b91139c3653df76121ba7776e015f \
+    --hash=sha256:d4b1a70a3e5219790d6b55b9507606fc4e02911d1497d16c18dd721eb7efe7d0 \
+    --hash=sha256:dc74fd9995513d33eac63d64e436240f5494ec74d522a9f0920194942fc3d2d7 \
+    --hash=sha256:e04fde367b2fe901b1d47234426fe8819909bd1dd862a5adb630f27789c20599
+numpy==1.26.4 ; python_version >= "3.12" \
+    --hash=sha256:03a8c78d01d9781b28a6989f6fa1bb2c4f2d51201cf99d3dd875df6fbd96b23b \
+    --hash=sha256:08beddf13648eb95f8d867350f6a018a4be2e5ad54c8d8caed89ebca558b2818 \
+    --hash=sha256:1af303d6b2210eb850fcf03064d364652b7120803a0b872f5211f5234b399f20 \
+    --hash=sha256:1dda2e7b4ec9dd512f84935c5f126c8bd8b9f2fc001e9f54af255e8c5f16b0e0 \
+    --hash=sha256:2a02aba9ed12e4ac4eb3ea9421c420301a0c6460d9830d74a9df87efa4912010 \
+    --hash=sha256:2e4ee3380d6de9c9ec04745830fd9e2eccb3e6cf790d39d7b98ffd19b0dd754a \
+    --hash=sha256:3373d5d70a5fe74a2c1bb6d2cfd9609ecf686d47a2d7b1d37a8f3b6bf6003aea \
+    --hash=sha256:47711010ad8555514b434df65f7d7b076bb8261df1ca9bb78f53d3b2db02e95c \
+    --hash=sha256:4c66707fabe114439db9068ee468c26bbdf909cac0fb58686a42a24de1760c71 \
+    --hash=sha256:50193e430acfc1346175fcbdaa28ffec49947a06918b7b92130744e81e640110 \
+    --hash=sha256:52b8b60467cd7dd1e9ed082188b4e6bb35aa5cdd01777621a1658910745b90be \
+    --hash=sha256:60dedbb91afcbfdc9bc0b1f3f402804070deed7392c23eb7a7f07fa857868e8a \
+    --hash=sha256:62b8e4b1e28009ef2846b4c7852046736bab361f7aeadeb6a5b89ebec3c7055a \
+    --hash=sha256:666dbfb6ec68962c033a450943ded891bed2d54e6755e35e5835d63f4f6931d5 \
+    --hash=sha256:675d61ffbfa78604709862923189bad94014bef562cc35cf61d3a07bba02a7ed \
+    --hash=sha256:679b0076f67ecc0138fd2ede3a8fd196dddc2ad3254069bcb9faf9a79b1cebcd \
+    --hash=sha256:7349ab0fa0c429c82442a27a9673fc802ffdb7c7775fad780226cb234965e53c \
+    --hash=sha256:7ab55401287bfec946ced39700c053796e7cc0e3acbef09993a9ad2adba6ca6e \
+    --hash=sha256:7e50d0a0cc3189f9cb0aeb3a6a6af18c16f59f004b866cd2be1c14b36134a4a0 \
+    --hash=sha256:95a7476c59002f2f6c590b9b7b998306fba6a5aa646b1e22ddfeaf8f78c3a29c \
+    --hash=sha256:96ff0b2ad353d8f990b63294c8986f1ec3cb19d749234014f4e7eb0112ceba5a \
+    --hash=sha256:9fad7dcb1aac3c7f0584a5a8133e3a43eeb2fe127f47e3632d43d677c66c102b \
+    --hash=sha256:9ff0f4f29c51e2803569d7a51c2304de5554655a60c5d776e35b4a41413830d0 \
+    --hash=sha256:a354325ee03388678242a4d7ebcd08b5c727033fcff3b2f536aea978e15ee9e6 \
+    --hash=sha256:a4abb4f9001ad2858e7ac189089c42178fcce737e4169dc61321660f1a96c7d2 \
+    --hash=sha256:ab47dbe5cc8210f55aa58e4805fe224dac469cde56b9f731a4c098b91917159a \
+    --hash=sha256:afedb719a9dcfc7eaf2287b839d8198e06dcd4cb5d276a3df279231138e83d30 \
+    --hash=sha256:b3ce300f3644fb06443ee2222c2201dd3a89ea6040541412b8fa189341847218 \
+    --hash=sha256:b97fe8060236edf3662adfc2c633f56a08ae30560c56310562cb4f95500022d5 \
+    --hash=sha256:bfe25acf8b437eb2a8b2d49d443800a5f18508cd811fea3181723922a8a82b07 \
+    --hash=sha256:cd25bcecc4974d09257ffcd1f098ee778f7834c3ad767fe5db785be9a4aa9cb2 \
+    --hash=sha256:d209d8969599b27ad20994c8e41936ee0964e6da07478d6c35016bc386b66ad4 \
+    --hash=sha256:d5241e0a80d808d70546c697135da2c613f30e28251ff8307eb72ba696945764 \
+    --hash=sha256:edd8b5fe47dab091176d21bb6de568acdd906d1887a4584a15a9a96a1dca06ef \
+    --hash=sha256:f870204a840a60da0b12273ef34f7051e98c3b5961b61b0c2c1be6dfd64fbcd3 \
+    --hash=sha256:ffa75af20b44f8dba823498024771d5ac50620e6915abac414251bd971b4529f
+scipy==1.11.4 \
+    --hash=sha256:00150c5eae7b610c32589dda259eacc7c4f1665aedf25d921907f4d08a951b1c \
+    --hash=sha256:028eccd22e654b3ea01ee63705681ee79933652b2d8f873e7949898dda6d11b6 \
+    --hash=sha256:1b7c3dca977f30a739e0409fb001056484661cb2541a01aba0bb0029f7b68db8 \
+    --hash=sha256:2c6ff6ef9cc27f9b3db93a6f8b38f97387e6e0591600369a297a50a8e96e835d \
+    --hash=sha256:36750b7733d960d7994888f0d148d31ea3017ac15eef664194b4ef68d36a4a97 \
+    --hash=sha256:530f9ad26440e85766509dbf78edcfe13ffd0ab7fec2560ee5c36ff74d6269ff \
+    --hash=sha256:5e347b14fe01003d3b78e196e84bd3f48ffe4c8a7b8a1afbcb8f5505cb710993 \
+    --hash=sha256:6550466fbeec7453d7465e74d4f4b19f905642c89a7525571ee91dd7adabb5a3 \
+    --hash=sha256:6df1468153a31cf55ed5ed39647279beb9cfb5d3f84369453b49e4b8502394fd \
+    --hash=sha256:6e619aba2df228a9b34718efb023966da781e89dd3d21637b27f2e54db0410d7 \
+    --hash=sha256:8fce70f39076a5aa62e92e69a7f62349f9574d8405c0a5de6ed3ef72de07f446 \
+    --hash=sha256:90a2b78e7f5733b9de748f589f09225013685f9b218275257f8a8168ededaeaa \
+    --hash=sha256:91af76a68eeae0064887a48e25c4e616fa519fa0d38602eda7e0f97d65d57937 \
+    --hash=sha256:933baf588daa8dc9a92c20a0be32f56d43faf3d1a60ab11b3f08c356430f6e56 \
+    --hash=sha256:acf8ed278cc03f5aff035e69cb511741e0418681d25fbbb86ca65429c4f4d9cd \
+    --hash=sha256:ad669df80528aeca5f557712102538f4f37e503f0c5b9541655016dd0932ca79 \
+    --hash=sha256:b030c6674b9230d37c5c60ab456e2cf12f6784596d15ce8da9365e70896effc4 \
+    --hash=sha256:b9999c008ccf00e8fbcce1236f85ade5c569d13144f77a1946bef8863e8f6eb4 \
+    --hash=sha256:bc9a714581f561af0848e6b69947fda0614915f072dfd14142ed1bfe1b806710 \
+    --hash=sha256:ce7fff2e23ab2cc81ff452a9444c215c28e6305f396b2ba88343a567feec9660 \
+    --hash=sha256:cf00bd2b1b0211888d4dc75656c0412213a8b25e80d73898083f402b50f47e41 \
+    --hash=sha256:d10e45a6c50211fe256da61a11c34927c68f277e03138777bdebedd933712fea \
+    --hash=sha256:ee410e6de8f88fd5cf6eadd73c135020bfbbbdfcd0f6162c36a7638a1ea8cc65 \
+    --hash=sha256:f313b39a7e94f296025e3cffc2c567618174c0b1dde173960cf23808f9fae4be \
+    --hash=sha256:f3cd9e7b3c2c1ec26364856f9fbe78695fe631150f94cd1c22228456404cf1ec
diff --git a/requirements_lock_3_9.txt b/requirements_lock_3_9.txt
new file mode 100644
index 0000000000..20bd3ed1de
--- /dev/null
+++ b/requirements_lock_3_9.txt
@@ -0,0 +1,73 @@
+lit==17.0.6 \
+    --hash=sha256:dfa9af9b55fc4509a56be7bf2346f079d7f4a242d583b9f2e0b078fd0abae31b
+ml-dtypes==0.5.0 \
+    --hash=sha256:099e09edd54e676903b4538f3815b5ab96f5b119690514602d96bfdb67172cbe \
+    --hash=sha256:2e7534392682c3098bc7341648c650864207169c654aed83143d7a19c67ae06f \
+    --hash=sha256:3e7d3a380fe73a63c884f06136f8baa7a5249cc8e9fdec677997dd78549f8128 \
+    --hash=sha256:54415257f00eb44fbcc807454efac3356f75644f1cbfc2d4e5522a72ae1dacab \
+    --hash=sha256:5f2b59233a0dbb6a560b3137ed6125433289ccba2f8d9c3695a52423a369ed15 \
+    --hash=sha256:60275f2b51b56834e840c4809fca840565f9bf8e9a73f6d8c94f5b5935701215 \
+    --hash=sha256:76942f6aeb5c40766d5ea62386daa4148e6a54322aaf5b53eae9e7553240222f \
+    --hash=sha256:7ee9c320bb0f9ffdf9f6fa6a696ef2e005d1f66438d6f1c1457338e00a02e8cf \
+    --hash=sha256:8c32138975797e681eb175996d64356bcfa124bdbb6a70460b9768c2b35a6fa4 \
+    --hash=sha256:968fede07d1f9b926a63df97d25ac656cac1a57ebd33701734eaf704bc55d8d8 \
+    --hash=sha256:a03fc861b86cc586728e3d093ba37f0cc05e65330c3ebd7688e7bae8290f8859 \
+    --hash=sha256:a38df8df61194aeaae1ab7579075779b4ad32cd1cffd012c28be227fa7f2a70a \
+    --hash=sha256:a988bac6572630e1e9c2edd9b1277b4eefd1c86209e52b0d061b775ac33902ff \
+    --hash=sha256:ab046f2ff789b1f11b2491909682c5d089934835f9a760fafc180e47dcb676b8 \
+    --hash=sha256:afa08343069874a30812871d639f9c02b4158ace065601406a493a8511180c02 \
+    --hash=sha256:c7a9152f5876fef565516aa5dd1dccd6fc298a5891b2467973905103eb5c7856 \
+    --hash=sha256:cb5cc7b25acabd384f75bbd78892d0c724943f3e2e1986254665a1aa10982e07 \
+    --hash=sha256:d3b3db9990c3840986a0e70524e122cfa32b91139c3653df76121ba7776e015f \
+    --hash=sha256:d4b1a70a3e5219790d6b55b9507606fc4e02911d1497d16c18dd721eb7efe7d0 \
+    --hash=sha256:dc74fd9995513d33eac63d64e436240f5494ec74d522a9f0920194942fc3d2d7 \
+    --hash=sha256:e04fde367b2fe901b1d47234426fe8819909bd1dd862a5adb630f27789c20599
+numpy==1.22.4 ; python_version <= "3.10" \
+    --hash=sha256:0791fbd1e43bf74b3502133207e378901272f3c156c4df4954cad833b1380207 \
+    --hash=sha256:1ce7ab2053e36c0a71e7a13a7475bd3b1f54750b4b433adc96313e127b870887 \
+    --hash=sha256:2d487e06ecbf1dc2f18e7efce82ded4f705f4bd0cd02677ffccfb39e5c284c7e \
+    --hash=sha256:37431a77ceb9307c28382c9773da9f306435135fae6b80b62a11c53cfedd8802 \
+    --hash=sha256:3e1ffa4748168e1cc8d3cde93f006fe92b5421396221a02f2274aab6ac83b077 \
+    --hash=sha256:425b390e4619f58d8526b3dcf656dde069133ae5c240229821f01b5f44ea07af \
+    --hash=sha256:43a8ca7391b626b4c4fe20aefe79fec683279e31e7c79716863b4b25021e0e74 \
+    --hash=sha256:4c6036521f11a731ce0648f10c18ae66d7143865f19f7299943c985cdc95afb5 \
+    --hash=sha256:59d55e634968b8f77d3fd674a3cf0b96e85147cd6556ec64ade018f27e9479e1 \
+    --hash=sha256:64f56fc53a2d18b1924abd15745e30d82a5782b2cab3429aceecc6875bd5add0 \
+    --hash=sha256:7228ad13744f63575b3a972d7ee4fd61815b2879998e70930d4ccf9ec721dce0 \
+    --hash=sha256:9ce7df0abeabe7fbd8ccbf343dc0db72f68549856b863ae3dd580255d009648e \
+    --hash=sha256:a911e317e8c826ea632205e63ed8507e0dc877dcdc49744584dfc363df9ca08c \
+    --hash=sha256:b89bf9b94b3d624e7bb480344e91f68c1c6c75f026ed6755955117de00917a7c \
+    --hash=sha256:ba9ead61dfb5d971d77b6c131a9dbee62294a932bf6a356e48c75ae684e635b3 \
+    --hash=sha256:c1d937820db6e43bec43e8d016b9b3165dcb42892ea9f106c70fb13d430ffe72 \
+    --hash=sha256:cc7f00008eb7d3f2489fca6f334ec19ca63e31371be28fd5dad955b16ec285bd \
+    --hash=sha256:d4c5d5eb2ec8da0b4f50c9a843393971f31f1d60be87e0fb0917a49133d257d6 \
+    --hash=sha256:e96d7f3096a36c8754207ab89d4b3282ba7b49ea140e4973591852c77d09eb76 \
+    --hash=sha256:f0725df166cf4785c0bc4cbfb320203182b1ecd30fee6e541c8752a92df6aa32 \
+    --hash=sha256:f3eb268dbd5cfaffd9448113539e44e2dd1c5ca9ce25576f7c04a5453edc26fa \
+    --hash=sha256:fb7a980c81dd932381f8228a426df8aeb70d59bbcda2af075b627bbc50207cba
+scipy==1.11.4 \
+    --hash=sha256:00150c5eae7b610c32589dda259eacc7c4f1665aedf25d921907f4d08a951b1c \
+    --hash=sha256:028eccd22e654b3ea01ee63705681ee79933652b2d8f873e7949898dda6d11b6 \
+    --hash=sha256:1b7c3dca977f30a739e0409fb001056484661cb2541a01aba0bb0029f7b68db8 \
+    --hash=sha256:2c6ff6ef9cc27f9b3db93a6f8b38f97387e6e0591600369a297a50a8e96e835d \
+    --hash=sha256:36750b7733d960d7994888f0d148d31ea3017ac15eef664194b4ef68d36a4a97 \
+    --hash=sha256:530f9ad26440e85766509dbf78edcfe13ffd0ab7fec2560ee5c36ff74d6269ff \
+    --hash=sha256:5e347b14fe01003d3b78e196e84bd3f48ffe4c8a7b8a1afbcb8f5505cb710993 \
+    --hash=sha256:6550466fbeec7453d7465e74d4f4b19f905642c89a7525571ee91dd7adabb5a3 \
+    --hash=sha256:6df1468153a31cf55ed5ed39647279beb9cfb5d3f84369453b49e4b8502394fd \
+    --hash=sha256:6e619aba2df228a9b34718efb023966da781e89dd3d21637b27f2e54db0410d7 \
+    --hash=sha256:8fce70f39076a5aa62e92e69a7f62349f9574d8405c0a5de6ed3ef72de07f446 \
+    --hash=sha256:90a2b78e7f5733b9de748f589f09225013685f9b218275257f8a8168ededaeaa \
+    --hash=sha256:91af76a68eeae0064887a48e25c4e616fa519fa0d38602eda7e0f97d65d57937 \
+    --hash=sha256:933baf588daa8dc9a92c20a0be32f56d43faf3d1a60ab11b3f08c356430f6e56 \
+    --hash=sha256:acf8ed278cc03f5aff035e69cb511741e0418681d25fbbb86ca65429c4f4d9cd \
+    --hash=sha256:ad669df80528aeca5f557712102538f4f37e503f0c5b9541655016dd0932ca79 \
+    --hash=sha256:b030c6674b9230d37c5c60ab456e2cf12f6784596d15ce8da9365e70896effc4 \
+    --hash=sha256:b9999c008ccf00e8fbcce1236f85ade5c569d13144f77a1946bef8863e8f6eb4 \
+    --hash=sha256:bc9a714581f561af0848e6b69947fda0614915f072dfd14142ed1bfe1b806710 \
+    --hash=sha256:ce7fff2e23ab2cc81ff452a9444c215c28e6305f396b2ba88343a567feec9660 \
+    --hash=sha256:cf00bd2b1b0211888d4dc75656c0412213a8b25e80d73898083f402b50f47e41 \
+    --hash=sha256:d10e45a6c50211fe256da61a11c34927c68f277e03138777bdebedd933712fea \
+    --hash=sha256:ee410e6de8f88fd5cf6eadd73c135020bfbbbdfcd0f6162c36a7638a1ea8cc65 \
+    --hash=sha256:f313b39a7e94f296025e3cffc2c567618174c0b1dde173960cf23808f9fae4be \
+    --hash=sha256:f3cd9e7b3c2c1ec26364856f9fbe78695fe631150f94cd1c22228456404cf1ec
diff --git a/third_party/tsl/third_party/eigen3/eigen.patch b/third_party/tsl/third_party/eigen3/eigen.patch
new file mode 100644
index 0000000000..8fa98599fe
--- /dev/null
+++ b/third_party/tsl/third_party/eigen3/eigen.patch
@@ -0,0 +1,13 @@
+diff --git a/Eigen/src/Core/util/Macros.h b/Eigen/src/Core/util/Macros.h
+index 4d10eecd0..93490c743 100644
+--- a/Eigen/src/Core/util/Macros.h
++++ b/Eigen/src/Core/util/Macros.h
+@@ -687,7 +687,7 @@
+ // For instance, if compiling with gcc and -std=c++17, then EIGEN_COMP_CXXVER
+ // is defined to 17.
+ #if EIGEN_CPLUSPLUS >= 202002L
+-#define EIGEN_COMP_CXXVER 20
++#define EIGEN_COMP_CXXVER 17
+ #elif EIGEN_CPLUSPLUS >= 201703L
+ #define EIGEN_COMP_CXXVER 17
+ #elif EIGEN_CPLUSPLUS >= 201402L
diff --git a/third_party/tsl/third_party/eigen3/workspace.bzl b/third_party/tsl/third_party/eigen3/workspace.bzl
index 36c73565a5..ff0aa3c47c 100644
--- a/third_party/tsl/third_party/eigen3/workspace.bzl
+++ b/third_party/tsl/third_party/eigen3/workspace.bzl
@@ -14,6 +14,7 @@ def repo():
     tf_http_archive(
         name = "eigen_archive",
         build_file = "//third_party/eigen3:eigen_archive.BUILD",
+        patch_file = ["//third_party/eigen3:eigen.patch"],
         sha256 = EIGEN_SHA256,
         strip_prefix = "eigen-{commit}".format(commit = EIGEN_COMMIT),
         urls = tf_mirror_urls("https://gitlab.com/libeigen/eigen/-/archive/{commit}/eigen-{commit}.tar.gz".format(commit = EIGEN_COMMIT)),
diff --git a/third_party/tsl/third_party/gpus/cuda_configure.bzl b/third_party/tsl/third_party/gpus/cuda_configure.bzl
index f4ed97ac4e..daa327a3eb 100644
--- a/third_party/tsl/third_party/gpus/cuda_configure.bzl
+++ b/third_party/tsl/third_party/gpus/cuda_configure.bzl
@@ -37,9 +37,9 @@ load(
     "find_vc_path",
     "setup_vc_env_vars",
 )
-load("//third_party/clang_toolchain:download_clang.bzl", "download_clang")
+load("@xla//third_party/tsl/third_party/clang_toolchain:download_clang.bzl", "download_clang")
 load(
-    "//third_party/remote_config:common.bzl",
+    "@xla//third_party/tsl/third_party/remote_config:common.bzl",
     "config_repo_label",
     "err_out",
     "execute",
diff --git a/third_party/tsl/third_party/gpus/sycl/build_defs.bzl.tpl b/third_party/tsl/third_party/gpus/sycl/build_defs.bzl.tpl
index d678f84ca8..790a9d5832 100644
--- a/third_party/tsl/third_party/gpus/sycl/build_defs.bzl.tpl
+++ b/third_party/tsl/third_party/gpus/sycl/build_defs.bzl.tpl
@@ -19,7 +19,7 @@ def sycl_build_is_configured():
     """Returns true if SYCL compiler was enabled during the configure process."""
     return %{sycl_build_is_configured}
 
-def if_sycl_is_configured(x):
+def if_sycl_is_configured(x, no_sycl = []):
     """Tests if the SYCL was enabled during the configure process.
 
     Unlike if_sycl(), this does not require that we are building with
@@ -27,7 +27,7 @@ def if_sycl_is_configured(x):
     """
     if %{sycl_is_configured}:
       return select({"//conditions:default": x})
-    return select({"//conditions:default": []})
+    return select({"//conditions:default": no_sycl})
 
 def if_sycl_build_is_configured(x, y):
     if sycl_build_is_configured():
diff --git a/third_party/tsl/third_party/grpc/c_ares.patch b/third_party/tsl/third_party/grpc/c_ares.patch
new file mode 100644
index 0000000000..752965f933
--- /dev/null
+++ b/third_party/tsl/third_party/grpc/c_ares.patch
@@ -0,0 +1,200 @@
+diff --git a/bazel/grpc_deps.bzl b/bazel/grpc_deps.bzl
+index c2f2f43820..72de3cea94 100644
+--- a/bazel/grpc_deps.bzl
++++ b/bazel/grpc_deps.bzl
+@@ -182,9 +182,9 @@ def grpc_deps():
+         http_archive(
+             name = "com_github_cares_cares",
+             build_file = "@com_github_grpc_grpc//third_party:cares/cares.BUILD",
+-            sha256 = "e8c2751ddc70fed9dc6f999acd92e232d5846f009ee1674f8aee81f19b2b915a",
+-            strip_prefix = "c-ares-e982924acee7f7313b4baa4ee5ec000c5e373c30",
+-            url = "https://github.com/c-ares/c-ares/archive/e982924acee7f7313b4baa4ee5ec000c5e373c30.tar.gz",
++            sha256 = "321700399b72ed0e037d0074c629e7741f6b2ec2dda92956abe3e9671d3e268e",
++            strip_prefix = "c-ares-1.19.1",
++            url = "https://github.com/c-ares/c-ares/releases/download/cares-1_19_1/c-ares-1.19.1.tar.gz",
+         )
+
+     if "com_google_absl" not in native.existing_rules():
+diff --git a/third_party/cares/cares.BUILD b/third_party/cares/cares.BUILD
+index 203712b182..2561b1a4bc 100644
+--- a/third_party/cares/cares.BUILD
++++ b/third_party/cares/cares.BUILD
+@@ -109,84 +109,95 @@ genrule(
+ cc_library(
+     name = "ares",
+     srcs = [
+-        "ares__close_sockets.c",
+-        "ares__get_hostent.c",
+-        "ares__read_line.c",
+-        "ares__timeval.c",
+-        "ares_cancel.c",
+-        "ares_create_query.c",
+-        "ares_data.c",
+-        "ares_destroy.c",
+-        "ares_expand_name.c",
+-        "ares_expand_string.c",
+-        "ares_fds.c",
+-        "ares_free_hostent.c",
+-        "ares_free_string.c",
+-        "ares_getenv.c",
+-        "ares_gethostbyaddr.c",
+-        "ares_gethostbyname.c",
+-        "ares_getnameinfo.c",
+-        "ares_getopt.c",
+-        "ares_getsock.c",
+-        "ares_init.c",
+-        "ares_library_init.c",
+-        "ares_llist.c",
+-        "ares_mkquery.c",
+-        "ares_nowarn.c",
+-        "ares_options.c",
+-        "ares_parse_a_reply.c",
+-        "ares_parse_aaaa_reply.c",
+-        "ares_parse_mx_reply.c",
+-        "ares_parse_naptr_reply.c",
+-        "ares_parse_ns_reply.c",
+-        "ares_parse_ptr_reply.c",
+-        "ares_parse_soa_reply.c",
+-        "ares_parse_srv_reply.c",
+-        "ares_parse_txt_reply.c",
+-        "ares_platform.c",
+-        "ares_process.c",
+-        "ares_query.c",
+-        "ares_search.c",
+-        "ares_send.c",
+-        "ares_strcasecmp.c",
+-        "ares_strdup.c",
+-        "ares_strsplit.c",
+-        "ares_strerror.c",
+-        "ares_timeout.c",
+-        "ares_version.c",
+-        "ares_writev.c",
+-        "bitncmp.c",
+-        "inet_net_pton.c",
+-        "inet_ntop.c",
+-        "windows_port.c",
++        "src/lib/ares__read_line.c",
++        "src/lib/ares__get_hostent.c",
++        "src/lib/ares__close_sockets.c",
++        "src/lib/ares__timeval.c",
++        "src/lib/ares_gethostbyaddr.c",
++        "src/lib/ares_getenv.c",
++        "src/lib/ares_free_string.c",
++        "src/lib/ares_free_hostent.c",
++        "src/lib/ares_fds.c",
++        "src/lib/ares_expand_string.c",
++        "src/lib/ares_create_query.c",
++        "src/lib/ares_cancel.c",
++        "src/lib/ares_android.c",
++        "src/lib/ares_parse_txt_reply.c",
++        "src/lib/ares_parse_srv_reply.c",
++        "src/lib/ares_parse_soa_reply.c",
++        "src/lib/ares_parse_ptr_reply.c",
++        "src/lib/ares_parse_ns_reply.c",
++        "src/lib/ares_parse_naptr_reply.c",
++        "src/lib/ares_parse_mx_reply.c",
++        "src/lib/ares_parse_caa_reply.c",
++        "src/lib/ares_options.c",
++        "src/lib/ares_nowarn.c",
++        "src/lib/ares_mkquery.c",
++        "src/lib/ares_llist.c",
++        "src/lib/ares_getsock.c",
++        "src/lib/ares_getnameinfo.c",
++        "src/lib/bitncmp.c",
++        "src/lib/ares_writev.c",
++        "src/lib/ares_version.c",
++        "src/lib/ares_timeout.c",
++        "src/lib/ares_strerror.c",
++        "src/lib/ares_strcasecmp.c",
++        "src/lib/ares_search.c",
++        "src/lib/ares_platform.c",
++        "src/lib/windows_port.c",
++        "src/lib/inet_ntop.c",
++        "src/lib/ares__sortaddrinfo.c",
++        "src/lib/ares__readaddrinfo.c",
++        "src/lib/ares_parse_uri_reply.c",
++        "src/lib/ares__parse_into_addrinfo.c",
++        "src/lib/ares_parse_a_reply.c",
++        "src/lib/ares_parse_aaaa_reply.c",
++        "src/lib/ares_library_init.c",
++        "src/lib/ares_init.c",
++        "src/lib/ares_gethostbyname.c",
++        "src/lib/ares_getaddrinfo.c",
++        "src/lib/ares_freeaddrinfo.c",
++        "src/lib/ares_expand_name.c",
++        "src/lib/ares_destroy.c",
++        "src/lib/ares_data.c",
++        "src/lib/ares__addrinfo_localhost.c",
++        "src/lib/ares__addrinfo2hostent.c",
++        "src/lib/inet_net_pton.c",
++        "src/lib/ares_strsplit.c",
++        "src/lib/ares_strdup.c",
++        "src/lib/ares_send.c",
++        "src/lib/ares_rand.c",
++        "src/lib/ares_query.c",
++        "src/lib/ares_process.c",
+     ],
+     hdrs = [
+-        "ares.h",
+         "ares_build.h",
+         "ares_config.h",
+-        "ares_data.h",
+-        "ares_dns.h",
+-        "ares_getenv.h",
+-        "ares_getopt.h",
+-        "ares_inet_net_pton.h",
+-        "ares_iphlpapi.h",
+-        "ares_ipv6.h",
+-        "ares_library_init.h",
+-        "ares_llist.h",
+-        "ares_nowarn.h",
+-        "ares_platform.h",
+-        "ares_private.h",
+-        "ares_rules.h",
+-        "ares_setup.h",
+-        "ares_strcasecmp.h",
+-        "ares_strdup.h",
+-        "ares_strsplit.h",
+-        "ares_version.h",
+-        "ares_writev.h",
+-        "bitncmp.h",
+-        "config-win32.h",
+-        "nameser.h",
+-        "setup_once.h",
++        "include/ares_version.h",
++        "include/ares.h",
++        "include/ares_rules.h",
++        "include/ares_dns.h",
++        "include/ares_nameser.h",
++        "src/tools/ares_getopt.h",
++        "src/lib/ares_strsplit.h",
++        "src/lib/ares_android.h",
++        "src/lib/ares_private.h",
++        "src/lib/ares_llist.h",
++        "src/lib/ares_platform.h",
++        "src/lib/ares_ipv6.h",
++        "src/lib/config-dos.h",
++        "src/lib/bitncmp.h",
++        "src/lib/ares_strcasecmp.h",
++        "src/lib/setup_once.h",
++        "src/lib/ares_inet_net_pton.h",
++        "src/lib/ares_data.h",
++        "src/lib/ares_getenv.h",
++        "src/lib/config-win32.h",
++        "src/lib/ares_strdup.h",
++        "src/lib/ares_iphlpapi.h",
++        "src/lib/ares_setup.h",
++        "src/lib/ares_writev.h",
++        "src/lib/ares_nowarn.h",
+     ],
+     copts = [
+         "-D_GNU_SOURCE",
+@@ -202,7 +213,7 @@ cc_library(
+         "//conditions:default": [],
+     }),
+     defines = ["CARES_STATICLIB"],
+-    includes = ["."],
++    includes = ["include", "."],
+     linkopts = select({
+         ":windows": ["-defaultlib:ws2_32.lib"],
+         "//conditions:default": [],
diff --git a/third_party/tsl/third_party/grpc/upb_platform_fix.patch b/third_party/tsl/third_party/grpc/upb_platform_fix.patch
index 6edd66067e..022c9d1557 100644
--- a/third_party/tsl/third_party/grpc/upb_platform_fix.patch
+++ b/third_party/tsl/third_party/grpc/upb_platform_fix.patch
@@ -11,3 +11,12 @@ index ad85b202..2311b2e4 100644
  )
 
  config_setting(
+@@ -24,7 +24,7 @@ exports_files([
+
+ CPPOPTS = [
+     # copybara:strip_for_google3_begin
+-    "-Werror",
++    # "-Werror",
+     "-Wno-long-long",
+     # copybara:strip_end
+ ]
diff --git a/third_party/tsl/third_party/llvm/build.patch b/third_party/tsl/third_party/llvm/build.patch
index 479e08cde8..33f585b709 100644
--- a/third_party/tsl/third_party/llvm/build.patch
+++ b/third_party/tsl/third_party/llvm/build.patch
@@ -44,3 +44,24 @@ index 7770284e5543..0b45127495dc 100644
          "//conditions:default": [
              "BLAKE3_NO_AVX2",
              "BLAKE3_NO_AVX512",
+diff --git a/mlir/lib/Bytecode/Reader/BytecodeReader.cpp b/mlir/lib/Bytecode/Reader/BytecodeReader.cpp
+index 177372c68046..40d49dc13b2f 100644
+--- a/mlir/lib/Bytecode/Reader/BytecodeReader.cpp
++++ b/mlir/lib/Bytecode/Reader/BytecodeReader.cpp
+@@ -1549,6 +1549,8 @@ private:
+   const std::shared_ptr<llvm::SourceMgr> &bufferOwnerRef;
+ };
+
++// TODO: Disable clang opt since it crashes with clang-17 compiler.
++#pragma clang optimize off
+ LogicalResult BytecodeReader::Impl::read(
+     Block *block, llvm::function_ref<bool(Operation *)> lazyOpsCallback) {
+   EncodingReader reader(buffer.getBuffer(), fileLoc);
+@@ -1628,6 +1630,7 @@ LogicalResult BytecodeReader::Impl::read(
+   // Finally, process the IR section.
+   return parseIRSection(*sectionDatas[bytecode::Section::kIR], block);
+ }
++#pragma clang optimize on
+
+ LogicalResult BytecodeReader::Impl::parseVersion(EncodingReader &reader) {
+   if (failed(reader.parseVarInt(version)))
diff --git a/third_party/tsl/third_party/llvm/spirv.patch b/third_party/tsl/third_party/llvm/spirv.patch
new file mode 100644
index 0000000000..8643e91813
--- /dev/null
+++ b/third_party/tsl/third_party/llvm/spirv.patch
@@ -0,0 +1,76 @@
+diff --git a/llvm/lib/Passes/PassBuilderPipelines.cpp b/llvm/lib/Passes/PassBuilderPipelines.cpp
+index 78e0e6353056..4f9f51164bfe 100644
+--- a/llvm/lib/Passes/PassBuilderPipelines.cpp
++++ b/llvm/lib/Passes/PassBuilderPipelines.cpp
+@@ -183,6 +183,10 @@ static cl::opt<bool> EnableGlobalAnalyses(
+     "enable-global-analyses", cl::init(true), cl::Hidden,
+     cl::desc("Enable inter-procedural analyses"));
+ 
++static cl::opt<bool>
++    SYCLOptimizationMode("sycl-opt", cl::init(false), cl::Hidden,
++                         cl::desc("Enable SYCL optimization mode."));
++
+ static cl::opt<bool>
+     RunPartialInlining("enable-partial-inlining", cl::init(false), cl::Hidden,
+                        cl::desc("Run Partial inlinining pass"));
+@@ -406,6 +410,7 @@ PassBuilder::buildO1FunctionSimplificationPipeline(OptimizationLevel Level,
+   // Form canonically associated expression trees, and simplify the trees using
+   // basic mathematical properties. For example, this will form (nearly)
+   // minimal multiplication trees.
++  if (!SYCLOptimizationMode) {
+   FPM.addPass(ReassociatePass());
+ 
+   // Add the primary loop simplification pipeline.
+@@ -477,7 +482,7 @@ PassBuilder::buildO1FunctionSimplificationPipeline(OptimizationLevel Level,
+   FPM.addPass(createFunctionToLoopPassAdaptor(std::move(LPM2),
+                                               /*UseMemorySSA=*/false,
+                                               /*UseBlockFrequencyInfo=*/false));
+-
++  }
+   // Delete small array after loop unroll.
+   FPM.addPass(SROAPass(SROAOptions::ModifyCFG));
+ 
+@@ -580,6 +585,7 @@ PassBuilder::buildFunctionSimplificationPipeline(OptimizationLevel Level,
+   // Form canonically associated expression trees, and simplify the trees using
+   // basic mathematical properties. For example, this will form (nearly)
+   // minimal multiplication trees.
++  if (!SYCLOptimizationMode) {
+   FPM.addPass(ReassociatePass());
+ 
+   if (EnableConstraintElimination)
+@@ -657,7 +663,7 @@ PassBuilder::buildFunctionSimplificationPipeline(OptimizationLevel Level,
+   FPM.addPass(createFunctionToLoopPassAdaptor(std::move(LPM2),
+                                               /*UseMemorySSA=*/false,
+                                               /*UseBlockFrequencyInfo=*/false));
+-
++  }
+   // Delete small array after loop unroll.
+   FPM.addPass(SROAPass(SROAOptions::ModifyCFG));
+ 
+@@ -715,6 +721,9 @@ PassBuilder::buildFunctionSimplificationPipeline(OptimizationLevel Level,
+ 
+   invokeScalarOptimizerLateEPCallbacks(FPM, Level);
+ 
++  if (SYCLOptimizationMode)
++    FPM.addPass(SimplifyCFGPass());
++  else
+   FPM.addPass(SimplifyCFGPass(SimplifyCFGOptions()
+                                   .convertSwitchRangeToICmp(true)
+                                   .hoistCommonInsts(true)
+@@ -1385,6 +1394,7 @@ PassBuilder::buildModuleOptimizationPipeline(OptimizationLevel Level,
+ 
+   invokeVectorizerStartEPCallbacks(OptimizePM, Level);
+ 
++  if (!SYCLOptimizationMode) {
+   LoopPassManager LPM;
+   // First rotate loops that may have been un-rotated by prior passes.
+   // Disable header duplication at -Oz.
+@@ -1408,7 +1418,7 @@ PassBuilder::buildModuleOptimizationPipeline(OptimizationLevel Level,
+   OptimizePM.addPass(InjectTLIMappings());
+ 
+   addVectorPasses(Level, OptimizePM, /* IsFullLTO */ false);
+-
++  }
+   // LoopSink pass sinks instructions hoisted by LICM, which serves as a
+   // canonicalization pass that enables other optimizations. As a result,
+   // LoopSink pass needs to be a very late IR pass to avoid undoing LICM
diff --git a/third_party/tsl/third_party/llvm/workspace.bzl b/third_party/tsl/third_party/llvm/workspace.bzl
index fc1cf70ed1..a08a682610 100644
--- a/third_party/tsl/third_party/llvm/workspace.bzl
+++ b/third_party/tsl/third_party/llvm/workspace.bzl
@@ -17,6 +17,7 @@ def repo(name):
         ],
         build_file = "//third_party/llvm:llvm.BUILD",
         patch_file = [
+            "//third_party/llvm:spirv.patch",
             "//third_party/llvm:generated.patch",  # Autogenerated, don't remove.
             "//third_party/llvm:build.patch",
             "//third_party/llvm:externc.patch",
diff --git a/third_party/tsl/workspace2.bzl b/third_party/tsl/workspace2.bzl
index 4bd2fd2e1c..d3aa1f6cf7 100644
--- a/third_party/tsl/workspace2.bzl
+++ b/third_party/tsl/workspace2.bzl
@@ -19,7 +19,6 @@ load("//third_party/gemmlowp:workspace.bzl", gemmlowp = "repo")
 load("//third_party/git:git_configure.bzl", "git_configure")
 load("//third_party/gpus:cuda_configure.bzl", "cuda_configure")
 load("//third_party/gpus:rocm_configure.bzl", "rocm_configure")
-load("//third_party/gpus:sycl_configure.bzl", "sycl_configure")
 load("//third_party/hwloc:workspace.bzl", hwloc = "repo")
 load("//third_party/implib_so:workspace.bzl", implib_so = "repo")
 load("//third_party/llvm:setup.bzl", "llvm_setup")
@@ -76,7 +75,6 @@ def _tf_toolchains():
     syslibs_configure(name = "local_config_syslibs")
     python_configure(name = "local_config_python")
     rocm_configure(name = "local_config_rocm")
-    sycl_configure(name = "local_config_sycl")
     remote_execution_configure(name = "local_config_remote_execution")
 
     # For windows bazel build
@@ -342,6 +340,7 @@ def _tf_repositories():
         patch_file = [
             "//third_party/grpc:generate_cc_env_fix.patch",
             "//third_party/grpc:register_go_toolchain.patch",
+            "//third_party/grpc:c_ares.patch",
         ],
         system_link_files = {
             "//third_party/systemlibs:BUILD": "bazel/BUILD",
diff --git a/xla/backends/profiler/plugin/BUILD b/xla/backends/profiler/plugin/BUILD
index 40c42f93ab..cdef653e21 100644
--- a/xla/backends/profiler/plugin/BUILD
+++ b/xla/backends/profiler/plugin/BUILD
@@ -61,6 +61,10 @@ cc_library(
     deps = [
         ":profiler_c_api_hdrs",
         ":profiler_error",
+        "//xla/backends/profiler/cpu:host_tracer",
+        "//xla/backends/profiler/cpu:metadata_collector",
+        "//xla/backends/profiler/cpu:python_tracer",
+        "@intel_extension_for_openxla//xla/profiler:sycl_device_tracer",
         "@tsl//tsl/platform:logging",
         "@tsl//tsl/profiler/lib:profiler_collection",
         "@tsl//tsl/profiler/lib:profiler_factory",
diff --git a/xla/pjrt/c/pjrt_c_api_gpu_internal.cc b/xla/pjrt/c/pjrt_c_api_gpu_internal.cc
index 93e48d9e80..91dc46387c 100644
--- a/xla/pjrt/c/pjrt_c_api_gpu_internal.cc
+++ b/xla/pjrt/c/pjrt_c_api_gpu_internal.cc
@@ -58,6 +58,8 @@ namespace gpu_plugin {
 
 #if TENSORFLOW_USE_ROCM
 #define PJRT_GPU_PLUGIN_PLATFORM_NAME "ROCM"
+#elif TENSORFLOW_USE_SYCL
+#define PJRT_GPU_PLUGIN_PLATFORM_NAME "SYCL"
 #else
 #define PJRT_GPU_PLUGIN_PLATFORM_NAME "CUDA"
 #endif
diff --git a/xla/pjrt/gpu/BUILD b/xla/pjrt/gpu/BUILD
index eeebebdf8f..d5d85b1535 100644
--- a/xla/pjrt/gpu/BUILD
+++ b/xla/pjrt/gpu/BUILD
@@ -41,6 +41,7 @@ cc_library(
     defines = if_cuda(["GOOGLE_CUDA=1"]) + if_rocm(["TENSORFLOW_USE_ROCM=1"]),
     visibility = internal_visibility(["//xla/pjrt:friends"]),
     deps = [
+        "@intel_extension_for_openxla//xla/stream_executor/sycl:hw_info",
         ":gpu_helpers",
         ":gpu_metrics",
         ":gpu_topology",
@@ -85,6 +86,7 @@ cc_library(
         "//xla/stream_executor:device_memory",
         "//xla/stream_executor:device_memory_allocator",
         "//xla/stream_executor:platform",
+        "//xla/stream_executor/gpu:gpu_malloc_allocator",
         "//xla/stream_executor/integrations:device_mem_allocator",
         "//xla/stream_executor/integrations:tf_allocator_adapter",
         "//xla/tsl/framework:allocator",
diff --git a/xla/pjrt/gpu/gpu_helpers.cc b/xla/pjrt/gpu/gpu_helpers.cc
index e33521addb..aa82a755a5 100644
--- a/xla/pjrt/gpu/gpu_helpers.cc
+++ b/xla/pjrt/gpu/gpu_helpers.cc
@@ -38,9 +38,10 @@ namespace xla {
 absl::StatusOr<LocalClient*> GetGpuXlaClient(
     const std::optional<std::string>& platform_name,
     const std::optional<std::set<int>>& allowed_devices) {
+  // SYCL: hardcode to SYCL
   TF_ASSIGN_OR_RETURN(
       se::Platform * platform,
-      PlatformUtil::GetPlatform(platform_name ? *platform_name : "gpu"));
+      PlatformUtil::GetPlatform(platform_name ? *platform_name : "SYCL"));
   if (platform->VisibleDeviceCount() <= 0) {
     return FailedPrecondition("No visible GPU devices.");
   }
diff --git a/xla/pjrt/gpu/se_gpu_pjrt_client.cc b/xla/pjrt/gpu/se_gpu_pjrt_client.cc
index 6e40add204..c1a51fe105 100644
--- a/xla/pjrt/gpu/se_gpu_pjrt_client.cc
+++ b/xla/pjrt/gpu/se_gpu_pjrt_client.cc
@@ -89,6 +89,9 @@ limitations under the License.
 #include "tsl/platform/threadpool.h"
 #include "tsl/profiler/lib/connected_traceme.h"
 #include "tsl/profiler/lib/traceme.h"
+#include "xla/stream_executor/gpu/gpu_malloc_allocator.h"
+#include "xla/stream_executor/sycl/hw_info.h"
+#include "xla/tsl/util/env_var.h"
 
 #if defined(GOOGLE_CUDA) || defined(TENSORFLOW_USE_ROCM)
 #include "xla/pjrt/compile_options.pb.h"
@@ -533,7 +536,7 @@ absl::string_view StreamExecutorGpuClient::platform_version() const {
 #elif GOOGLE_CUDA && defined(CUDART_VERSION)  // cuda
   return "cuda " STRINGIFY(CUDART_VERSION);
 #else
-  return "<unknown>";
+  return "sycl";
 #endif  // TENSORFLOW_USE_ROCM && defined(TF_ROCM_VERSION)
 }
 
@@ -793,6 +796,27 @@ StreamExecutorGpuClient::Load(std::unique_ptr<PjRtExecutable> executable) {
 
 namespace {
 
+absl::StatusOr<std::vector<se::MultiDeviceAdapter::AllocatorInfo>>
+CreateDefaultGPUAllocator(
+    se::Platform* platform,
+    const std::map<int, std::unique_ptr<LocalDeviceState>>&
+        addressable_devices) {
+  CHECK_GT(addressable_devices.size(), 0);
+  std::vector<se::MultiDeviceAdapter::AllocatorInfo> allocators;
+
+  for (auto& ordinal_and_device : addressable_devices) {
+    se::StreamExecutor* executor = ordinal_and_device.second->executor();
+    int device_ordinal = executor->device_ordinal();
+
+    auto allocator = std::make_unique<se::GpuMallocAllocator>(
+        executor, tsl::PlatformDeviceId(device_ordinal));
+    allocators.emplace_back(std::move(allocator),
+                            ordinal_and_device.second->compute_stream(),
+                            /*memory_space=*/0);
+  }
+  return allocators;
+}
+
 #if defined(GOOGLE_CUDA) && CUDA_VERSION >= 11020
 
 absl::StatusOr<std::vector<se::MultiDeviceAdapter::AllocatorInfo>>
@@ -908,17 +932,26 @@ GetStreamExecutorGpuDeviceAllocator(
       }
       break;
     }
-
-    case GpuAllocatorConfig::Kind::kPlatform:
-      LOG(INFO) << "Using platform allocator.";
+    case GpuAllocatorConfig::Kind::kPlatform: {
       if (allocator_config.collective_memory_size != 0) {
         LOG(WARNING)
             << "collective_memory_size is non-zero, but allocator kind is set "
                "to \"platform\". Collective memory will not be allocated.";
       }
+      auto allocators_or =
+          CreateDefaultGPUAllocator(platform, addressable_devices);
+      if (allocators_or.ok()) {
+        LOG(INFO) << "Using platform allocator.";
+        allocators = std::move(allocators_or.value());
+        break;
+      }
+      LOG(ERROR) << "Failed to initialize platform allocator: "
+                 << allocators_or.status() << "; Please choose other kind allocator.";
+
       // Returning null will cause the client to use the default backend
       // allocator.
       return nullptr;
+    }
   }
 
   // Add any additional allocators for alternate memory spaces.
diff --git a/xla/python/BUILD b/xla/python/BUILD
index d23f488a66..bc22f5ad40 100644
--- a/xla/python/BUILD
+++ b/xla/python/BUILD
@@ -1,6 +1,11 @@
 load("@bazel_skylib//rules:common_settings.bzl", "bool_flag")
 load("@local_config_cuda//cuda:build_defs.bzl", "if_cuda")
 load("@local_config_rocm//rocm:build_defs.bzl", "if_rocm")
+load("@local_config_sycl//sycl:build_defs.bzl", "if_sycl_is_configured")
+load(
+    "//xla/stream_executor:build_defs.bzl",
+    "if_gpu_is_configured",
+)
 load("@tsl//tsl/platform:build_config.bzl", "tf_proto_library")
 load("@tsl//tsl/platform:rules_cc.bzl", "cc_library")
 load(
@@ -420,7 +425,7 @@ cc_library(
         "//xla/stream_executor/cuda:cuda_driver",
     ]) + if_rocm([
         "@local_config_rocm//rocm:rocm_headers",
-    ]) + if_cuda_or_rocm([":py_client_gpu"]),  # TODO(b/337876408): remove after migration to plugin
+    ]) + if_gpu_is_configured([":py_client_gpu"]),  # TODO(b/337876408): remove after migration to plugin
 )
 
 cc_library(
@@ -462,11 +467,11 @@ cc_library(
     name = "py_client_gpu",
     srcs = if_google(
         ["py_client_gpu.cc"],
-        if_cuda_or_rocm(["py_client_gpu.cc"]),
+        if_gpu_is_configured(["py_client_gpu.cc"]),
     ),
     hdrs = if_google(
         ["py_client_gpu.h"],
-        if_cuda_or_rocm(["py_client_gpu.h"]),
+        if_gpu_is_configured(["py_client_gpu.h"]),
     ),
     compatible_with = [],
     copts = [
@@ -491,7 +496,10 @@ cc_library(
     ] + if_rocm(
         ["@local_config_rocm//rocm:rocm_headers"],
         ["@local_config_cuda//cuda:cuda_headers"],
-    ),
+    ) + if_sycl_is_configured([
+        "@intel_extension_for_openxla//xla/stream_executor:sycl_platform",
+        "@local_config_sycl//sycl:sycl_headers",
+    ]),
 )
 
 cc_library(
diff --git a/xla/python/py_client.cc b/xla/python/py_client.cc
index 10474e6af4..0f1deffaec 100644
--- a/xla/python/py_client.cc
+++ b/xla/python/py_client.cc
@@ -96,9 +96,9 @@ limitations under the License.
 #include "tsl/platform/status.h"
 #include "tsl/platform/statusor.h"
 
-#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM
+#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM || TENSORFLOW_USE_SYCL
 #include "xla/python/py_client_gpu.h"
-#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM
+#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM || TENSORFLOW_USE_SYCL
 
 namespace xla {
 
@@ -627,7 +627,7 @@ PyClient::GetEmitPythonCallbackDescriptor(nb::callable callable,
 XLA_CPU_REGISTER_CUSTOM_CALL_TARGET_WITH_SYM("xla_python_cpu_callback",
                                              &XlaPythonCpuCallback);
 
-#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM
+#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM || TENSORFLOW_USE_SYCL
 XLA_REGISTER_CUSTOM_CALL_TARGET_WITH_SYM(
     "xla_python_gpu_callback", &XlaPythonGpuCallback,
     absl::AsciiStrToUpper(PlatformUtil::CanonicalPlatformName("gpu").value()));
diff --git a/xla/python/py_client_gpu.cc b/xla/python/py_client_gpu.cc
index 8804920878..33f43feeaa 100644
--- a/xla/python/py_client_gpu.cc
+++ b/xla/python/py_client_gpu.cc
@@ -12,6 +12,7 @@ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 See the License for the specific language governing permissions and
 limitations under the License.
 ==============================================================================*/
+#include "xla/python/py_client_gpu.h"
 
 #include <vector>
 
@@ -20,6 +21,8 @@ limitations under the License.
 #include "tsl/platform/errors.h"
 #if TENSORFLOW_USE_ROCM
 #include "rocm/include/hip/hip_runtime.h"
+#elif TENSORFLOW_USE_SYCL
+#include "xla/stream_executor/sycl/sycl_gpu_runtime.h"
 #else
 #include "third_party/gpus/cuda/include/cuda.h"
 #include "third_party/gpus/cuda/include/cuda_runtime_api.h"
@@ -38,6 +41,13 @@ limitations under the License.
 #define gpuStreamSynchronize hipStreamSynchronize
 #define gpuMemcpyDeviceToHost hipMemcpyDeviceToHost
 #define gpuMemcpyHostToDevice hipMemcpyHostToDevice
+#elif TENSORFLOW_USE_SYCL
+#define gpuSuccess SYCL_SUCCESS
+#define gpuStreamHandle ::sycl::queue*
+#define gpuMemcpyAsync SYCLMemcpyAsync
+#define gpuStreamSynchronize SYCLStreamSynchronize
+#define gpuMemcpyDeviceToHost SYCLMemcpyDtoHAsync
+#define gpuMemcpyHostToDevice SYCLMemcpyHtoDAsync
 #else
 #define gpuSuccess cudaSuccess
 #define gpuStreamHandle CUstream
diff --git a/xla/python/py_client_gpu.h b/xla/python/py_client_gpu.h
index d7675e1b6a..17da528bec 100644
--- a/xla/python/py_client_gpu.h
+++ b/xla/python/py_client_gpu.h
@@ -18,6 +18,8 @@ limitations under the License.
 
 #if TENSORFLOW_USE_ROCM
 #include "rocm/include/hip/hip_runtime.h"
+#elif TENSORFLOW_USE_SYCL
+#include "xla/stream_executor/sycl/sycl_gpu_runtime.h"
 #else
 #include "third_party/gpus/cuda/include/cuda.h"
 #endif
@@ -25,8 +27,10 @@ limitations under the License.
 
 #if TENSORFLOW_USE_ROCM
 #define gpuStreamHandle hipStream_t
-#else
+#elif GOOGLE_CUDA
 #define gpuStreamHandle CUstream
+#else
+#define gpuStreamHandle ::sycl::queue*
 #endif
 
 namespace xla {
diff --git a/xla/service/BUILD b/xla/service/BUILD
index a0b65b2012..9fc6094208 100644
--- a/xla/service/BUILD
+++ b/xla/service/BUILD
@@ -8,6 +8,10 @@ load(
     "if_rocm",
     "if_rocm_is_configured",
 )
+load(
+    "@local_config_sycl//sycl:build_defs.bzl",
+    "if_sycl_is_configured",
+)
 load(
     "@tsl//tsl/platform:build_config.bzl",
     "tf_proto_library",
@@ -1473,6 +1477,9 @@ cc_library(
     ]) + if_rocm_is_configured([
         "//xla/service/gpu:amdgpu_compiler",
         "//xla/stream_executor/rocm:stream_executor_rocm",
+    ]) + if_sycl_is_configured([
+        "//xla/service/gpu:spir_compiler",
+        "@intel_extension_for_openxla//xla/stream_executor/sycl:stream_executor_sycl",
     ]),
 )
 
@@ -4131,6 +4138,7 @@ cc_library(
         "//xla/stream_executor/cuda:cuda_platform_id",
         "//xla/stream_executor/host:host_platform_id",
         "//xla/stream_executor/rocm:rocm_platform_id",
+        "@intel_extension_for_openxla//xla/stream_executor/sycl:sycl_platform_id",
         "@com_google_absl//absl/container:flat_hash_map",
         "@com_google_absl//absl/memory",
         "@com_google_absl//absl/status",
@@ -7572,6 +7580,19 @@ cc_library(
     ],
 )
 
+cc_library(
+    name = "scatter_promotion",
+    srcs = ["scatter_promotion.cc"],
+    hdrs = ["scatter_promotion.h"],
+    deps = [
+        ":hlo_creation_utils",
+        ":hlo_pass",
+        ":op_expander_pass",
+        "//xla:xla_data_proto_cc",
+        "//xla/hlo/ir:hlo",
+    ],
+)
+
 cc_library(
     name = "scatter_simplifier",
     srcs = ["scatter_simplifier.cc"],
@@ -7653,8 +7674,10 @@ cc_library(
     deps = [
         ":hlo_creation_utils",
         ":hlo_pass",
-        "//xla/service/cpu:onednn_matmul_rewriter",
-    ],
+    ] + if_sycl_is_configured(
+        [],
+        ["//xla/service/cpu:onednn_matmul_rewriter"]
+    ),
 )
 
 cc_library(
diff --git a/xla/service/algebraic_simplifier.h b/xla/service/algebraic_simplifier.h
index 18e8b56f6d..faa8fce89d 100644
--- a/xla/service/algebraic_simplifier.h
+++ b/xla/service/algebraic_simplifier.h
@@ -27,6 +27,7 @@ limitations under the License.
 #include <vector>
 
 #include "absl/container/inlined_vector.h"
+#include "xla/tsl/util/env_var.h"
 #include "xla/hlo/ir/dfs_hlo_visitor_with_default.h"
 #include "xla/hlo/ir/hlo_instruction.h"
 #include "xla/hlo/ir/hlo_instructions.h"
@@ -452,7 +453,9 @@ class AlgebraicSimplifierVisitor : public DfsHloRewriteVisitor {
   virtual bool IsValidLayout(const Shape& shape) { return true; }
   // Allow backend targets to determine whether a layout is inefficient.
   virtual bool ShouldStrengthReduceDotToReduce(const HloInstruction* hlo) {
-    return true;
+    bool llm_flag = false;
+    tsl::ReadBoolFromEnvVar("LLM", false, &llm_flag);
+    return !llm_flag;
   }
 
  protected:
diff --git a/xla/service/computation_placer.cc b/xla/service/computation_placer.cc
index 58aedeb631..0aa11b7c43 100644
--- a/xla/service/computation_placer.cc
+++ b/xla/service/computation_placer.cc
@@ -31,6 +31,7 @@ limitations under the License.
 #include "xla/stream_executor/cuda/cuda_platform_id.h"
 #include "xla/stream_executor/host/host_platform_id.h"
 #include "xla/stream_executor/rocm/rocm_platform_id.h"
+#include "xla/stream_executor/sycl/sycl_platform_id.h"
 #include "xla/types.h"
 #include "xla/util.h"
 #include "tsl/platform/errors.h"
@@ -163,6 +164,11 @@ absl::StatusOr<DeviceAssignment> ComputationPlacer::AssignDevices(
   absl::MutexLock lock(&ComputationPlacer::platform_computation_placer_mutex_);
   auto* computation_placers = GetPlatformComputationPlacers();
   if (computation_placers->find(platform_id) != computation_placers->end()) {
+    // FIXME(intel): Temporarily skip the registry to avoid linking warning.
+    // Will reopen this check once refine oneDNN custom call code.
+#ifdef TENSORFLOW_USE_SYCL
+    return;
+#endif
     // TODO(b/282059652): Consider logging the platform name using
     // PlatformManager::PlatformWithId(). No doing that for now to avoid
     // introducing unwanted dependency.
@@ -215,6 +221,8 @@ static bool InitModule() {
       stream_executor::cuda::kCudaPlatformId, &CreateComputationPlacer);
   xla::ComputationPlacer::RegisterComputationPlacer(
       stream_executor::rocm::kROCmPlatformId, &CreateComputationPlacer);
+  xla::ComputationPlacer::RegisterComputationPlacer(
+      stream_executor::sycl::kSyclPlatformId, &CreateComputationPlacer);
   return true;
 }
 static bool module_initialized = InitModule();
diff --git a/xla/service/dump.cc b/xla/service/dump.cc
index b23042a9ac..f50a765978 100644
--- a/xla/service/dump.cc
+++ b/xla/service/dump.cc
@@ -633,6 +633,8 @@ void DumpToFileInDirOrStdout(const HloModule& module, string_view file_prefix,
   if (opts.dumping_to_stdout()) return op->dump();
 
   mlir::OpPrintingFlags print_flags = mlir::OpPrintingFlags();
+  // Avoid printing large constant weight.
+  print_flags.elideLargeElementsAttrs(8);
   // Enable debug info so that it is easier to see the corresponding HLO node.
   if (file_prefix == "lmhlo") {
     print_flags.enableDebugInfo(/*enable=*/true,
diff --git a/xla/service/elemental_ir_emitter.cc b/xla/service/elemental_ir_emitter.cc
index 614e709c35..632442ee8c 100644
--- a/xla/service/elemental_ir_emitter.cc
+++ b/xla/service/elemental_ir_emitter.cc
@@ -211,6 +211,28 @@ absl::StatusOr<llvm::Value*> EmitReducePrecisionIR(
   return result;
 }
 
+absl::StatusOr<llvm::Value*> DefaultEmitF32ToBF16Impl(llvm::Value* f32_value,
+                                                      llvm::IRBuilder<>* b) {
+  TF_ASSIGN_OR_RETURN(
+      auto reduced_precision,
+      EmitReducePrecisionIR(
+          /*src_ty=*/F32, f32_value,
+          /*dest_exponent_bits=*/primitive_util::ExponentWidth(BF16),
+          /*dest_mantissa_bits=*/primitive_util::SignificandWidth(BF16) - 1,
+          /*quiet_nans=*/true, b));
+  auto as_int32 = b->CreateBitCast(reduced_precision, b->getInt32Ty());
+  auto shifted = b->CreateLShr(as_int32, 16);
+  auto truncated = b->CreateTrunc(shifted, b->getInt16Ty());
+  return b->CreateBitCast(truncated, b->getInt16Ty());
+}
+
+llvm::Value* EmitBF16ToF32(llvm::Value* bf16_value, llvm::IRBuilder<>* b) {
+  auto as_int16 = b->CreateBitCast(bf16_value, b->getInt16Ty());
+  auto as_int32 = b->CreateZExt(as_int16, b->getInt32Ty());
+  auto shifted = b->CreateShl(as_int32, 16);
+  return b->CreateBitCast(shifted, b->getFloatTy());
+}
+
 absl::StatusOr<llvm::Value*> EmitF16ToF8e5m2(llvm::Value* f16_value,
                                              llvm::IRBuilder<>* b) {
   TF_ASSIGN_OR_RETURN(
@@ -589,6 +611,10 @@ absl::StatusOr<llvm::Value*> ElementalIrEmitter::EmitIntegerUnaryOp(
                        primitive_util::IsSignedIntegralType(from_type));
       }
       if (primitive_util::IsFloatingPointType(to_type)) {
+        if (to_type == BF16) {
+          return EmitF32ToBF16(EmitIntegralToFloating(operand_value, from_type,
+                                                      F32, module_, b_));
+        }
         if (to_type == F8E5M2) {
           return EmitF16ToF8e5m2(
               EmitIntegralToFloating(operand_value, from_type, F16, module_,
@@ -718,8 +744,7 @@ absl::StatusOr<llvm::Value*> ElementalIrEmitter::EmitFloatUnaryOp(
       }
       if (from_type == BF16) {
         TF_RET_CHECK(to_type != BF16);
-        // The code below expects the source type to be F32.
-        operand_value = b_->CreateFPExt(operand_value, b_->getFloatTy());
+        operand_value = EmitBF16ToF32(operand_value, b_);
         from_type = F32;
         if (from_type == to_type) {
           return operand_value;
@@ -774,11 +799,13 @@ absl::StatusOr<llvm::Value*> ElementalIrEmitter::EmitFloatUnaryOp(
             nullptr);
       }
       if (to_type == BF16) {
-        // F16 to BF16 has to go through an intermediate F32.
-        if (from_type == F16) {
-          operand_value = b_->CreateFPExt(operand_value, b_->getFloatTy());
+        // Cast to F32 first. Other floating point formats are not supported by
+        // EmitReducePrecisionIR.
+        if (from_type != F32) {
+          operand_value = b_->CreateFPCast(
+              operand_value, llvm_ir::PrimitiveTypeToIrType(F32, module_));
         }
-        return FPCast(operand_value, b_->getBFloatTy());
+        return EmitF32ToBF16(operand_value);
       }
       if (to_type == F8E5M2) {
         // Cast to F16 first. Casts to F8E5M2 must be from F16.
@@ -1332,7 +1359,10 @@ absl::StatusOr<llvm::Value*> ElementalIrEmitter::EmitFloatBinaryOp(
     // matches C++'s semantics.
     case HloOpcode::kCompare: {
       PrimitiveType operand_type = op->operand(0)->shape().element_type();
-      if (operand_type == F8E5M2) {
+      if (operand_type == BF16) {
+        lhs_value = EmitBF16ToF32(lhs_value, b_);
+        rhs_value = EmitBF16ToF32(rhs_value, b_);
+      } else if (operand_type == F8E5M2) {
         lhs_value = EmitF8e5m2ToF16(lhs_value, b_);
         rhs_value = EmitF8e5m2ToF16(rhs_value, b_);
       } else if (operand_type == F8E4M3FN) {
@@ -3018,8 +3048,8 @@ absl::StatusOr<llvm::Value*> ElementalIrEmitter::EmitElementalDot(
   TF_ASSIGN_OR_RETURN(llvm::Value * rhs_value, rhs_generator(rhs_index));
 
   if (primitive_type == BF16) {
-    lhs_value = b_->CreateFPExt(lhs_value, b_->getFloatTy());
-    rhs_value = b_->CreateFPExt(rhs_value, b_->getFloatTy());
+    lhs_value = EmitBF16ToF32(lhs_value, b_);
+    rhs_value = EmitBF16ToF32(rhs_value, b_);
   }
 
   llvm::Value* next_accumulator =
@@ -3031,7 +3061,7 @@ absl::StatusOr<llvm::Value*> ElementalIrEmitter::EmitElementalDot(
   llvm::Value* result =
       Load(accumulator_alloca->getAllocatedType(), accumulator_alloca);
 
-  return primitive_type == BF16 ? FPTrunc(result, b_->getBFloatTy()) : result;
+  return primitive_type == BF16 ? EmitF32ToBF16(result) : result;
 }
 
 llvm_ir::ElementGenerator ElementalIrEmitter::MakeElementGenerator(
@@ -3184,7 +3214,9 @@ llvm_ir::ElementGenerator ElementalIrEmitter::MakeElementGenerator(
               primitive_util::IsFloatingPointType(component_element_type))
               << component_element_type;
           llvm::Type* float_ir_type;
-          if (component_element_type == F8E4M3FNUZ) {
+          if (component_element_type == BF16) {
+            float_ir_type = llvm_ir::PrimitiveTypeToIrType(F32, module_);
+          } else if (component_element_type == F8E4M3FNUZ) {
             float_ir_type = llvm_ir::PrimitiveTypeToIrType(F16, module_);
           } else if (component_element_type == F8E5M2FNUZ) {
             float_ir_type = llvm_ir::PrimitiveTypeToIrType(F16, module_);
@@ -3194,8 +3226,10 @@ llvm_ir::ElementGenerator ElementalIrEmitter::MakeElementGenerator(
           }
           llvm::Value* float_val =
               b_->CreateUIToFP(elem_index_linear, float_ir_type);
-          if (component_element_type == F8E4M3FNUZ ||
-              component_element_type == F8E5M2FNUZ) {
+          if (component_element_type == BF16) {
+            TF_ASSIGN_OR_RETURN(iota_result, EmitF32ToBF16(float_val));
+          } else if (component_element_type == F8E4M3FNUZ ||
+                     component_element_type == F8E5M2FNUZ) {
             TF_ASSIGN_OR_RETURN(
                 iota_result, EmitFloatingToF8fnuz(F16, float_val,
                                                   component_element_type, b_));
@@ -3343,6 +3377,11 @@ llvm::Value* ElementalIrEmitter::EmitExtractImag(llvm::Value* value) {
   return ExtractValue(value, {1});
 }
 
+absl::StatusOr<llvm::Value*> ElementalIrEmitter::EmitF32ToBF16(
+    llvm::Value* f32_value) {
+  return DefaultEmitF32ToBF16Impl(f32_value, b_);
+}
+
 llvm::Value* ElementalIrEmitter::EmitComposeComplex(const HloInstruction* op,
                                                     llvm::Value* real,
                                                     llvm::Value* imag) {
diff --git a/xla/service/elemental_ir_emitter.h b/xla/service/elemental_ir_emitter.h
index cff5bbb164..d59d85c695 100644
--- a/xla/service/elemental_ir_emitter.h
+++ b/xla/service/elemental_ir_emitter.h
@@ -77,6 +77,8 @@ class ElementalIrEmitter : public IrBuilderMixin<ElementalIrEmitter> {
   virtual llvm::Value* EmitExtractReal(llvm::Value* value);
   virtual llvm::Value* EmitExtractImag(llvm::Value* value);
 
+  virtual absl::StatusOr<llvm::Value*> EmitF32ToBF16(llvm::Value* f32_value);
+
  private:
   virtual absl::StatusOr<llvm::Value*> EmitUnaryOp(const HloInstruction* op,
                                                    llvm::Value* operand_value);
diff --git a/xla/service/float8_fnuz_ir_emitter.cc b/xla/service/float8_fnuz_ir_emitter.cc
index fe3a104193..aa7f85023d 100644
--- a/xla/service/float8_fnuz_ir_emitter.cc
+++ b/xla/service/float8_fnuz_ir_emitter.cc
@@ -19,6 +19,7 @@ limitations under the License.
 
 #include "llvm/IR/Constants.h"
 #include "llvm/IR/Intrinsics.h"
+#include "llvm/TargetParser/Triple.h"
 #include "xla/primitive_util.h"
 #include "xla/status_macros.h"
 #include "xla/util.h"
@@ -74,7 +75,7 @@ absl::StatusOr<llvm::Type*> PrimitiveTypeToLLVMType(llvm::IRBuilder<>* b,
     case F8E5M2FNUZ:
       return b->getInt8Ty();
     case BF16:
-      return b->getBFloatTy();
+      return b->getInt16Ty();
     case F16:
       return b->getHalfTy();
     case F32:
@@ -610,13 +611,17 @@ absl::StatusOr<llvm::Value*> EmitF8fnuzToFloating(PrimitiveType input_type,
         llvm::Constant* result_lut_array =
             llvm::ConstantArray::get(result_lut_array_type, result_lut);
 
+        int addrspace = llvm::Triple(module->getTargetTriple()).isSPIR() ? 1 : 0;
         return new llvm::GlobalVariable(
             /*M=*/*module,
             /*Ty=*/result_lut_array_type,
             /*isConstant=*/true,
             /*Linkage=*/llvm::GlobalValue::PrivateLinkage,
             /*Initializer=*/result_lut_array,
-            /*Name=*/lut_name);
+            /*Name=*/lut_name,
+            /*InsertBefore*/nullptr,
+            /*TLMode=*/llvm::GlobalValue::NotThreadLocal,
+            /*AddressSpace=*/addrspace);
       });
 
   // Check for NaN, since it's a special case.
diff --git a/xla/service/gpu/BUILD b/xla/service/gpu/BUILD
index 6fc2ed5edc..d0304b7340 100644
--- a/xla/service/gpu/BUILD
+++ b/xla/service/gpu/BUILD
@@ -21,6 +21,7 @@ load(
     "@tsl//tsl/platform/default:cuda_build_defs.bzl",
     "if_cuda_is_configured",
 )
+load("@local_config_sycl//sycl:build_defs.bzl", "if_sycl_is_configured")
 load("//xla:xla.bzl", "xla_cc_test", "xla_cub_deps", "xla_internal")
 load(
     "//xla/service/gpu:build_defs.bzl",
@@ -30,6 +31,7 @@ load(
 )
 load(
     "//xla/stream_executor:build_defs.bzl",
+    "if_cuda_or_rocm",
     "if_gpu_is_configured",
 )
 load("//xla/tests:build_defs.bzl", "xla_test")
@@ -284,7 +286,8 @@ cc_library(
         "//xla/hlo/ir:hlo",
         "//xla/service:buffer_assignment",
         "//xla/service:name_uniquer",
-        "//xla/service/gpu/runtime:nccl_collective_thunk",
+        # "//xla/service/gpu/runtime:nccl_collective_thunk",
+        "@intel_extension_for_openxla//xla/service/gpu:ccl_collective_thunks",
         "//xla/stream_executor:device_description",
         "@com_google_absl//absl/algorithm:container",
         "@com_google_absl//absl/container:flat_hash_map",
@@ -346,9 +349,9 @@ cc_library(
         "//xla/service/gpu/kernels:custom_kernel",
         "//xla/service/gpu/kernels:topk_custom_kernel",
         "//xla/service/gpu/model:tiled_hlo_computation",
-        "//xla/service/gpu/runtime:command_buffer_cmd",
-        "//xla/service/gpu/runtime:command_buffer_cmd_emitter",
-        "//xla/service/gpu/runtime:command_buffer_thunk",
+        # "//xla/service/gpu/runtime:command_buffer_cmd",
+        # "//xla/service/gpu/runtime:command_buffer_cmd_emitter",
+        # "//xla/service/gpu/runtime:command_buffer_thunk",
         "//xla/service/gpu/runtime:conditional_thunk",
         "//xla/service/gpu/runtime:convolution_thunk",
         "//xla/service/gpu/runtime:copy_thunk",
@@ -358,16 +361,16 @@ cc_library(
         "//xla/service/gpu/runtime:gemm_thunk",
         "//xla/service/gpu/runtime:infeed_thunk",
         "//xla/service/gpu/runtime:kernel_thunk",
-        "//xla/service/gpu/runtime:nccl_all_gather_thunk",
-        "//xla/service/gpu/runtime:nccl_all_reduce_thunk",
-        "//xla/service/gpu/runtime:nccl_all_to_all_thunk",
+        # "//xla/service/gpu/runtime:nccl_all_gather_thunk",
+        # "//xla/service/gpu/runtime:nccl_all_reduce_thunk",
+        # "//xla/service/gpu/runtime:nccl_all_to_all_thunk",
         "//xla/service/gpu/runtime:nccl_api",
-        "//xla/service/gpu/runtime:nccl_collective_broadcast_thunk",
-        "//xla/service/gpu/runtime:nccl_collective_permute_thunk",
-        "//xla/service/gpu/runtime:nccl_collective_thunk",
-        "//xla/service/gpu/runtime:nccl_p2p_thunk_common",
-        "//xla/service/gpu/runtime:nccl_recv_thunk",
-        "//xla/service/gpu/runtime:nccl_send_thunk",
+        # "//xla/service/gpu/runtime:nccl_collective_broadcast_thunk",
+        # "//xla/service/gpu/runtime:nccl_collective_permute_thunk",
+        # "//xla/service/gpu/runtime:nccl_collective_thunk",
+        # "//xla/service/gpu/runtime:nccl_p2p_thunk_common",
+        # "//xla/service/gpu/runtime:nccl_recv_thunk",
+        # "//xla/service/gpu/runtime:nccl_send_thunk",
         "//xla/service/gpu/runtime:norm_thunk",
         "//xla/service/gpu/runtime:outfeed_thunk",
         "//xla/service/gpu/runtime:replica_id_thunk",
@@ -415,11 +418,13 @@ cc_library(
         "@tsl//tsl/platform:human_readable_json",
         "@tsl//tsl/platform:statusor",
         "@tsl//tsl/protobuf:dnn_proto_cc",
+        "@intel_extension_for_openxla//xla/service/gpu:ccl_collective_thunks",
+        "@intel_extension_for_openxla//xla/service/gpu:sycl_custom_call",
     ] + if_gpu_is_configured([
-        ":ir_emitter_triton",
+        # ":ir_emitter_triton",
         "//xla/service/gpu/fusions",
         "//xla/service/gpu/runtime:cholesky_thunk",
-        "//xla/service/gpu/runtime:cub_sort_thunk",
+        # "//xla/service/gpu/runtime:cub_sort_thunk",
         "//xla/service/gpu/runtime:gpublas_lt_matmul_thunk",
         "//xla/service/gpu/runtime:triangular_solve_thunk",
     ]) + if_rocm_is_configured([
@@ -1035,6 +1040,7 @@ cc_library(
         "@tsl//tsl/platform:statusor",
         "@tsl//tsl/profiler/lib:scoped_annotation",
         "@tsl//tsl/profiler/lib:traceme",
+        "@intel_extension_for_openxla//xla/service/gpu:xetla_gpu_fused_mha_runner",
     ] + if_gpu_is_configured([
         ":make_batch_pointers",
     ]) + if_cuda_is_configured([
@@ -2285,6 +2291,8 @@ cc_library(
         "//xla/stream_executor/rocm:rocblas_wrapper",
         "//xla/stream_executor/rocm:rocsolver_wrapper",
         "//xla/stream_executor/rocm:hipsolver_wrapper",
+    ]) + if_sycl_is_configured([
+        "@local_config_sycl//sycl:sycl_headers",
     ]),
 )
 
@@ -3076,6 +3084,7 @@ cc_library(
         "//xla/stream_executor:memory_allocation",
         "//xla/stream_executor/cuda:cuda_platform_id",
         "//xla/stream_executor/rocm:rocm_platform_id",
+        "@intel_extension_for_openxla//xla/stream_executor/sycl:sycl_platform_id",
         "@com_google_absl//absl/base:core_headers",
         "@com_google_absl//absl/cleanup",
         "@com_google_absl//absl/container:node_hash_map",
@@ -3404,6 +3413,7 @@ cc_library(
         "gpu_compiler.h",
     ]),
     deps = if_gpu_is_configured([
+        "@intel_extension_for_openxla//xla/service/gpu:dot_expand_dims",
         ":gpu_p2p_pipeliner",
         ":gpu_spmd_pipeline",
         ":pipelined_p2p_rewriter",
@@ -3466,7 +3476,6 @@ cc_library(
         ":topk_specializer",
         ":topk_splitter",
         ":tree_reduction_rewriter",
-        ":triton_fusion_numerics_verifier",
         ":variadic_op_splitter",
         "@com_google_absl//absl/container:flat_hash_set",
         "@com_google_absl//absl/log:check",
@@ -3564,6 +3573,7 @@ cc_library(
         "//xla/service:result_caster",
         "//xla/service:rng_bit_generator_expander",
         "//xla/service:rng_expander",
+        "//xla/service:scatter_promotion",
         "//xla/service:scatter_simplifier",
         "//xla/service:sharding_remover",
         "//xla/service:simplify_fp_conversions",
@@ -3634,7 +3644,9 @@ cc_library(
         "@com_google_absl//absl/status",
         "@llvm-project//mlir:FuncDialect",
         "@tsl//tsl/lib/monitoring:counter",
-    ],
+    ] + if_cuda_or_rocm([
+        ":triton_fusion_numerics_verifier",
+    ]),
 )
 
 xla_test(
@@ -3891,6 +3903,61 @@ xla_test(
     ],
 )
 
+cc_library(
+    name = "spir_compiler_impl",
+    srcs = [
+        "spir_compiler.cc",
+    ],
+    hdrs = [
+        "spir_compiler.h",
+    ],
+    deps = [
+        "@intel_extension_for_openxla//xla/service/gpu:gemm_impl_picker",
+        "@intel_extension_for_openxla//xla/service/gpu:redundant_convert_mover",
+        "@intel_extension_for_openxla//xla/stream_executor/sycl:hw_info",
+        "@intel_extension_for_openxla//xla/stream_executor/sycl:sycl_platform_id",
+        "@com_google_absl//absl/base",
+        "@com_google_absl//absl/container:node_hash_map",
+        "@com_google_absl//absl/types:optional",
+        "@llvm-project//llvm:IRReader",
+        "@llvm-project//llvm:Support",
+        "//xla/service:dot_dimension_merger",
+        "//xla/service:float_normalization",
+        "//xla/service:float_support",
+        "//xla/service:hlo_constant_folding",
+        "//xla/service:hlo_cse",
+        "//xla/service:hlo_dce",
+        "//xla/service:hlo_pass",
+        "//xla/service:hlo_pass_pipeline",
+        "//xla/service:hlo_proto_cc",
+        "//xla/service:hlo_verifier",
+        "//xla/service:llvm_compiler",
+        "//xla/service:reshape_mover",
+        "//xla/service:tuple_simplifier",
+        "//xla/service/gpu:cudnn_fused_conv_rewriter",
+        "//xla/service/gpu:cudnn_fused_mha_rewriter",
+        "//xla/service/gpu:cusolver_rewriter",
+        "//xla/service/gpu:gpu_compiler",
+        "//xla/service/gpu:gpu_conv_padding_legalization",
+        "//xla/service/gpu:target_constants",
+        "//xla/service/gpu:triangular_solve_rewriter",
+        "//xla/service/gpu/llvm_gpu_backend",
+    ],
+)
+
+cc_library(
+    name = "spir_compiler",
+    srcs = [
+        "spir_compiler_registration.cc",
+    ],
+    deps = [
+        ":spir_compiler_impl",
+        "@intel_extension_for_openxla//xla/stream_executor/sycl:sycl_platform_id",
+        "@tsl//tsl/platform:path",
+    ],
+    alwayslink = True,  # Contains compiler registration
+)
+
 xla_cc_test(
     name = "gpu_aot_compilation_test",
     srcs = if_gpu_is_configured([
@@ -6125,9 +6192,9 @@ xla_cc_test(
 
 cc_library(
     name = "triton_fusion_numerics_verifier",
-    srcs = if_gpu_is_configured(["triton_fusion_numerics_verifier.cc"]),
-    hdrs = if_gpu_is_configured(["triton_fusion_numerics_verifier.h"]),
-    deps = if_gpu_is_configured([
+    srcs = if_cuda_or_rocm(["triton_fusion_numerics_verifier.cc"]),
+    hdrs = if_cuda_or_rocm(["triton_fusion_numerics_verifier.h"]),
+    deps = if_cuda_or_rocm([
         ":autotuner_compile_util",
         ":autotuner_util",
         ":backend_configs_cc",
diff --git a/xla/service/gpu/buffer_sharing.cc b/xla/service/gpu/buffer_sharing.cc
index 624d324e73..f7e1a2a090 100644
--- a/xla/service/gpu/buffer_sharing.cc
+++ b/xla/service/gpu/buffer_sharing.cc
@@ -237,6 +237,15 @@ std::optional<bool> CanShareBufferHint(const HloInstruction* user,
                 ->gemm_backend_config();
         return (config.beta() != 0.) && user->operand(2) == operand;
       }
+      // SYCL: inplace sum for onednn conv with side input.
+      if (user->custom_call_target() ==
+          kCudnnConvBiasActivationForwardCallTarget) {
+        CudnnConvBackendConfig config =
+            std::move(user->backend_config<GpuBackendConfig>())
+                ->cudnn_conv_backend_config();
+        return (config.side_input_scale() != 0.) &&
+               (user->operand(user->operand_count() - 1) == operand);
+      }
       // The operand of cholesky can be shared with the first output.
       if (user->custom_call_target() == kCusolverCholeskyCallTarget) {
         return user_index.size() == 1 && user_index[0] == 0;
diff --git a/xla/service/gpu/cudnn_fused_conv_rewriter.cc b/xla/service/gpu/cudnn_fused_conv_rewriter.cc
index e9cb21b9fa..1ba8c60b50 100644
--- a/xla/service/gpu/cudnn_fused_conv_rewriter.cc
+++ b/xla/service/gpu/cudnn_fused_conv_rewriter.cc
@@ -823,6 +823,8 @@ absl::StatusOr<bool> FuseBiasOrSideInput(HloComputation* comp) {
     bool can_accept_bias =
         Match(conv->operand(2), m::Broadcast(m::ConstantEffectiveScalar(0)));
     bool can_accept_side_input = conv->operand_count() < 4;
+    // Flag to tell whether the `side_input` is really accepted.
+    bool accepted_side_input = false;
 
     // The addend can be fused as a bias if
     //  - it is 1D broadcasted in the output feature dimension, and
@@ -859,6 +861,7 @@ absl::StatusOr<bool> FuseBiasOrSideInput(HloComputation* comp) {
       CHECK_EQ(new_operands.size(), 3);
       new_operands.push_back(addend);
       config.set_side_input_scale(1);
+      accepted_side_input = true;
     } else {
       // Can't fuse; this op already has a bias and a side-input.
       continue;
@@ -868,6 +871,13 @@ absl::StatusOr<bool> FuseBiasOrSideInput(HloComputation* comp) {
         conv->CloneWithNewOperands(conv->shape(), new_operands));
     comp->parent()->SetAndUniquifyInstrName(new_conv, conv->name());
     TF_RETURN_IF_ERROR(new_conv->set_backend_config(gpu_config));
+#if TENSORFLOW_USE_SYCL
+    if (accepted_side_input) {
+      xla::Cast<HloCustomCallInstruction>(new_conv)
+          ->set_output_to_operand_aliasing(
+              {{{0}, {static_cast<long>(new_operands.size()) - 1, {}}}});
+    }
+#endif
     TF_ASSIGN_OR_RETURN(HloInstruction * new_instr,
                         MakeGetTupleElementHlo(new_conv, 0));
     TF_RETURN_IF_ERROR(comp->ReplaceInstruction(instr, new_instr));
diff --git a/xla/service/gpu/cudnn_fused_mha_rewriter.cc b/xla/service/gpu/cudnn_fused_mha_rewriter.cc
index f03fe4f0fa..d7975eba83 100644
--- a/xla/service/gpu/cudnn_fused_mha_rewriter.cc
+++ b/xla/service/gpu/cudnn_fused_mha_rewriter.cc
@@ -234,12 +234,14 @@ auto GetUnfusedReduceMaxSumSoftmaxPattern(
   auto unfused_softmax_max_subpattern = m::SharedSubpattern(
       m::Subtract(
           m::Op(),
-          m::Broadcast(OptionalConvert(
-              m::Op()
-                  .WithPredicate(IsReduceMax)
-                  .WithOneUse()
-                  .WithOperand(0, OptionalBitcast(OptionalConvert(
-                                      m::Op(softmax_input).WithNumUser(2)))))))
+          m::Broadcast(
+              OptionalBitcast(OptionalConvert(OptionalBitcast(OptionalConvert(
+                  m::Op()
+                      .WithPredicate(IsReduceMax)
+                      .WithOneUse()
+                      .WithOperand(
+                          0, OptionalBitcast(OptionalConvert(
+                                 m::Op(softmax_input).WithNumUser(2))))))))))
           .WithOneUse());
   // The reduce-add part of the softmax
   // reduce_sum and reduce_sum_broadcast should have 2 users in training
@@ -248,12 +250,12 @@ auto GetUnfusedReduceMaxSumSoftmaxPattern(
       OptionalBitcast(m::Exp(unfused_softmax_max_subpattern)),
       m::Broadcast(
           softmax_reduce_sum_bcast,
-          OptionalConvert(
+          OptionalBitcast(OptionalConvert(
               m::Op(softmax_reduce_sum)
                   .WithOperand(0, OptionalBitcast(OptionalConvert(
                                       m::Exp(unfused_softmax_max_subpattern))))
                   .WithPredicate(IsReduceSum)
-                  .WithAtMostNumUser(2)))
+                  .WithAtMostNumUser(2))))
           .WithAtMostNumUser(2)));
   return unfused_softmax_sum_subpattern;
 }
@@ -428,6 +430,7 @@ absl::StatusOr<bool> IsFlashAttention(
   int64_t s_q = qkv_layout.seqlen_q;
   int64_t s_kv = qkv_layout.seqlen_kv;
   int64_t hidden_dim = qkv_layout.hidden_dim;
+#if !TENSORFLOW_USE_SYCL
   // start with most relaxed constraint
   bool is_seqlen_supported = (!is_training || (s_q % 2 == 0 && s_kv % 2 == 0));
   bool is_hidden_dim_supported = hidden_dim <= 128 && hidden_dim % 8 == 0;
@@ -464,6 +467,11 @@ absl::StatusOr<bool> IsFlashAttention(
     VLOG(2) << "Require cuDNN 8.9.4 to run flash attention.";
     return false;
   }
+#else
+  auto is_hidden_dim_supported =
+      hidden_dim <= 256 && hidden_dim % 2 == 0;
+  auto is_flash_attention = is_hidden_dim_supported;
+#endif
   return is_flash_attention;
 }
 
@@ -621,6 +629,7 @@ MatchFwdResult MatchBmm1UnfusedBiasSoftmaxBmm2(MatchFwdResult previous_result,
         has_dropout ? kCudnnfMHAScaleBiasSoftmaxDropoutCallTarget
                     : kCudnnfMHAScaleBiasSoftmaxCallTarget;
     match_result.is_causal_mask |= IsCausalMaskPattern(bias);
+#if !TENSORFLOW_USE_SYCL
     if (!match_result.is_causal_mask &&
         bias->opcode() == HloOpcode::kBroadcast) {
       // we can take the bias before broadcast
@@ -640,6 +649,7 @@ MatchFwdResult MatchBmm1UnfusedBiasSoftmaxBmm2(MatchFwdResult previous_result,
             bias_bc));
       }
     }
+#endif
     match_result.matched_bias = bias;
     match_result.has_match = true;
   } else {
@@ -676,6 +686,12 @@ MatchFwdResult MatchFwdMHAPatternsForCanonicalization(HloInstruction* instr) {
       continue;
     }
     has_dropout = match_result.matched_dropout_rate > 0.0;
+#if TENSORFLOW_USE_SYCL
+    if (has_dropout) {
+      match_result.has_match = false;
+      return match_result;
+    }
+#endif
     match_result = MatchBmm1UnfusedBiasSoftmaxBmm2(
         match_result, match_result.matched_softmax_input, has_dropout);
     if (match_result.has_match) {
@@ -1087,6 +1103,7 @@ absl::StatusOr<bool> IsMHABlockSupported(
   TF_ASSIGN_OR_RETURN(
       bool is_flash_attention,
       IsFlashAttention(qkv_layout.value(), is_training, cc, cudnn_version));
+#if !TENSORFLOW_USE_SYCL
   if (is_flash_attention) {
     if (is_causal_mask) {
       // if bias is causal mask, needs to remove bias from name
@@ -1097,6 +1114,11 @@ absl::StatusOr<bool> IsMHABlockSupported(
       }
     }
   }
+#else
+  if (is_causal_mask) {
+    return false;
+  }
+#endif
   return is_flash_attention;
 }
 
@@ -1627,6 +1649,7 @@ absl::StatusOr<bool> CudnnFusedMHARewriter::Run(
         comp->parent()->config().debug_options();
     const se::dnn::VersionInfo cudnn_version =
         GetDnnVersionInfoOrDefault(stream_executor_, cudnn_version_);
+#if !TENSORFLOW_USE_SYCL
 #if !defined(GOOGLE_CUDA) || CUDA_VERSION < 12000
     // CUDA needs to be >= 12.0 for cuDNN to work with all supported hardware.
     // Some cuDNN versions work with CUDA 11, but it is impractical for us to
@@ -1639,6 +1662,7 @@ absl::StatusOr<bool> CudnnFusedMHARewriter::Run(
             stream_executor::dnn::VersionInfo(8, 9, 4))) {
       return false;
     }
+#endif  // !TENSORFLOW_USE_SYCL
     for (HloInstruction* instr : comp->MakeInstructionPostOrder()) {
       bool v_transposed = false;
       bool changed = false;
@@ -1721,6 +1745,7 @@ absl::StatusOr<bool> CudnnFusedMHARewriter::Run(
                               matched_result.need_canonicalization));
           continue;
         }
+#if !TENSORFLOW_USE_SYCL
         if (matched_bwd_result.matched_dbias &&
             !(compute_capability_.IsAtLeastHopper() &&
               compute_capability_.minor == 0 &&
@@ -1734,6 +1759,17 @@ absl::StatusOr<bool> CudnnFusedMHARewriter::Run(
                               matched_result.need_canonicalization));
           continue;
         }
+#else
+        if (matched_bwd_result.matched_dbias != nullptr) {
+          // restore fwd graph if bwd pattern match dbias
+          TF_RETURN_IF_ERROR(
+              RestoreFwdGraph(comp, fwd_fmha_call, original_bmm2, activation,
+                              original_bmm2_producer0, original_bmm2_producer1,
+                              original_activation_producers,
+                              matched_result.need_canonicalization));
+          continue;
+        }
+#endif
         // Canonicalize gemms
         if (matched_bwd_result.bmm_1_grad_1_need_canonicalization) {
           TF_ASSIGN_OR_RETURN(
diff --git a/xla/service/gpu/cusolver_context.cc b/xla/service/gpu/cusolver_context.cc
index ccea46d992..88863b3f91 100644
--- a/xla/service/gpu/cusolver_context.cc
+++ b/xla/service/gpu/cusolver_context.cc
@@ -39,6 +39,7 @@ limitations under the License.
 namespace xla {
 namespace gpu {
 
+#if !TENSORFLOW_USE_SYCL
 namespace {
 
 // Type traits to get CUDA complex types from std::complex<T>.
@@ -548,5 +549,19 @@ absl::Status GpuSolverContext::Potrf(
 }
 #endif  // TENSORFLOW_USE_HIPSOLVER
 
+#else // !TENSORFLOW_USE_SYCL
+
+StatusOr<GpuSolverContext> GpuSolverContext::Create() {
+  return GpuSolverContext();
+}
+
+Status GpuSolverContext::SetStream(se::Stream* stream) {
+  gpu_stream_ = stream_executor::gpu::AsGpuStreamValue(stream);
+  return OkStatus();
+}
+
+GpuSolverContext::GpuSolverContext() {}
+
+#endif // !TENSORFLOW_USE_SYCL
 }  // namespace gpu
 }  // namespace xla
diff --git a/xla/service/gpu/cusolver_context.h b/xla/service/gpu/cusolver_context.h
index 74f287254f..18bab8fc66 100644
--- a/xla/service/gpu/cusolver_context.h
+++ b/xla/service/gpu/cusolver_context.h
@@ -31,6 +31,12 @@ limitations under the License.
 #define TENSORFLOW_USE_CUSOLVER_OR_HIPSOLVER \
   (!TENSORFLOW_USE_ROCM || TENSORFLOW_USE_HIPSOLVER)
 
+#if TENSORFLOW_USE_SYCL
+#include "oneapi/mkl/blas.hpp"
+#include "oneapi/mkl/lapack.hpp"
+#include "oneapi/mkl/dfti.hpp"
+#include "oneapi/mkl/exceptions.hpp"
+#else // TENSORFLOW_USE_SYCL
 #if !TENSORFLOW_USE_ROCM
 #include "third_party/gpus/cuda/include/cusolverDn.h"
 using gpusolverHandle_t = cusolverDnHandle_t;
@@ -46,15 +52,17 @@ using gpusolverHandle_t = hipsolverHandle_t;
 using gpusolverHandle_t = rocblas_handle;
 #endif  // TF_ROCM_VERSION >= 40500
 #endif  // TENSORFLOW_USE_ROCM
+#endif  // TENSORFLOW_USE_SYCL
 
 #include "xla/stream_executor/blas.h"
 #include "xla/stream_executor/stream_executor.h"
 #include "xla/xla_data.pb.h"
+#include "xla/stream_executor/gpu/gpu_types.h"
 
 namespace xla {
 namespace gpu {
-
 namespace se = ::stream_executor;
+#if !TENSORFLOW_USE_SYCL
 
 class GpuSolverContext {
  public:
@@ -125,6 +133,90 @@ class GpuSolverContext {
   std::unique_ptr<std::remove_pointer_t<gpusolverHandle_t>, Deleter> handle_;
 };
 
+#else // !TENSORFLOW_USE_SYCL
+class GpuSolverContext {
+ public:
+  static absl::StatusOr<GpuSolverContext> Create();
+  absl::Status SetStream(se::Stream* stream);
+
+  template <typename T>
+  absl::Status PotrfBatched(se::blas::UpperLower uplo, int n, se::DeviceMemory<T*> as,
+                      int lda, se::DeviceMemory<int> lapack_info,
+                      int batch_size, T* a_base) {
+    T* scratch_data = static_cast<T*>(as.opaque());
+    int64_t scratchpad_size = as.size() / sizeof(T);
+
+    const int64_t stride_a = n * n;
+
+    oneapi::mkl::uplo params_uplo;
+    switch (uplo) {
+      case se::blas::UpperLower::kLower:
+        params_uplo = oneapi::mkl::uplo::L;
+        break;
+      case se::blas::UpperLower::kUpper:
+        params_uplo = oneapi::mkl::uplo::U;
+        break;
+      default:
+        params_uplo = static_cast<oneapi::mkl::uplo>(uplo);
+    }
+
+    try {
+      oneapi::mkl::lapack::potrf_batch(*gpu_stream_, params_uplo, n, a_base,
+                                       lda, stride_a, batch_size, scratch_data,
+                                       scratchpad_size);
+    } catch (oneapi::mkl::lapack::batch_error const& be) {
+      int i = 0;
+      auto& ids = be.ids();
+      for (auto const& e : be.exceptions()) {
+        try {
+          std::rethrow_exception(e);
+        } catch (oneapi::mkl::lapack::exception& e) {
+          LOG(FATAL) << "Exception " << ids[i++]
+                     << " in a batch says: " << e.what()
+                     << " (info code: " << e.info() << ")";
+        }
+      }
+    }
+    return absl::OkStatus();
+  }
+
+  template <typename T>
+  absl::Status Potrf(se::blas::UpperLower uplo, int n,
+                     se::DeviceMemory<T> a, int lda,
+                     se::DeviceMemory<int> lapack_info,
+                     se::DeviceMemory<T> workspace){
+    T* a_data = static_cast<T*>(a.opaque());
+    T* scratch_data = static_cast<T*>(workspace.opaque());
+    int64_t scratchpad_size = workspace.size() / sizeof(T);
+
+    oneapi::mkl::uplo params_uplo;
+    switch (uplo) {
+      case se::blas::UpperLower::kLower:
+        params_uplo = oneapi::mkl::uplo::L;
+        break;
+      case se::blas::UpperLower::kUpper:
+        params_uplo = oneapi::mkl::uplo::U;
+        break;
+      default:
+        params_uplo = static_cast<oneapi::mkl::uplo>(uplo);
+    }
+
+    try {
+      oneapi::mkl::lapack::potrf(*gpu_stream_, params_uplo, n, a_data,
+                                 lda, scratch_data, scratchpad_size);
+    } catch (oneapi::mkl::lapack::computation_error const& ce) {
+        LOG(ERROR) << "Exception " << ce.what()
+                   << " (info code: " << ce.info() << ")";
+    }
+    return absl::OkStatus();
+  }
+
+ private:
+  explicit GpuSolverContext();
+  stream_executor::gpu::GpuStreamHandle gpu_stream_;
+};
+
+#endif // !TENSORFLOW_USE_SYCL
 }  // namespace gpu
 }  // namespace xla
 
diff --git a/xla/service/gpu/cusolver_rewriter.cc b/xla/service/gpu/cusolver_rewriter.cc
index ddfda66382..f5c317f5f1 100644
--- a/xla/service/gpu/cusolver_rewriter.cc
+++ b/xla/service/gpu/cusolver_rewriter.cc
@@ -71,6 +71,7 @@ absl::StatusOr<HloInstruction*> CreateCholesky(GpuSolverContext* context,
   absl::c_iota(batch_dim_ids, 0);
   int64_t batch_size = absl::c_accumulate(batch_dims, 1, std::multiplies<>{});
 
+#if !TENSORFLOW_USE_SYCL
   // Find the workspace size.
   se::blas::UpperLower uplo = options.lower() ? se::blas::UpperLower::kLower
                                               : se::blas::UpperLower::kUpper;
@@ -78,7 +79,36 @@ absl::StatusOr<HloInstruction*> CreateCholesky(GpuSolverContext* context,
   TF_ASSIGN_OR_RETURN(
       workspace_size,
       context->PotrfBufferSize(a_shape.element_type(), uplo, n, n, batch_size));
-
+#else
+  // Find the workspace size.
+  int64_t workspace_size = 0;
+  oneapi::mkl::uplo uplo =
+      options.lower() ? oneapi::mkl::uplo::L : oneapi::mkl::uplo::U;
+  sycl::property_list propList{sycl::property::queue::in_order()};
+  sycl::queue queue(sycl::gpu_selector{}, propList);
+  switch (a_shape.element_type()) {
+    case F32:
+      workspace_size = oneapi::mkl::lapack::potrf_batch_scratchpad_size<float>(
+          queue, uplo, n, n, n * n, batch_size);
+      break;
+    case F64:
+      workspace_size = oneapi::mkl::lapack::potrf_batch_scratchpad_size<double>(
+          queue, uplo, n, n, n * n, batch_size);
+      break;
+    case C64:
+      workspace_size =
+          oneapi::mkl::lapack::potrf_batch_scratchpad_size<std::complex<float>>(
+              queue, uplo, n, n, n * n, batch_size);
+      break;
+    case C128:
+      workspace_size = oneapi::mkl::lapack::potrf_batch_scratchpad_size<
+          std::complex<double>>(queue, uplo, n, n, n * n, batch_size);
+      break;
+    default:
+      return InvalidArgument("Invalid type for cholesky %s",
+                             PrimitiveType_Name(a_shape.element_type()));
+  }
+#endif
   // TODO(phawkins): Ideally we would relax this constraint. What we actually
   // want is that:
   // a) the batch dimensions are major, in no particular order.
@@ -105,6 +135,9 @@ absl::StatusOr<HloInstruction*> CreateCholesky(GpuSolverContext* context,
   TF_RETURN_IF_ERROR(custom_call->set_backend_config(options));
   HloInstruction* out = computation->AddInstruction(
       HloInstruction::CreateGetTupleElement(a_shape, custom_call, 0));
+#if TENSORFLOW_USE_SYCL
+  return out;
+#else
   HloInstruction* info = computation->AddInstruction(
       HloInstruction::CreateGetTupleElement(info_shape, custom_call, 2));
 
@@ -134,6 +167,7 @@ absl::StatusOr<HloInstruction*> CreateCholesky(GpuSolverContext* context,
       computation->AddInstruction(HloInstruction::CreateTernary(
           a_shape, HloOpcode::kSelect, ok, out, nans));
   return select;
+#endif
 }
 
 // Tries to rewrite a single convolution into a call to cudnn.
diff --git a/xla/service/gpu/elemental_ir_emitter.cc b/xla/service/gpu/elemental_ir_emitter.cc
index 0ff1cc6d3b..3c5ae69e7c 100644
--- a/xla/service/gpu/elemental_ir_emitter.cc
+++ b/xla/service/gpu/elemental_ir_emitter.cc
@@ -330,6 +330,11 @@ absl::StatusOr<llvm::Value*> GpuElementalIrEmitter::EmitCbrt(
                             prim_type);
 }
 
+absl::StatusOr<llvm::Value*> GpuElementalIrEmitter::EmitF32ToBF16(
+    llvm::Value* f32_value) {
+  return ElementalIrEmitter::EmitF32ToBF16(f32_value);
+}
+
 absl::StatusOr<std::vector<llvm::Value*>>
 GpuElementalIrEmitter::EmitThreadLocalCall(
     const HloComputation& callee, absl::Span<llvm::Value* const> parameters,
diff --git a/xla/service/gpu/elemental_ir_emitter.h b/xla/service/gpu/elemental_ir_emitter.h
index 83f82818f2..a66142c063 100644
--- a/xla/service/gpu/elemental_ir_emitter.h
+++ b/xla/service/gpu/elemental_ir_emitter.h
@@ -100,6 +100,8 @@ class GpuElementalIrEmitter : public ElementalIrEmitter {
       const HloComputation& callee, absl::Span<llvm::Value* const> parameters,
       absl::string_view, bool /*is_reducer*/) override;
 
+  absl::StatusOr<llvm::Value*> EmitF32ToBF16(llvm::Value* f32_value) override;
+
   bool fast_min_max() override {
     return ir_emitter_context_.debug_options().xla_gpu_enable_fast_min_max();
   }
diff --git a/xla/service/gpu/fusions/fusion_emitter.cc b/xla/service/gpu/fusions/fusion_emitter.cc
index cc01f915d0..109df9d880 100644
--- a/xla/service/gpu/fusions/fusion_emitter.cc
+++ b/xla/service/gpu/fusions/fusion_emitter.cc
@@ -64,6 +64,10 @@ limitations under the License.
 #include "tsl/platform/errors.h"
 #include "tsl/platform/statusor.h"
 
+#if TENSORFLOW_USE_SYCL
+#include "xla/service/gpu/ir_emission_utils.h"
+#endif
+
 namespace xla {
 namespace gpu {
 namespace {
@@ -233,6 +237,7 @@ BuildKernelPrototype(IrEmitterContext& ir_emitter_context,
   auto* llvm_module = ir_emitter_context.llvm_module();
   llvm::LLVMContext& context = llvm_module->getContext();
   // Explicitly set global addrspace for SPIR backend.
+  // TODD SYCL may wrong
   int addrspace = llvm::Triple(llvm_module->getTargetTriple()).isSPIR() ? 1 : 0;
   llvm::FunctionType* kernel_type = llvm::FunctionType::get(
       /*Result=*/llvm::Type::getVoidTy(context),
@@ -247,6 +252,17 @@ BuildKernelPrototype(IrEmitterContext& ir_emitter_context,
       ir_emitter_context.gpu_device_info(), launch_dimensions, kernel_name,
       llvm_module));
 
+  // SYCL: Set function metadata
+  if (IsSPIR(llvm_module)) {
+    llvm::LLVMContext& context = llvm_module->getContext();
+    llvm::IntegerType* i32 = llvm::Type::getInt32Ty(context);
+    kernel->setMetadata(
+        "intel_reqd_sub_group_size",
+        llvm::MDNode::get(context,
+                          {llvm::ConstantAsMetadata::get(
+                              llvm::ConstantInt::get(i32, WarpSize()))}));
+  }
+
   // TODO(b/65380986): Investigate if adding fast math flags for generated
   // kernels makes sense.
 
diff --git a/xla/service/gpu/fusions/reduction.cc b/xla/service/gpu/fusions/reduction.cc
index 881b17a52c..f08d077cc4 100644
--- a/xla/service/gpu/fusions/reduction.cc
+++ b/xla/service/gpu/fusions/reduction.cc
@@ -1153,7 +1153,11 @@ ReductionInfo ReductionInfo::Create(const HloFusionAnalysis& analysis) {
   // parallelizing the z dimension (major reduced dimensions). The general
   // recommendation is to use between 128 and 512 threads, so we just go for
   // 256. See https://forums.developer.nvidia.com/t/55529
+#ifdef TENSORFLOW_USE_SYCL
+  constexpr int64_t kThreadsPerBlockTarget = 32;
+#else
   constexpr int64_t kThreadsPerBlockTarget = 256;
+#endif
   if (reduction_dimensions.is_row_reduction &&
       num_threads_x * 2 <= kThreadsPerBlockTarget) {
     int64_t kept_size =
diff --git a/xla/service/gpu/fusions/reduction_base.cc b/xla/service/gpu/fusions/reduction_base.cc
index 66b095a617..e680411f6e 100644
--- a/xla/service/gpu/fusions/reduction_base.cc
+++ b/xla/service/gpu/fusions/reduction_base.cc
@@ -269,4 +269,4 @@ void AddGroupIdConstraint(IndexingMap& map, int64_t root_index,
 }
 
 }  // namespace gpu
-}  // namespace xla
+}  // namespace xla
\ No newline at end of file
diff --git a/xla/service/gpu/fusions/reduction_mlir.cc b/xla/service/gpu/fusions/reduction_mlir.cc
index b3ec1f8e00..fefe200e34 100644
--- a/xla/service/gpu/fusions/reduction_mlir.cc
+++ b/xla/service/gpu/fusions/reduction_mlir.cc
@@ -441,7 +441,11 @@ MlirRowReductionFusion::MlirRowReductionFusion(
   // parallelizing the z dimension (major reduced dimensions). The general
   // recommendation is to use between 128 and 512 threads, so we just go for
   // 256. See https://forums.developer.nvidia.com/t/55529
+#ifdef TENSORFLOW_USE_SYCL
+  constexpr int64_t kThreadsPerBlockTarget = 32;
+#else
   constexpr int64_t kThreadsPerBlockTarget = 256;
+#endif
   if (num_threads_x * 2 <= kThreadsPerBlockTarget) {
     int64_t kept_size = reduction_dimensions_
                             .dimensions[ReductionDimensions::kRowKeptDimension];
diff --git a/xla/service/gpu/fusions/transpose.cc b/xla/service/gpu/fusions/transpose.cc
index 4f205843c0..d235cdbcba 100644
--- a/xla/service/gpu/fusions/transpose.cc
+++ b/xla/service/gpu/fusions/transpose.cc
@@ -49,6 +49,7 @@ limitations under the License.
 #include "xla/service/gpu/target_util.h"
 #include "xla/service/llvm_ir/fused_ir_emitter.h"
 #include "xla/service/llvm_ir/ir_array.h"
+#include "xla/service/llvm_ir/kernel_support_library.h"
 #include "xla/service/llvm_ir/llvm_util.h"
 #include "xla/service/llvm_ir/loop_emitter.h"
 #include "xla/shape_util.h"
@@ -78,7 +79,7 @@ Tiling ComputeTransposeTiling(const se::DeviceDescription& gpu_device_info,
                                              transposed_dims[permutation[2]]};
 
   // We tile along the minor dimensions pre- and post-transpose.
-  absl::InlinedVector<int64_t, 4> tile_sizes{1, 1, 1};
+  absl::InlinedVector<int64_t, 4> tile_sizes{1, 1, 2};
   tile_sizes[permutation[2]] = WarpSize() / kNumRows;
   absl::InlinedVector<int64_t, 4> num_threads{1, 1, WarpSize()};
   num_threads[permutation[2]] = kNumRows;
@@ -202,61 +203,145 @@ absl::Status TransposeFusion::EmitKernel(IrEmitterContext& ir_emitter_context,
         tile_size, absl::StrCat("tr_tile_", tile_idx));
   }
 
+  KernelSupportLibrary ksl(builder, llvm_ir::UnrollMode::kDefaultUnroll);
+
   auto tile_generator = [&](const TilingThreadIdInfo& thread_id_info,
                             const llvm_ir::IrArray::Index& tile_start_index,
                             absl::Span<llvm::Value* const> tile_dimensions) {
     // Copy input parameter values to shared memory buffers:
-    // tile[thread_id_y, thread_id_x] = input[index]
-    EmitTile(builder, tiling_, thread_id_info, tile_dimensions,
-             [&](absl::Span<llvm::Value* const> index_in_tile) {
-               auto index = tile_start_index.AddOffset(index_in_tile, builder);
-               for (const auto& tr : transposes) {
-                 auto input_gen =
-                     *fused_emitter.GetGenerator(*tr.instr->operand(0));
-                 auto input_index = index.SourceIndexOfBitcast(
-                     tr.instr->operand(0)->shape(), builder);
-                 llvm::Value* value = *input_gen(input_index);
-                 tiles[tr.instr].Store(value, index_in_tile, builder);
-               }
-
-               // Compute all extra output values before writing them. This
-               // avoids overwriting aliased input/output values before all
-               // reads occurred.
-               std::vector<std::tuple<llvm_ir::IrArray, llvm_ir::IrArray::Index,
-                                      llvm::Value*>>
-                   scheduled_writes;
-               for (const auto& [output_idx, root] : extra_outputs) {
-                 auto extra_output_index =
-                     index.SourceIndexOfBitcast(root->shape(), builder);
-                 auto output_gen = *fused_emitter.GetGenerator(*root);
-                 llvm::Value* output_value = *output_gen(extra_output_index);
-                 scheduled_writes.emplace_back(
-                     outputs[output_idx], extra_output_index, output_value);
-               }
-
-               for (const auto& [output, idx, value] : scheduled_writes) {
-                 output.EmitWriteArrayElement(idx, value, builder);
-               }
-             });
+    // tile[thread_id_y, thread_id_x * 2] = input[index]
+    // tile[thread_id_y, thread_id_x * 2 + 1] = input[index + 1]
+    llvm::Type* index_ty = thread_id_info.thread_id->getType();
+    auto constant = [&](int64_t val) {
+      return llvm::ConstantInt::get(index_ty, val);
+    };
+    absl::InlinedVector<llvm::Value*, 4> tile_dimensions_Div2{
+        tile_dimensions[0], tile_dimensions[1],
+        builder->CreateUDiv(builder->CreateAdd(tile_dimensions[2], constant(1)),
+                            constant(2))};
+    EmitTile(
+        builder, tiling_, thread_id_info, tile_dimensions_Div2,
+        [&](absl::Span<llvm::Value* const> index_in_tile) {
+          absl::InlinedVector<llvm::Value*, 4> index_in_tile_1{
+              index_in_tile[0], index_in_tile[1],
+              builder->CreateMul(index_in_tile[2], constant(2))};
+          absl::InlinedVector<llvm::Value*, 4> index_in_tile_2{
+              index_in_tile[0], index_in_tile[1],
+              builder->CreateAdd(index_in_tile_1[2], constant(1))};
+
+          auto index_1 = tile_start_index.AddOffset(index_in_tile_1, builder);
+          auto index_2 = tile_start_index.AddOffset(index_in_tile_2, builder);
+
+          auto boundary = tile_start_index.AddOffset(tile_dimensions, builder);
+          auto boundary_multidim = boundary.multidim();
+          // auto index_1_multidim = index_1.multidim();
+          // auto* is_read_1 =
+          //     builder->CreateICmpULT(index_1_multidim[2], boundary_multidim[2]);
+          auto index_2_multidim = index_2.multidim();
+          auto* is_read_2 =
+              builder->CreateICmpULT(index_2_multidim[2], boundary_multidim[2]);
+
+          for (const auto& tr : transposes) {
+            auto input_gen = *fused_emitter.GetGenerator(*tr.instr->operand(0));
+            auto input_index_1 = index_1.SourceIndexOfBitcast(
+                tr.instr->operand(0)->shape(), builder);
+            auto input_index_2 = index_2.SourceIndexOfBitcast(
+                tr.instr->operand(0)->shape(), builder);
+            // ksl.If("is_read_1", is_read_1, [&]() {
+            llvm::Value* value_1 = *input_gen(input_index_1);
+            tiles[tr.instr].Store(value_1, index_in_tile_1, builder);
+            // });
+            ksl.If("is_read_2", is_read_2, [&]() {
+              llvm::Value* value_2 = *input_gen(input_index_2);
+              tiles[tr.instr].Store(value_2, index_in_tile_2, builder);
+            });
+          }
 
+          // Compute all extra output values before writing them. This
+          // avoids overwriting aliased input/output values before all
+          // reads occurred.
+          // std::vector<std::tuple<llvm_ir::IrArray, llvm_ir::IrArray::Index,
+          //                        llvm::Value*>>
+          //     scheduled_writes;
+          // FIXME(Intel): It's not needed now because in-place optimization is
+          // not supported. May need to fix it in the future.
+          for (const auto& [output_idx, root] : extra_outputs) {
+            auto extra_output_index_1 =
+                index_1.SourceIndexOfBitcast(root->shape(), builder);
+            auto extra_output_index_2 =
+                index_2.SourceIndexOfBitcast(root->shape(), builder);
+            auto output_gen = *fused_emitter.GetGenerator(*root);
+            llvm::Value* output_value_1 = *output_gen(extra_output_index_1);
+            llvm::Value* output_value_2 = *output_gen(extra_output_index_2);
+            // ksl.If("is_read_1", is_read_1, [&]() {
+            outputs[output_idx].EmitWriteArrayElement(
+                extra_output_index_1, output_value_1, builder);
+            // });
+            ksl.If("is_read_2", is_read_2, [&]() {
+              outputs[output_idx].EmitWriteArrayElement(
+                  extra_output_index_2, output_value_2, builder);
+            });
+          }
+        });
     EmitSyncThreads(builder, ir_emitter_context);
 
     auto output_tile_index = PermuteIndex(tile_start_index, permutation);
     auto transposed_tile_dimensions = Permute(tile_dimensions, permutation);
+    absl::InlinedVector<llvm::Value*, 4> transposed_tile_dimensions_Div2{
+        transposed_tile_dimensions[0], transposed_tile_dimensions[1],
+        builder->CreateUDiv(
+            builder->CreateAdd(transposed_tile_dimensions[2], constant(1)),
+            constant(2))};
 
     EmitTile(
-        builder, tiling_, thread_id_info, transposed_tile_dimensions,
+        builder, tiling_, thread_id_info, transposed_tile_dimensions_Div2,
         /*emit_elem_function=*/
         [&](absl::Span<llvm::Value* const> index_in_tile) {
-          auto index = output_tile_index.AddOffset(index_in_tile, builder);
+          absl::InlinedVector<llvm::Value*, 4> index_in_tile_1{
+              index_in_tile[0], index_in_tile[1],
+              builder->CreateMul(index_in_tile[2], constant(2))};
+          absl::InlinedVector<llvm::Value*, 4> index_in_tile_2{
+              index_in_tile[0], index_in_tile[1],
+              builder->CreateAdd(index_in_tile_1[2], constant(1))};
+
+          auto index_1 = output_tile_index.AddOffset(index_in_tile_1, builder);
+          auto index_2 = output_tile_index.AddOffset(index_in_tile_2, builder);
+
+          auto boundary =
+              output_tile_index.AddOffset(transposed_tile_dimensions, builder);
+          auto boundary_multidim = boundary.multidim();
+          // auto index_1_multidim = index_1.multidim();
+          // auto* is_write_1 =
+          //     builder->CreateICmpULT(index_1_multidim[2], boundary_multidim[2]);
+          auto index_2_multidim = index_2.multidim();
+          auto* is_write_2 =
+              builder->CreateICmpULT(index_2_multidim[2], boundary_multidim[2]);
+
           for (const auto& tr : transposes) {
-            llvm::Value* loaded = tiles[tr.instr].Load(
-                Permute(index_in_tile, permutation), builder);
+            llvm::Value* loaded_1 = tiles[tr.instr].Load(
+                Permute(index_in_tile_1, permutation), builder);
+            llvm::Value* loaded_2 = tiles[tr.instr].Load(
+                Permute(index_in_tile_2, permutation), builder);
+
+            for (const auto& [output_idx, root] :
+                 transposes_to_roots[tr.instr]) {
+              if (root != tr.instr) continue;
+              auto output_index_1 =
+                  index_1.SourceIndexOfBitcast(tr.instr->shape(), builder);
+              auto output_index_2 =
+                  index_2.SourceIndexOfBitcast(tr.instr->shape(), builder);
+              // ksl.If("is_write_1", is_write_1, [&]() {
+              outputs[output_idx].EmitWriteArrayElement(output_index_1,
+                                                        loaded_1, builder);
+              // });
+              ksl.If("is_write_2", is_write_2, [&]() {
+                outputs[output_idx].EmitWriteArrayElement(output_index_2,
+                                                          loaded_2, builder);
+              });
+            }
 
             FusedIrEmitter fused_emitter(elemental_emitter);
-            fused_emitter.BindGenerator(
-                *tr.instr,
-                [&](const llvm_ir::IrArray::Index&) { return loaded; });
+
             for (int64_t i = 0;
                  i < fusion.fused_instructions_computation()->num_parameters();
                  ++i) {
@@ -273,24 +358,30 @@ absl::Status TransposeFusion::EmitKernel(IrEmitterContext& ir_emitter_context,
             // Compute all output values before writing them. This avoids
             // overwriting aliased input/output values before all reads
             // occurred.
-            std::vector<std::tuple<llvm_ir::IrArray, llvm_ir::IrArray::Index,
-                                   llvm::Value*>>
-                scheduled_writes;
             for (const auto& [output_idx, root] :
                  transposes_to_roots[tr.instr]) {
+              if (root == tr.instr) continue;
               TF_ASSIGN_OR_RETURN(llvm_ir::ElementGenerator gen,
                                   fused_emitter.GetGenerator(*root));
 
               // Both for emission and writing it should be
               // index-as-transformed by the computation.
-              auto untiled_index =
-                  index.SourceIndexOfBitcast(root->shape(), builder);
-              TF_ASSIGN_OR_RETURN(llvm::Value * generated, gen(untiled_index));
-              scheduled_writes.emplace_back(outputs[output_idx], untiled_index,
-                                            generated);
-            }
-            for (const auto& [output, idx, value] : scheduled_writes) {
-              output.EmitWriteArrayElement(idx, value, builder);
+              auto untiled_index_1 =
+                  index_1.SourceIndexOfBitcast(root->shape(), builder);
+              auto untiled_index_2 =
+                  index_2.SourceIndexOfBitcast(root->shape(), builder);
+              TF_ASSIGN_OR_RETURN(llvm::Value * generated_1,
+                                  gen(untiled_index_1));
+              TF_ASSIGN_OR_RETURN(llvm::Value * generated_2,
+                                  gen(untiled_index_2));
+              // ksl.If("is_write_1", is_write_1, [&]() {
+              outputs[output_idx].EmitWriteArrayElement(untiled_index_1,
+                                                        generated_1, builder);
+              // });
+              ksl.If("is_write_2", is_write_2, [&]() {
+                outputs[output_idx].EmitWriteArrayElement(untiled_index_2,
+                                                          generated_2, builder);
+              });
             }
           }
           return absl::OkStatus();
diff --git a/xla/service/gpu/fusions/triton.cc b/xla/service/gpu/fusions/triton.cc
index e9f14aa865..72e7f53282 100644
--- a/xla/service/gpu/fusions/triton.cc
+++ b/xla/service/gpu/fusions/triton.cc
@@ -37,7 +37,6 @@ limitations under the License.
 #include "xla/service/gpu/hlo_traversal.h"
 #include "xla/service/gpu/ir_emission_utils.h"
 #include "xla/service/gpu/ir_emitter_context.h"
-#include "xla/service/gpu/ir_emitter_triton.h"
 #include "xla/service/gpu/kernel_arguments.h"
 #include "xla/service/gpu/kernel_reuse_cache.h"
 #include "xla/service/gpu/launch_dimensions.h"
@@ -51,6 +50,11 @@ limitations under the License.
 #include "xla/status_macros.h"
 #include "tsl/platform/statusor.h"
 
+#if !TENSORFLOW_USE_SYCL
+#include "xla/service/gpu/ir_emitter_triton.h"
+#else
+#include "absl/status/status.h"
+#endif // TENSORFLOW_USE_SYCL
 namespace xla {
 namespace gpu {
 namespace {
@@ -143,6 +147,7 @@ std::optional<TritonFusion::LaunchConfig> CalculateSoftMaxLaunchConfig(
 absl::StatusOr<FusionEmissionResult> TritonFusion::Emit(
     IrEmitterContext& ir_emitter_context,
     const HloFusionInstruction& fusion) const {
+#if !TENSORFLOW_USE_SYCL
   llvm::IRBuilder builder(ir_emitter_context.llvm_module()->getContext());
   VLOG(3) << fusion.ToString();
   std::string suggested_kernel_name = std::string(fusion.name());
@@ -278,6 +283,9 @@ absl::StatusOr<FusionEmissionResult> TritonFusion::Emit(
       entry->launch_dimensions, entry->cluster_dim, entry->shmem_bytes));
 
   return result;
+#else
+  return absl::UnimplementedError("SYCL does not surpport Triton");
+#endif // TENSORFLOW_USE_SYCL
 }
 
 std::optional<TritonFusion::LaunchConfig> TritonFusion::launch_config() const {
diff --git a/xla/service/gpu/gemm_rewriter.cc b/xla/service/gpu/gemm_rewriter.cc
index e109c6b3fa..620466d8b9 100644
--- a/xla/service/gpu/gemm_rewriter.cc
+++ b/xla/service/gpu/gemm_rewriter.cc
@@ -667,6 +667,7 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {
 
   absl::Status HandleMultiply(HloInstruction *instr) override {
     HloInstruction *alpha, *existing_gemm;
+#if !TENSORFLOW_USE_SYCL
     if (Match(instr,
               m::MultiplyAnyOrder(
                   GemmOrCublasLtMatmulMaybeF8(&existing_gemm).WithOneUser(),
@@ -690,6 +691,7 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {
         return ReplaceInstruction(instr, existing_gemm);
       }
     }
+#endif  // !TENSORFLOW_USE_SYCL
 
     HloInstruction *d_scale;
     if (Match(instr, m::MultiplyAnyOrder(
@@ -702,6 +704,7 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {
     // (https://arxiv.org/abs/1606.08415), where:
     // approx_gelu(x) = x * cdf(x)
     // cdf(x) = 0.5 * (1 + tanh(sqrt(2 / pi) * (x + 0.044715 * x**3))
+    // SYCL: OptionalBitcast?
     HloInstruction *cdf, *slice_or_bitcast = nullptr;
     if (Match(instr, m::MultiplyAnyOrder(
                          m::AnyOf<HloInstruction>(
@@ -741,6 +744,14 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {
                                   .WithOneUser())
                               .WithOneUser())
                           .WithOneUser())))) {
+      // SYCL
+      // gemm - bitcast - gelu - bitcast
+      if (instr->user_count() == 1) {
+        auto bitcast = instr->users()[0];
+        if (bitcast->opcode() == HloOpcode::kBitcast &&
+            ShapeUtil::Compatible(bitcast->shape(), existing_gemm->shape()))
+          return FuseGeluActivation(bitcast, existing_gemm);
+      }
       return FuseGeluActivation(instr, existing_gemm, slice_or_bitcast);
     }
     return absl::OkStatus();
@@ -1514,8 +1525,8 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {
       return in_out_alias_config.ParameterHasAlias(bias->parameter_number(),
                                                    /*param_index=*/{});
     }();
-    bool want_to_fuse_bias = IsCublasLtMatmulF8(*gemm) ||
-                             IsCublasLtMatmul(*gemm) || can_overwrite_bias;
+    // SYCL: cannot always fuse bias.
+    bool want_to_fuse_bias = can_overwrite_bias;
 
     auto gpu_config = gemm->backend_config<GpuBackendConfig>().value();
     GemmBackendConfig &config = *gpu_config.mutable_gemm_backend_config();
@@ -1822,13 +1833,14 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {
   absl::StatusOr<absl::string_view> GetNonFp8GemmCustomCallTarget(
       const HloInstruction &instr,
       const GemmBackendConfig &gemm_backend_config) const {
-    if (!instr.GetModule()
-             ->config()
-             .debug_options()
-             .xla_gpu_enable_cublaslt()) {
-      // cublasLt is not enabled.
-      return absl::string_view(kGemmCallTarget);
-    }
+    // SYCL: disable fallback
+    // if (!instr.GetModule()
+    //          ->config()
+    //          .debug_options()
+    //          .xla_gpu_enable_cublaslt()) {
+    //   // cublasLt is not enabled.
+    //   return absl::string_view(kGemmCallTarget);
+    // }
 
     // cublasLt is enabled, check if other internal conditions are met.
     const HloInstruction *lhs = instr.operand(0);
@@ -2177,11 +2189,6 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {
     for (auto batch_dimension : batch_dimensions) {
       batch_count *= lhs->shape().dimensions(batch_dimension);
     }
-    if (batch_count > kMaxBatchCount) {
-      // This is not supported by cublasLt.
-      return false;
-    }
-
     TF_ASSIGN_OR_RETURN(bool output_is_column_major,
                         MatrixIsColumnMajor(instr, gemm_backend_config));
 
diff --git a/xla/service/gpu/gpu_compiler.cc b/xla/service/gpu/gpu_compiler.cc
index 601619ee6e..875fe203f9 100644
--- a/xla/service/gpu/gpu_compiler.cc
+++ b/xla/service/gpu/gpu_compiler.cc
@@ -170,7 +170,6 @@ limitations under the License.
 #include "xla/service/gpu/topk_specializer.h"
 #include "xla/service/gpu/topk_splitter.h"
 #include "xla/service/gpu/tree_reduction_rewriter.h"
-#include "xla/service/gpu/triton_fusion_numerics_verifier.h"
 #include "xla/service/hlo.pb.h"
 #include "xla/service/hlo_computation_deduplicator.h"
 #include "xla/service/hlo_constant_folding.h"
@@ -205,6 +204,7 @@ limitations under the License.
 #include "xla/service/rng_bit_generator_expander.h"
 #include "xla/service/rng_expander.h"
 #include "xla/service/scatter_expander.h"
+#include "xla/service/scatter_promotion.h"
 #include "xla/service/scatter_simplifier.h"
 #include "xla/service/sharding_remover.h"
 #include "xla/service/simplify_fp_conversions.h"
@@ -252,6 +252,12 @@ limitations under the License.
 #include "xla/hlo/experimental/auto_sharding/auto_sharding.h"
 #endif  // PLATFORM_GOOGLE
 
+#if !TENSORFLOW_USE_SYCL
+#include "xla/service/gpu/triton_fusion_numerics_verifier.h"
+#endif  // TENSORFLOW_USE_SYCL
+
+#include "xla/service/gpu/dot_expand_dims.h"
+
 namespace xla {
 namespace gpu {
 namespace {
@@ -512,7 +518,9 @@ AlgebraicSimplifierOptions LayoutInsensitiveAlgebraicSimplifierOptions(
   layout_insensitive_algsimp_opts
       .set_unconditionally_simplify_reduce_of_transpose_or_reshape(true);
 
-  if (gpu_target_config.platform_name == "ROCM") {
+  // SYCL: Conv swap has accuracy issue in some cases.
+  if (gpu_target_config.platform_name == "ROCM" ||
+      gpu_target_config.platform_name == "SYCL") {
     layout_insensitive_algsimp_opts.set_enable_conv_operand_swap(false);
   }
   layout_insensitive_algsimp_opts
@@ -746,6 +754,10 @@ absl::Status RunOptimizationPasses(
 
     pipeline.AddPass<GatherSimplifier>();
     pipeline.AddPass<GatherExpander>(GatherExpander::kEliminateSimpleGathers);
+    // promote 16 bit integer scatter to 32-bit to avoid 16-bit atomic.
+    const std::pair<PrimitiveType, PrimitiveType> ar_promoted_types[] = {
+        {F16, F32}, {BF16, F32}};
+    pipeline.AddPass<ScatterPromotion>(ar_promoted_types);
     pipeline.AddPass<ScatterSimplifier>();
     pipeline.AddPass<ScatterExpander>(
         ScatterExpander::kEliminateSimpleScatters);
@@ -930,6 +942,10 @@ absl::Status RunLayoutAssignmentPasses(HloModule* hlo_module,
   HloPassPipeline pipeline("layout assignment");
   // Layout assignment uses alias analysis, which requires the call graph to
   // be flattened.
+  // SYCL: LLM passes.
+  bool llm_flag = false;
+  tsl::ReadBoolFromEnvVar("LLM", false, &llm_flag);
+  if (llm_flag) pipeline.AddPass<DotExpandDims>();
   pipeline.AddPass<FlattenCallGraph>();
   ChannelLayoutConstraints layout_constraints;
   pipeline.AddPass<GpuLayoutAssignment>(
@@ -1120,6 +1136,7 @@ absl::Status RunPostFusionVerificationPasses(
     HloModule* hlo_module, se::StreamExecutor* stream_exec,
     const GpuCompiler::CompileOptions& options,
     const Compiler::TargetConfig& gpu_target_config) {
+#if !TENSORFLOW_USE_SYCL
   HloPassPipeline pipeline("post-fusion-verification-pipeline optimization");
 
   if (hlo_module->config()
@@ -1134,6 +1151,10 @@ absl::Status RunPostFusionVerificationPasses(
   }
 
   return pipeline.Run(hlo_module).status();
+#else
+  // SYCL does not support Triton
+  return absl::OkStatus();
+#endif //TENSORFLOW_USE_SYCL
 }
 
 }  // namespace
@@ -1170,11 +1191,13 @@ absl::Status GpuCompiler::OptimizeHloModule(
   // Run target-specific HLO optimization passes for convolution
   // canonicalization.
   se::dnn::VersionInfo dnn_version = gpu_target_config.dnn_version_info;
+  // SYCL: do not check dnn version
+#if 0
   if (stream_exec != nullptr) {
     gpu_version = GetGpuVersion(stream_exec);
     TF_ASSIGN_OR_RETURN(dnn_version, GetDnnVersionInfo(stream_exec));
   }
-
+#endif
   TF_RETURN_IF_ERROR(OptimizeHloConvolutionCanonicalization(
       hlo_module, gpu_version, dnn_version, options.device_allocator));
 
@@ -1343,12 +1366,14 @@ absl::Status GpuCompiler::OptimizeHloPostLayoutAssignment(
 
     // Rewrite FP8 GEMMs ahead of Triton which currently lacks support for FP8
     // and may rewrite quantized FP8 GEMMs as higher-precision GEMMs.
+    // SYCL doesn't support fp8 gemm yet.
     pipeline.AddPass<GemmRewriter>(gpu_version, GetToolkitVersion(),
-                                   /*f8_rewrite=*/true);
+                                   /*f8_rewrite=*/false);
     if (debug_options.xla_gpu_enable_triton_gemm() && cuda_cc != nullptr &&
         cuda_cc->IsAtLeast(se::CudaComputeCapability::AMPERE)) {
       pipeline.AddPass<GemvRewriter>();
-      pipeline.AddPass<GemmFusion>(gpu_version);
+      // SYCL: disable triton gemm
+      // pipeline.AddPass<GemmFusion>(gpu_version);
     }
     // Rewrite non-FP8 GEMMs.
     pipeline.AddPass<GemmRewriter>(gpu_version, GetToolkitVersion(),
@@ -1376,9 +1401,9 @@ absl::Status GpuCompiler::OptimizeHloPostLayoutAssignment(
     if (debug_options.xla_gpu_enable_triton_softmax_fusion() &&
         cuda_cc != nullptr &&
         cuda_cc->IsAtLeast(se::CudaComputeCapability::AMPERE)) {
-      pipeline.AddPass<HloPassFix<GpuAlgebraicSimplifier>>(simplifier_options,
-                                                           gpu_version);
-      pipeline.AddPass<SoftmaxRewriterTriton>(gpu_version);
+      // SYCL: disable triton
+      // pipeline.AddPass<HloPassFix<AlgebraicSimplifier>>(simplifier_options);
+      // pipeline.AddPass<SoftmaxRewriterTriton>(gpu_version);
     }
 
     pipeline.AddPass<ReductionDimensionGrouper>();
@@ -1720,6 +1745,11 @@ GpuCompiler::CompileSingleModule(const HloModuleConfig& module_config,
 
   // Write PTX to IR dump directory, if IR dumping was requested.
   if (should_dump) {
+    // SYCL: dump spv
+    auto spir_vector = result.binary;
+    std::string spir(spir_vector.begin(), spir_vector.end());
+    DumpToFileInDirOrStdout(*debug_module, "", "spv", spir);
+
     absl::string_view ptx = result.asm_text;
     if (debug_module) {
       DumpToFileInDirOrStdout(*debug_module, "",
@@ -2232,8 +2262,8 @@ GpuCompiler::CompileAheadOfTime(std::unique_ptr<HloModuleGroup> module_group,
                                 const AotCompilationOptions& options) {
   // Check that we are on the platform (CUDA or ROCm) that was chosen for AOT
   // compilation.
-  CHECK_EQ(options.PlatformId(), PlatformId());
-
+  // CHECK_EQ(options.PlatformId(), PlatformId());
+#if 0
   std::vector<std::unique_ptr<HloModule>> modules =
       module_group->ConsumeModules();
 
@@ -2285,6 +2315,7 @@ GpuCompiler::CompileAheadOfTime(std::unique_ptr<HloModuleGroup> module_group,
   }
 
   return std::move(results);
+#endif
 }
 
 HloCostAnalysis::ShapeSizeFunction GpuCompiler::ShapeSizeBytesFunction() const {
@@ -2404,10 +2435,18 @@ absl::Status GpuCompiler::RunPostSchedulingPipelines(
   {
     HloPassPipeline pipeline("command-buffer-scheduling");
     auto driver_version = se::gpu::GpuDriver::GetDriverVersion();
-    const int32_t toolkit_version = GetToolkitVersion();
+#if GOOGLE_CUDA
+    constexpr int toolkit_version = CUDA_VERSION;
+#elif TENSORFLOW_USE_ROCM
+    constexpr int toolkit_version = TF_ROCM_VERSION;
+#else
+    constexpr int toolkit_version = -1;
+#endif
+#if 0
     pipeline.AddPass<CommandBufferScheduling>(
         gpu_device_info, toolkit_version,
         driver_version.value_or(toolkit_version));
+#endif
     pipeline.AddPass<GpuSanitizeConstantNames>();
     TF_RETURN_IF_ERROR(pipeline.Run(module).status());
   }
diff --git a/xla/service/gpu/gpu_executable.cc b/xla/service/gpu/gpu_executable.cc
index 7a9927e076..f218d69c28 100644
--- a/xla/service/gpu/gpu_executable.cc
+++ b/xla/service/gpu/gpu_executable.cc
@@ -201,7 +201,9 @@ absl::Status GpuExecutable::CheckCompatibilityWithServiceExecutableRunOptions(
         << "}, but was {" << std::get<se::CudaComputeCapability>(cc).ToString()
         << "}";
   } else {
+#if !TENSORFLOW_USE_SYCL
     return Internal("Unknown platform");
+#endif
   }
 
   return absl::OkStatus();
@@ -375,6 +377,9 @@ absl::Status ExecuteThunks(
   se::StreamExecutor* executor = main_stream->parent();
   stream_executor::StreamPriority stream_priority =
       stream_executor::StreamPriority::Default;
+#if TENSORFLOW_USE_SYCL
+  use_highest_priority_for_async_stream = false;
+#endif
   if (use_highest_priority_for_async_stream) {
     stream_priority = stream_executor::StreamPriority::Highest;
   }
@@ -636,11 +641,17 @@ GpuExecutable::ResolveConstantGlobals(se::Stream* stream) {
   // The CUDA driver isn't able to load a PTX and a binary which are both empty.
   // It's okay if we skip loading in this case; if the module isn't loaded, all
   // symbol lookups will fail, just as they should for an empty module.
+#if !TENSORFLOW_USE_SYCL
   if (!(executor->GetPlatform()->id() ==
             stream_executor::cuda::kCudaPlatformId &&
         binary().empty() && text().empty())) {
     TF_RETURN_IF_ERROR(executor->LoadModule(module_spec, &module_handle));
   }
+#else
+  if (module_spec.has_cuda_cubin_in_memory()) {
+    TF_RETURN_IF_ERROR(executor->LoadModule(module_spec, &module_handle));
+  }
+#endif
 
   // A flag signalling if constant initialization submitted memcpy operations
   // to the `stream`.
@@ -668,6 +679,26 @@ GpuExecutable::ResolveConstantGlobals(se::Stream* stream) {
         submitted_mem_copies = true;
       }
     } else {
+#if TENSORFLOW_USE_SYCL
+      // SYCL: content may be empty
+      if (info.content.span().empty()) {
+        // LLVM module contains the const variable, but it still fails to look
+        // up symbol. So allocate an empty buffer here.
+        void* opaque = nullptr;
+        size_t bytes = 0;
+        global = se::DeviceMemoryBase(opaque, bytes);
+      } else {
+        TF_ASSIGN_OR_RETURN(
+            auto shared, executor->CreateOrShareConstant(stream, info.content.span()));
+        global = *shared;
+        VLOG(3) << "Allocated (or shared) global " << info.symbol_name << " at "
+                << global.opaque();
+        // XLA will continue to own this global at least until this executable
+        // is destroyed (longer if another, longer-lived executable shares the
+        // same constant).
+        shared_constants_.push_back(std::move(shared));
+      }
+#else
       // The constant was not defined in the PTX and therefore must be both
       // allocated and initialized by XLA here.
       CHECK(!info.content.span().empty());
@@ -681,6 +712,7 @@ GpuExecutable::ResolveConstantGlobals(se::Stream* stream) {
       // destroyed (longer if another, longer-lived executable shares the same
       // constant).
       shared_constants_.push_back(std::move(shared));
+#endif
     }
 
     if (info.allocation_index != -1) {
diff --git a/xla/service/gpu/gpu_fusible.cc b/xla/service/gpu/gpu_fusible.cc
index 848f968049..5f937a20d9 100644
--- a/xla/service/gpu/gpu_fusible.cc
+++ b/xla/service/gpu/gpu_fusible.cc
@@ -483,6 +483,13 @@ FusionDecision IsProducerConsumerFusible(const HloInstruction& producer,
   }
 
   if (IsInputFusibleReduction(producer)) {
+#if TENSORFLOW_USE_SYCL
+    // TODO: Check on latest XLA. Reduction epilogue fusion may cause
+    // regression for row reductions cases with large dim.
+    // It changes fusion kind from kInput to kLoop, and
+    // will fail to enter the row vectorization pass.
+    return "Reduction epilogue fusion is not enabled.";
+#endif
     if (!producer.GetModule()
              ->config()
              .debug_options()
diff --git a/xla/service/gpu/gpu_layout_assignment.cc b/xla/service/gpu/gpu_layout_assignment.cc
index 008dbaeade..7bea6615e5 100644
--- a/xla/service/gpu/gpu_layout_assignment.cc
+++ b/xla/service/gpu/gpu_layout_assignment.cc
@@ -88,7 +88,7 @@ HeuristicLayoutAssignment(const HloInstruction* instr,
       std::make_tuple(DataLayout::kBatchDepthYX4, FilterLayout::kOutputInputYX4,
                       DataLayout::kBatchDepthYX4);
   constexpr auto kAllNHWC =
-      std::make_tuple(DataLayout::kBatchYXDepth, FilterLayout::kOutputYXInput,
+      std::make_tuple(DataLayout::kBatchYXDepth, FilterLayout::kYXInputOutput,
                       DataLayout::kBatchYXDepth);
 
   // Integer convolution must use NHWC or NCHW_VECT_C.
diff --git a/xla/service/gpu/gpu_spmd_pipeline.cc b/xla/service/gpu/gpu_spmd_pipeline.cc
index e0973e1411..6f4f88d18d 100644
--- a/xla/service/gpu/gpu_spmd_pipeline.cc
+++ b/xla/service/gpu/gpu_spmd_pipeline.cc
@@ -66,8 +66,9 @@ void AddSPMDPasses(
   spmd_simplify.AddPass<TupleSimplifier>();
   spmd_simplify.AddPass<ScatterExpander>(
       ScatterExpander::kEliminateSimpleScatters);
-  spmd_simplify.AddPass<GatherExpander>(
-      GatherExpander::kEliminateSimpleGathers);
+// FIXME(intel): Reopen below pass when fix sharding error.
+//   spmd_simplify.AddPass<GatherExpander>(
+//       GatherExpander::kEliminateSimpleGathers);
   spmd_simplify.AddPass<WhileLoopConstantSinking>();
   spmd_simplify.AddPass<WhileLoopSimplifier>();
 
diff --git a/xla/service/gpu/gpu_transfer_manager.cc b/xla/service/gpu/gpu_transfer_manager.cc
index f628e17aa5..f675fb4533 100644
--- a/xla/service/gpu/gpu_transfer_manager.cc
+++ b/xla/service/gpu/gpu_transfer_manager.cc
@@ -46,6 +46,7 @@ limitations under the License.
 #include "xla/stream_executor/memory_allocation.h"
 #include "xla/stream_executor/platform.h"
 #include "xla/stream_executor/rocm/rocm_platform_id.h"
+#include "xla/stream_executor/sycl/sycl_platform_id.h"
 #include "xla/stream_executor/stream_executor.h"
 #include "xla/util.h"
 #include "tsl/platform/errors.h"
@@ -342,11 +343,20 @@ static std::unique_ptr<xla::TransferManager> CreateAMDGPUTransferManager() {
           .getPointerSize(0 /* default address space */));
 }
 
+static std::unique_ptr<xla::TransferManager> CreateSYCLTransferManager() {
+  return std::make_unique<xla::gpu::GpuTransferManager>(
+      /*id=*/stream_executor::sycl::kSyclPlatformId,
+      /*pointer_size=*/llvm::DataLayout(xla::gpu::spir::DataLayout())
+          .getPointerSize(0 /* default address space */));
+}
+
 static bool InitModule() {
   xla::TransferManager::RegisterTransferManager(
       stream_executor::cuda::kCudaPlatformId, &CreateNVPTXTransferManager);
   xla::TransferManager::RegisterTransferManager(
       stream_executor::rocm::kROCmPlatformId, &CreateAMDGPUTransferManager);
+  xla::TransferManager::RegisterTransferManager(
+      stream_executor::sycl::kSyclPlatformId, &CreateSYCLTransferManager);
   return true;
 }
 
diff --git a/xla/service/gpu/ir_emission_utils.cc b/xla/service/gpu/ir_emission_utils.cc
index 069ae1cf75..8440a33b4f 100644
--- a/xla/service/gpu/ir_emission_utils.cc
+++ b/xla/service/gpu/ir_emission_utils.cc
@@ -117,13 +117,12 @@ bool IsMatrixMultiplication(const HloInstruction& dot) {
   const DotDimensionNumbers& dim_numbers = dot.dot_dimension_numbers();
 
   PrimitiveType output_primitive_type = dot.shape().element_type();
+  // Disable F64, C64, C128
   bool type_is_allowed =
       (output_primitive_type == F8E4M3FN || output_primitive_type == F8E5M2 ||
        output_primitive_type == F8E4M3FNUZ ||
        output_primitive_type == F8E5M2FNUZ || output_primitive_type == F16 ||
-       output_primitive_type == BF16 || output_primitive_type == F32 ||
-       output_primitive_type == F64 || output_primitive_type == C64 ||
-       output_primitive_type == C128) ||
+       output_primitive_type == BF16 || output_primitive_type == F32) ||
       (output_primitive_type == S32 && lhs_shape.element_type() == S8 &&
        rhs_shape.element_type() == S8);
   bool shapes_are_valid =
@@ -146,11 +145,11 @@ bool IsMatrixVectorMultiplication(const HloInstruction& dot) {
   const DotDimensionNumbers& dim_numbers = dot.dot_dimension_numbers();
 
   PrimitiveType output_primitive_type = dot.shape().element_type();
+  // Disable F64, C64, C128
   bool type_is_allowed =
       (output_primitive_type == F8E4M3FN || output_primitive_type == F8E5M2 ||
        output_primitive_type == F16 || output_primitive_type == BF16 ||
-       output_primitive_type == F32 || output_primitive_type == F64 ||
-       output_primitive_type == C64 || output_primitive_type == C128) ||
+       output_primitive_type == F32) ||
       (output_primitive_type == S32 && lhs_shape.element_type() == S8 &&
        rhs_shape.element_type() == S8);
 
diff --git a/xla/service/gpu/ir_emitter_context.cc b/xla/service/gpu/ir_emitter_context.cc
index 0a51d97622..03672a9fc8 100644
--- a/xla/service/gpu/ir_emitter_context.cc
+++ b/xla/service/gpu/ir_emitter_context.cc
@@ -67,6 +67,8 @@ void IrEmitterContext::emit_constant(int64_t num_elements,
       return llvm::ConstantAggregateZero::get(global_type);
     }
 
+    // SYCL: always set info.content.
+    info.content = content;
     std::vector<uint8_t> padded(kMinConstAllocationInBytes, 0);
     absl::c_copy(content.span(), padded.begin());
     return llvm::ConstantDataArray::get<uint8_t>(
@@ -77,8 +79,8 @@ void IrEmitterContext::emit_constant(int64_t num_elements,
   }();
 
   // Explicitly set global addrspace for SPIR backend.
-  int addrspace =
-      llvm::Triple(llvm_module_constants()->getTargetTriple()).isSPIR() ? 1 : 0;
+  auto is_spir = llvm::Triple(llvm_module_->getTargetTriple()).isSPIR();
+  int addrspace = is_spir ? 1 : 0;
   // These globals will be looked up by name by GpuExecutable so we need to
   // give them an external linkage.  Not all of their uses are visible in
   // the LLVM IR so we can't give then a linkage that merely preserves their
@@ -95,6 +97,27 @@ void IrEmitterContext::emit_constant(int64_t num_elements,
       /*AddressSpace=*/addrspace,
       /*isExternallyInitialized=*/false);
   global_for_const->setAlignment(llvm::Align(kConstantBufferAlignBytes));
+
+  if (is_spir) {
+    // SYCL: Add spirv.Decorations for global variable. See document about the
+    // annotation:
+    // https://github.com/intel/llvm/blob/sycl/sycl/doc/design/spirv-extensions/SPV_INTEL_global_variable_decorations.asciidoc
+    llvm::LLVMContext& context = llvm_module_->getContext();
+    llvm::SmallVector<llvm::Metadata*, 4> metadatas;
+    std::vector<llvm::Metadata*> ops;
+
+    auto* kind = llvm::ConstantAsMetadata::get(llvm::ConstantInt::get(
+        llvm::Type::getInt32Ty(context), /*IDecHostAccessINTEL*/ 6147));
+    ops.push_back(kind);
+    auto* const acc_mode = llvm::ConstantAsMetadata::get(llvm::ConstantInt::get(
+        llvm::Type::getInt32Ty(context), /*AccessMode*/ 2));
+    ops.push_back(acc_mode);
+    ops.push_back(llvm::MDString::get(context, symbol_name));
+    metadatas.push_back(llvm::MDNode::get(context, ops));
+
+    llvm::MDNode* md_list = llvm::MDNode::get(context, metadatas);
+    global_for_const->setMetadata("spirv.Decorations", md_list);
+  }
   llvm_module_constants()->insertGlobalVariable(global_for_const);
 
   info.symbol_name.assign(symbol_name);
diff --git a/xla/service/gpu/ir_emitter_context.h b/xla/service/gpu/ir_emitter_context.h
index b1a7760c6a..5fd29e2cc6 100644
--- a/xla/service/gpu/ir_emitter_context.h
+++ b/xla/service/gpu/ir_emitter_context.h
@@ -36,7 +36,8 @@ limitations under the License.
 #include "xla/service/gpu/gpu_executable.h"
 #include "xla/service/gpu/ir_emission_utils.h"
 #include "xla/service/gpu/kernel_reuse_cache.h"
-#include "xla/service/gpu/runtime/nccl_collective_thunk.h"
+// #include "xla/service/gpu/runtime/nccl_collective_thunk.h"
+#include "xla/service/gpu/ccl_collective_thunk.h"
 #include "xla/service/name_uniquer.h"
 #include "xla/stream_executor/device_description.h"
 
diff --git a/xla/service/gpu/ir_emitter_nested.cc b/xla/service/gpu/ir_emitter_nested.cc
index c278ee930e..cc7beaa19b 100644
--- a/xla/service/gpu/ir_emitter_nested.cc
+++ b/xla/service/gpu/ir_emitter_nested.cc
@@ -326,7 +326,8 @@ void EmitAMDGPUAtomicAdd(llvm::IRBuilder<>* builder,
 
   builder->CreateAtomicRMW(
       llvm::AtomicRMWInst::FAdd, output_ptr, source, llvm::MaybeAlign(),
-      llvm::AtomicOrdering::SequentiallyConsistent,
+      // SYCL: set Monotonic.
+      llvm::AtomicOrdering::Monotonic,
       builder->getContext().getOrInsertSyncScopeID("agent"));
 }
 
@@ -402,7 +403,7 @@ bool MaybeEmitDirectAtomicOperation(llvm::IRBuilder<>* builder,
       if (atomic_add_supported) {
         builder->CreateAtomicRMW(llvm::AtomicRMWInst::FAdd, output_address,
                                  source, llvm::MaybeAlign(),
-                                 llvm::AtomicOrdering::SequentiallyConsistent);
+                                 llvm::AtomicOrdering::Monotonic);
         return true;
       }
     }
@@ -416,11 +417,19 @@ bool MaybeEmitDirectAtomicOperation(llvm::IRBuilder<>* builder,
       return true;
     }
 
+    if (target_triple.isSPIR() &&
+        element_type == F32) {
+      builder->CreateAtomicRMW(llvm::AtomicRMWInst::FAdd, output_address,
+                               source, llvm::MaybeAlign(),
+                               llvm::AtomicOrdering::Monotonic);
+      return true;
+    }
+
     if (is_atomic_integral) {
       // integral + integral
       builder->CreateAtomicRMW(
           llvm::AtomicRMWInst::Add, output_address, source, llvm::MaybeAlign(),
-          llvm::AtomicOrdering::SequentiallyConsistent, sync_scope);
+          llvm::AtomicOrdering::Monotonic, sync_scope);
       return true;
     }
   }
@@ -437,7 +446,7 @@ bool MaybeEmitDirectAtomicOperation(llvm::IRBuilder<>* builder,
                         : llvm::AtomicRMWInst::UMax;
       builder->CreateAtomicRMW(
           opcode, output_address, source, llvm::MaybeAlign(),
-          llvm::AtomicOrdering::SequentiallyConsistent, sync_scope);
+          llvm::AtomicOrdering::Monotonic, sync_scope);
       return true;
     } else if (element_type == F32) {
       // max(float, float) via AtomicMax and AtomicMin on int
@@ -490,7 +499,7 @@ bool MaybeEmitDirectAtomicOperation(llvm::IRBuilder<>* builder,
                     builder->CreateAtomicRMW(
                         llvm::AtomicRMWInst::Max, output_address,
                         source_float_as_int, llvm::MaybeAlign(),
-                        llvm::AtomicOrdering::SequentiallyConsistent,
+                        llvm::AtomicOrdering::Monotonic,
                         sync_scope);
                   },
                   [&]() {
@@ -498,7 +507,7 @@ bool MaybeEmitDirectAtomicOperation(llvm::IRBuilder<>* builder,
                     builder->CreateAtomicRMW(
                         llvm::AtomicRMWInst::UMin, output_address,
                         source_float_as_int, llvm::MaybeAlign(),
-                        llvm::AtomicOrdering::SequentiallyConsistent,
+                        llvm::AtomicOrdering::Monotonic,
                         sync_scope);
                   });
             });
@@ -514,7 +523,7 @@ bool MaybeEmitDirectAtomicOperation(llvm::IRBuilder<>* builder,
                       ? llvm::AtomicRMWInst::Min
                       : llvm::AtomicRMWInst::UMin;
     builder->CreateAtomicRMW(opcode, output_address, source, llvm::MaybeAlign(),
-                             llvm::AtomicOrdering::SequentiallyConsistent,
+                             llvm::AtomicOrdering::Monotonic,
                              sync_scope);
     return true;
   }
@@ -683,8 +692,8 @@ absl::Status EmitAtomicOperationUsingCAS(llvm::IRBuilder<>* builder,
   //                                       cas_new_output);
   llvm::Value* ret_value = builder->CreateAtomicCmpXchg(
       atomic_memory_address, cas_old_output, cas_new_output, llvm::MaybeAlign(),
-      llvm::AtomicOrdering::SequentiallyConsistent,
-      llvm::AtomicOrdering::SequentiallyConsistent, DetermineSyncScope(module));
+      llvm::AtomicOrdering::Monotonic,
+      llvm::AtomicOrdering::Monotonic, DetermineSyncScope(module));
 
   // Extract the memory value returned from atomicCAS and store it as
   // cas_old_output.
diff --git a/xla/service/gpu/ir_emitter_unnested.cc b/xla/service/gpu/ir_emitter_unnested.cc
index 77ad59390f..c67edfb726 100644
--- a/xla/service/gpu/ir_emitter_unnested.cc
+++ b/xla/service/gpu/ir_emitter_unnested.cc
@@ -1,4 +1,6 @@
-/*Copyright 2022 The OpenXLA Authors.
+/* Copyright (c) 2024 Intel Corporation
+
+Copyright 2022 The OpenXLA Authors.
 
 Licensed under the Apache License, Version 2.0 (the "License");
 you may not use this file except in compliance with the License.
@@ -111,11 +113,14 @@ limitations under the License.
 #include "xla/service/gpu/kernels/topk_custom_kernel.h"
 #include "xla/service/gpu/launch_dimensions.h"
 #include "xla/service/gpu/matmul_utils.h"
+#include "xla/service/gpu/ccl_all_to_all_thunk.h"
+#include "xla/service/gpu/ccl_collective_broadcast_thunk.h"
+#include "xla/service/gpu/ccl_collective_permute_thunk.h"
+#include "xla/service/gpu/ccl_collective_thunk.h"
+#include "xla/service/gpu/ccl_all_gather_thunk.h"
+#include "xla/service/gpu/ccl_all_reduce_thunk.h"
 #include "xla/service/gpu/model/tiled_hlo_computation.h"
 #include "xla/service/gpu/parallel_loop_emitter.h"
-#include "xla/service/gpu/runtime/command_buffer_cmd.h"
-#include "xla/service/gpu/runtime/command_buffer_cmd_emitter.h"
-#include "xla/service/gpu/runtime/command_buffer_thunk.h"
 #include "xla/service/gpu/runtime/conditional_thunk.h"
 #include "xla/service/gpu/runtime/convolution_thunk.h"
 #include "xla/service/gpu/runtime/copy_thunk.h"
@@ -125,20 +130,19 @@ limitations under the License.
 #include "xla/service/gpu/runtime/gemm_thunk.h"
 #include "xla/service/gpu/runtime/infeed_thunk.h"
 #include "xla/service/gpu/runtime/kernel_thunk.h"
-#include "xla/service/gpu/runtime/nccl_all_gather_thunk.h"
-#include "xla/service/gpu/runtime/nccl_all_reduce_thunk.h"
-#include "xla/service/gpu/runtime/nccl_all_to_all_thunk.h"
-#include "xla/service/gpu/runtime/nccl_api.h"
-#include "xla/service/gpu/runtime/nccl_collective_broadcast_thunk.h"
-#include "xla/service/gpu/runtime/nccl_collective_permute_thunk.h"
-#include "xla/service/gpu/runtime/nccl_collective_thunk.h"
-#include "xla/service/gpu/runtime/nccl_p2p_thunk_common.h"
-#include "xla/service/gpu/runtime/nccl_recv_thunk.h"
-#include "xla/service/gpu/runtime/nccl_send_thunk.h"
+// #include "xla/service/gpu/runtime/nccl_all_gather_thunk.h"
+// #include "xla/service/gpu/runtime/nccl_all_reduce_thunk.h"
+// #include "xla/service/gpu/runtime/nccl_all_to_all_thunk.h"
+// #include "xla/service/gpu/runtime/nccl_api.h"
+// #include "xla/service/gpu/runtime/nccl_collective_broadcast_thunk.h"
+// #include "xla/service/gpu/runtime/nccl_collective_permute_thunk.h"
+// #include "xla/service/gpu/runtime/nccl_collective_thunk.h"
+// #include "xla/service/gpu/runtime/nccl_recv_thunk.h"
+// #include "xla/service/gpu/runtime/nccl_send_thunk.h"
 #include "xla/service/gpu/runtime/norm_thunk.h"
 #include "xla/service/gpu/runtime/outfeed_thunk.h"
 #include "xla/service/gpu/runtime/replica_id_thunk.h"
-#include "xla/service/gpu/runtime/send_recv_thunk.h"
+// #include "xla/service/gpu/runtime/send_recv_thunk.h"
 #include "xla/service/gpu/runtime/sequential_thunk.h"
 #include "xla/service/gpu/runtime/thunk.h"
 #include "xla/service/gpu/runtime/wait_for_streams_thunk.h"
@@ -167,16 +171,16 @@ limitations under the License.
 #include "tsl/protobuf/dnn.pb.h"
 #include "triton/Dialect/Triton/IR/Dialect.h"
 
-#if GOOGLE_CUDA || TF_HIPBLASLT
+#if GOOGLE_CUDA || TF_HIPBLASLT || TENSORFLOW_USE_SYCL
 #include "xla/service/gpu/runtime/gpublas_lt_matmul_thunk.h"
-#endif  // GOOGLE_CUDA || TF_HIPBLASLT
+#endif  // GOOGLE_CUDA || TF_HIPBLASLT || TENSORFLOW_USE_SYCL
 
-#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM
-#include "xla/service/gpu/ir_emitter_triton.h"
+#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM || TENSORFLOW_USE_SYCL
+// #include "xla/service/gpu/ir_emitter_triton.h"
 #include "xla/service/gpu/runtime/cholesky_thunk.h"
-#include "xla/service/gpu/runtime/cub_sort_thunk.h"
+// #include "xla/service/gpu/runtime/cub_sort_thunk.h"
 #include "xla/service/gpu/runtime/triangular_solve_thunk.h"
-#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM
+#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM || TENSORFLOW_USE_SYCL
 
 namespace xla {
 namespace gpu {
@@ -561,6 +565,7 @@ absl::Status IrEmitterUnnested::EmitSliceToDynamic(
 
 absl::Status IrEmitterUnnested::EmitCommandBufferThunk(
     const HloInstruction* instr) {
+#ifndef TENSORFLOW_USE_SYCL
   // Spawn a new IrEmitterUnnested to emit thunks for the command buffer
   // computation. Then convert emitted thunks to a sequence of CommandBufferCmd.
   // The resulting thunk added to the thunk sequence is a CommandBufferThunk.
@@ -587,6 +592,7 @@ absl::Status IrEmitterUnnested::EmitCommandBufferThunk(
   AddThunkToThunkSequence(std::make_unique<CommandBufferThunk>(
       std::move(cmd_sequence), Thunk::ThunkInfo::WithProfileAnnotation(instr),
       std::move(*thunk_sequence)));
+#endif  // TENSORFLOW_USE_SYCL
 
   return absl::OkStatus();
 }
@@ -963,6 +969,8 @@ absl::Status IrEmitterUnnested::EmitNormThunk(
   return absl::OkStatus();
 }
 
+#endif  // GOOGLE_CUDA
+
 absl::Status IrEmitterUnnested::EmitFusedMHAThunk(
     const HloCustomCallInstruction* instr) {
   const HloInstruction* lhs_bmm1 = instr->operand(0);
@@ -1179,8 +1187,6 @@ absl::Status IrEmitterUnnested::EmitFusedMHABackwardThunk(
   return absl::OkStatus();
 }
 
-#endif  // GOOGLE_CUDA
-
 absl::StatusOr<BufferAllocation::Slice>
 IrEmitterUnnested::GetAllocationSliceForHlo(const HloInstruction* instr,
                                             const ShapeIndex& index) const {
@@ -1188,8 +1194,8 @@ IrEmitterUnnested::GetAllocationSliceForHlo(const HloInstruction* instr,
                                       instr, index);
 }
 
-#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM
-
+#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM || TENSORFLOW_USE_SYCL
+#if 0
 absl::Status IrEmitterUnnested::EmitCubDeviceRadixSort(
     const HloCustomCallInstruction* instr) {
   if (instr->operand_count() != 1 && instr->operand_count() != 2) {
@@ -1233,7 +1239,7 @@ absl::Status IrEmitterUnnested::EmitCubDeviceRadixSort(
   AddThunkToThunkSequence(std::move(thunk));
   return absl::OkStatus();
 }
-
+#endif
 absl::Status IrEmitterUnnested::EmitCholeskyThunk(const HloInstruction* instr) {
   TF_ASSIGN_OR_RETURN(CholeskyOptions options,
                       instr->backend_config<CholeskyOptions>());
@@ -1281,7 +1287,144 @@ absl::Status IrEmitterUnnested::EmitCholeskyThunk(const HloInstruction* instr) {
 
   return absl::OkStatus();
 }
-#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM
+#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM || TENSORFLOW_USE_SYCL
+
+#ifdef TENSORFLOW_USE_SYCL
+absl::StatusOr<CustomCallThunk::AttributesMap> BuildAttributesMap(
+    const HloCustomCallInstruction* instr) {
+  CustomCallThunk::AttributesMap attrs;
+  if (IsCustomCallToDnnConvolution(*instr)) {
+    TF_ASSIGN_OR_RETURN(auto gpu_config,
+                        instr->backend_config<GpuBackendConfig>());
+    const CudnnConvBackendConfig& backend_config =
+        gpu_config.cudnn_conv_backend_config();
+    TF_ASSIGN_OR_RETURN(CudnnConvKind kind, GetCudnnConvKind(instr));
+    attrs["conv_result_scale"] =
+        static_cast<float>(backend_config.conv_result_scale());
+    attrs["side_input_scale"] =
+        static_cast<float>(backend_config.side_input_scale());
+    attrs["activation_mode"] =
+        static_cast<int32_t>(backend_config.activation_mode());
+    attrs["leakyrelu_alpha"] =
+        static_cast<float>(backend_config.leakyrelu_alpha());
+
+    const Window& window = instr->window();
+    const ConvolutionDimensionNumbers& dnums =
+        instr->convolution_dimension_numbers();
+    const int num_dimensions = window.dimensions_size();
+    const Shape& operand0_shape = instr->operand(0)->shape();
+    const Shape& operand1_shape = instr->operand(1)->shape();
+    const Shape& result_shape = instr->shape().tuple_shapes(0);
+
+    attrs["window_ShortDebugString"] = window.ShortDebugString();
+    attrs["window_num_dimensions"] = window.dimensions_size();
+    for (int i = 0; i < window.dimensions_size(); ++i) {
+      attrs["window_padding_low_" + std::to_string(i)] =
+          window.dimensions(i).padding_low();
+      attrs["window_padding_high_" + std::to_string(i)] =
+          window.dimensions(i).padding_high();
+      attrs["window_stride_" + std::to_string(i)] =
+          window.dimensions(i).stride();
+      attrs["window_dilation_" + std::to_string(i)] =
+          window.dimensions(i).window_dilation();
+    }
+
+    attrs["dnums_ShortDebugString"] = dnums.ShortDebugString();
+    attrs["input_feature_dimension"] = dnums.input_feature_dimension();
+    attrs["input_batch_dimension"] = dnums.input_batch_dimension();
+    attrs["output_feature_dimension"] = dnums.output_feature_dimension();
+    attrs["kernel_input_feature_dimension"] =
+        dnums.kernel_input_feature_dimension();
+    attrs["kernel_output_feature_dimension"] =
+        dnums.kernel_output_feature_dimension();
+    for (int i = 0; i < num_dimensions; ++i) {
+      attrs["input_spatial_dimensions_" + std::to_string(i)] =
+          dnums.input_spatial_dimensions(i);
+      attrs["output_spatial_dimensions_" + std::to_string(i)] =
+          dnums.output_spatial_dimensions(i);
+      attrs["kernel_spatial_dimensions_" + std::to_string(i)] =
+          dnums.kernel_spatial_dimensions(i);
+      attrs["kernel_spatial_dimensions_" + std::to_string(i)] =
+          dnums.kernel_spatial_dimensions(i);
+    }
+    stream_executor::dnn::DataLayout input_dl;
+    stream_executor::dnn::FilterLayout filter_dl;
+    stream_executor::dnn::DataLayout output_dl;
+    if (kind == CudnnConvKind::kForward ||
+        kind == CudnnConvKind::kForwardActivation) {
+      TF_ASSIGN_OR_RETURN(
+          std::tie(input_dl, filter_dl, output_dl),
+          XlaConvShapesToStreamExecutorLayouts(dnums, operand0_shape,
+                                               operand1_shape, result_shape));
+    } else if (kind == CudnnConvKind::kBackwardInput) {
+      TF_ASSIGN_OR_RETURN(
+          std::tie(input_dl, filter_dl, output_dl),
+          XlaConvShapesToStreamExecutorLayouts(dnums, result_shape,
+                                               operand1_shape, operand0_shape));
+    } else if (kind == CudnnConvKind::kBackwardFilter) {
+      TF_ASSIGN_OR_RETURN(
+          std::tie(input_dl, filter_dl, output_dl),
+          XlaConvShapesToStreamExecutorLayouts(dnums, operand0_shape,
+                                               result_shape, operand1_shape));
+    } else {
+      return Internal("Unkown convolution kind");
+    }
+    attrs["input_dl"] = static_cast<int32_t>(input_dl);
+    attrs["filter_dl"] = static_cast<int32_t>(filter_dl);
+    attrs["output_dl"] = static_cast<int32_t>(output_dl);
+  } else if (IsLegacyCublasMatmul(*instr) || IsCublasLtMatmul(*instr)) {
+    TF_ASSIGN_OR_RETURN(const auto gpu_config,
+                        instr->backend_config<xla::gpu::GpuBackendConfig>());
+    xla::gpu::GemmBackendConfig config = gpu_config.gemm_backend_config();
+    xla::gpu::GemmBackendConfig_Epilogue epilogue = config.epilogue();
+    attrs["epilogue"] = static_cast<int32_t>(epilogue);
+
+    TF_ASSIGN_OR_RETURN(
+        auto gemm_config,
+        GemmConfig::For(static_cast<const HloInstruction*>(instr)));
+
+    attrs["lhs_layout_dtype"] =
+        static_cast<int32_t>(gemm_config.lhs_layout.dtype);
+    attrs["lhs_order"] = static_cast<int32_t>(gemm_config.lhs_layout.order);
+    attrs["lhs_num_cols"] = gemm_config.lhs_layout.num_cols;
+    attrs["lhs_num_rows"] = gemm_config.lhs_layout.num_rows;
+    attrs["lhs_batch_stride"] = gemm_config.lhs_layout.batch_stride;
+    attrs["lhs_leading_dim_stride"] = gemm_config.lhs_layout.leading_dim_stride;
+
+    attrs["rhs_layout_dtype"] =
+        static_cast<int32_t>(gemm_config.rhs_layout.dtype);
+    attrs["rhs_order"] = static_cast<int32_t>(gemm_config.rhs_layout.order);
+    attrs["rhs_num_cols"] = gemm_config.rhs_layout.num_cols;
+    attrs["rhs_num_rows"] = gemm_config.rhs_layout.num_rows;
+    attrs["rhs_batch_stride"] = gemm_config.rhs_layout.batch_stride;
+    attrs["rhs_leading_dim_stride"] = gemm_config.rhs_layout.leading_dim_stride;
+
+    attrs["output_layout_dtype"] =
+        static_cast<int32_t>(gemm_config.output_layout.dtype);
+    attrs["output_order"] =
+        static_cast<int32_t>(gemm_config.output_layout.order);
+    attrs["output_num_cols"] = gemm_config.output_layout.num_cols;
+    attrs["output_num_rows"] = gemm_config.output_layout.num_rows;
+    attrs["output_batch_stride"] = gemm_config.output_layout.batch_stride;
+    attrs["output_leading_dim_stride"] =
+        gemm_config.output_layout.leading_dim_stride;
+
+    attrs["batch_size"] =
+        static_cast<int64_t>(gemm_config.output_layout.batch_size);
+    attrs["alpha"] = static_cast<float>(gemm_config.alpha.real());
+    attrs["beta"] = static_cast<float>(gemm_config.beta);
+
+    // config.algorithm is less than 0, thus 0 means no algorithm
+    if (gemm_config.algorithm.has_value()) {
+      attrs["algorithm"] = static_cast<int64_t>(gemm_config.algorithm.value());
+    } else
+      attrs["algorithm"] = static_cast<int64_t>(0);
+  } else {
+    return absl::InternalError("Unknown CustomCall To SYCL FFI Call");
+  }
+  return attrs;
+}
+#endif  // TENSORFLOW_USE_SYCL
 
 absl::Status IrEmitterUnnested::EmitCustomCallThunk(
     const HloCustomCallInstruction* instr) {
@@ -1405,6 +1548,12 @@ absl::Status IrEmitterUnnested::EmitCustomCallThunk(
       break;
 
     case CustomCallApiVersion::API_VERSION_TYPED_FFI:
+#ifdef TENSORFLOW_USE_SYCL
+    {
+      TF_ASSIGN_OR_RETURN(attributes, BuildAttributesMap(instr));
+      break;
+    }
+#else
       if (!backend_config_str.empty()) {
         mlir::Attribute attr = mlir::parseAttribute(
             backend_config_str, ir_emitter_context_->mlir_context());
@@ -1417,7 +1566,7 @@ absl::Status IrEmitterUnnested::EmitCustomCallThunk(
             "dictionary attribute");
       }
       break;
-
+#endif  // TENSORFLOW_USE_SYCL
     default:
       return Internal("Unknown custom-call API version enum value: %d",
                       instr->api_version());
@@ -1458,7 +1607,7 @@ absl::Status IrEmitterUnnested::EmitFftThunk(const HloFftInstruction* instr) {
   return absl::OkStatus();
 }
 
-#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM
+#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM || TENSORFLOW_USE_SYCL
 
 absl::Status IrEmitterUnnested::EmitTriangularSolveCustomCall(
     const HloInstruction* instr) {
@@ -1538,7 +1687,7 @@ absl::Status IrEmitterUnnested::EmitTriangularSolveCustomCall(
   }
   return absl::OkStatus();
 }
-#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM
+#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM || TENSORFLOW_USE_SYCL
 
 absl::Status IrEmitterUnnested::EmitTopKCustomCall(
     const HloCustomCallInstruction* instr) {
@@ -2368,6 +2517,7 @@ absl::Status IrEmitterUnnested::EmitNcclThunk(
 
 absl::Status IrEmitterUnnested::EmitNcclAsyncDone(Thunk::Kind kind,
                                                   const HloInstruction* inst) {
+#if 0  
   CollectivesAsyncEvents& collectives_async_events =
       GetCollectivesAsyncEvents();
   if (kind == Thunk::Kind::kNcclRecvDone ||
@@ -2400,6 +2550,7 @@ absl::Status IrEmitterUnnested::EmitNcclAsyncDone(Thunk::Kind kind,
         kind, Thunk::ThunkInfo::WithProfileAnnotation(inst),
         std::move(async_events.mapped()), AsyncStreamKind::kCollective));
   }
+#endif
   return absl::OkStatus();
 }
 
@@ -2639,6 +2790,7 @@ absl::Status IrEmitterUnnested::EmitCopyDoneThunk(const HloInstruction* instr) {
 }
 
 absl::Status IrEmitterUnnested::EmitSendThunk(const HloSendInstruction* instr) {
+#if 0
   if (!instr->channel_id().has_value())
     return absl::InternalError("Unknown send instruction channel id");
 
@@ -2691,12 +2843,14 @@ absl::Status IrEmitterUnnested::EmitSendThunk(const HloSendInstruction* instr) {
       *instr->channel_id(), send_recv_events_,
       ConvertFrontendAttributes(instr->frontend_attributes()),
       DeviceConstraint(instr)));
+#endif  // 0
 
-  return absl::OkStatus();
+  return absl::UnimplementedError("SendThunk unsupported in SYCL");
 }
 
 absl::Status IrEmitterUnnested::EmitSendDoneThunk(
     const HloSendDoneInstruction* instr) {
+#if 0
   if (!instr->channel_id().has_value())
     return absl::InternalError("Unknown send done instruction channel id");
 
@@ -2707,11 +2861,13 @@ absl::Status IrEmitterUnnested::EmitSendDoneThunk(
   AddThunkToThunkSequence(std::make_unique<SendDoneThunk>(
       Thunk::ThunkInfo::WithProfileAnnotation(instr), *instr->channel_id(),
       send_recv_events_, DeviceConstraint(instr)));
+#endif  // 0
 
-  return absl::OkStatus();
+  return absl::UnimplementedError("SendDoneThunk unsupported in SYCL");
 }
 
 absl::Status IrEmitterUnnested::EmitRecvThunk(const HloRecvInstruction* instr) {
+#if 0
   if (!instr->channel_id().has_value())
     return absl::InternalError("Unknown recv instruction channel id");
   TF_RET_CHECK(instr->shape().IsTuple());
@@ -2766,11 +2922,13 @@ absl::Status IrEmitterUnnested::EmitRecvThunk(const HloRecvInstruction* instr) {
       ConvertFrontendAttributes(instr->frontend_attributes()),
       DeviceConstraint(instr)));
 
-  return absl::OkStatus();
+#endif  // 0
+  return absl::UnimplementedError("RecvThunk unsupported in SYCL");
 }
 
 absl::Status IrEmitterUnnested::EmitRecvDoneThunk(
     const HloRecvDoneInstruction* instr) {
+#if 0
   if (!instr->channel_id().has_value())
     return absl::InternalError("Unknown recv done instruction channel id");
 
@@ -2781,8 +2939,9 @@ absl::Status IrEmitterUnnested::EmitRecvDoneThunk(
   AddThunkToThunkSequence(std::make_unique<RecvDoneThunk>(
       Thunk::ThunkInfo::WithProfileAnnotation(instr), *instr->channel_id(),
       send_recv_events_, DeviceConstraint(instr)));
+#endif  // 0
 
-  return absl::OkStatus();
+  return absl::UnimplementedError("RecvDoneThunk unsupported in SYCL");
 }
 
 absl::Status IrEmitterUnnested::EmitHloInstruction(
@@ -2896,47 +3055,67 @@ absl::Status IrEmitterUnnested::EmitHloInstruction(
     case HloOpcode::kCustomCall: {
       auto* custom_call = Cast<HloCustomCallInstruction>(instr);
       if (IsLegacyCublasMatmul(*instr)) {
+#if TENSORFLOW_USE_SYCL
+        const_cast<HloCustomCallInstruction*>(custom_call)
+            ->set_api_version(CustomCallApiVersion::API_VERSION_TYPED_FFI);
+        return EmitCustomCallThunk(custom_call);
+#else
         return EmitGemmThunk(custom_call);
+#endif
       }
-#if GOOGLE_CUDA || TF_HIPBLASLT
+#if GOOGLE_CUDA || TF_HIPBLASLT || TENSORFLOW_USE_SYCL
       if (IsCublasLtMatmul(*instr)) {
+#if TENSORFLOW_USE_SYCL
+        const_cast<HloCustomCallInstruction*>(custom_call)
+            ->set_api_version(CustomCallApiVersion::API_VERSION_TYPED_FFI);
+        return EmitCustomCallThunk(custom_call);
+#else
         return EmitCublasLtMatmulThunk(custom_call);
+#endif
       }
+#endif  // GOOGLE_CUDA || TF_HIPBLASLT || TENSORFLOW_USE_SYCL
+#if GOOGLE_CUDA
       if (IsCublasLtMatmulF8(*instr)) {
         return EmitCublasLtMatmulThunkF8(custom_call);
       }
-#endif  // GOOGLE_CUDA || TF_HIPBLASLT
-#if GOOGLE_CUDA
       if (IsCudnnConvolutionReorder(*instr)) {
         return EmitConvolutionReorderThunk(custom_call);
       }
       if (IsCustomCallToDnnNorm(*instr)) {
         return EmitNormThunk(custom_call);
       }
+#endif  // GOOGLE_CUDA
       if (IsFwdCustomCallTofMHA(*instr)) {
         return EmitFusedMHAThunk(custom_call);
       }
       if (IsBwdCustomCallTofMHA(*instr)) {
         return EmitFusedMHABackwardThunk(custom_call);
       }
-#endif  // GOOGLE_CUDA
       if (IsCustomCallToTopK(*instr)) {
         return EmitTopKCustomCall(custom_call);
       }
       if (IsCustomCallToDnnConvolution(*instr)) {
+#ifdef TENSORFLOW_USE_SYCL
+        const_cast<HloCustomCallInstruction*>(custom_call)
+            ->set_api_version(CustomCallApiVersion::API_VERSION_TYPED_FFI);
+        return EmitCustomCallThunk(custom_call);
+#else
         return EmitConvolutionThunk(custom_call);
+#endif
       }
-#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM
+#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM || TENSORFLOW_USE_SYCL
       if (IsCustomCallToCusolver(*instr)) {
         return EmitCholeskyThunk(instr);
       }
       if (IsTriangularSolve(*instr)) {
         return EmitTriangularSolveCustomCall(instr);
       }
+#if 0
       if (IsCubDeviceRadixSort(*instr)) {
         return EmitCubDeviceRadixSort(custom_call);
       }
-#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM
+#endif
+#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM || TENSORFLOW_USE_SYCL
       if (custom_call->custom_call_target() == "PadToStatic") {
         return EmitPadToStatic(custom_call);
       }
diff --git a/xla/service/gpu/ir_emitter_unnested.h b/xla/service/gpu/ir_emitter_unnested.h
index 5774dbe96e..0937d1a6d5 100644
--- a/xla/service/gpu/ir_emitter_unnested.h
+++ b/xla/service/gpu/ir_emitter_unnested.h
@@ -1,4 +1,6 @@
-/* Copyright 2018 The OpenXLA Authors.
+/* Copyright (c) 2024 Intel Corporation
+
+Copyright 2019 The TensorFlow Authors. All Rights Reserved.
 
 Licensed under the Apache License, Version 2.0 (the "License");
 you may not use this file except in compliance with the License.
@@ -137,6 +139,8 @@ class IrEmitterUnnested : public IrEmitter {
   absl::Status EmitConditional(const HloInstruction* instr);
   absl::Status EmitConvolutionThunk(const HloCustomCallInstruction* instr);
   absl::Status EmitGemmThunk(const HloCustomCallInstruction* instr);
+  absl::Status EmitFusedMHAThunk(const HloCustomCallInstruction* instr);
+  absl::Status EmitFusedMHABackwardThunk(const HloCustomCallInstruction* instr);
 #if GOOGLE_CUDA || TF_HIPBLASLT
   absl::Status EmitCublasLtMatmulThunk(const HloCustomCallInstruction* instr);
   absl::Status EmitCublasLtMatmulThunkF8(const HloCustomCallInstruction* instr);
@@ -145,13 +149,11 @@ class IrEmitterUnnested : public IrEmitter {
   absl::Status EmitConvolutionReorderThunk(
       const HloCustomCallInstruction* instr);
   absl::Status EmitNormThunk(const HloCustomCallInstruction* instr);
-  absl::Status EmitFusedMHAThunk(const HloCustomCallInstruction* instr);
-  absl::Status EmitFusedMHABackwardThunk(const HloCustomCallInstruction* instr);
 #endif  // GOOGLE_CUDA
-#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM
+#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM || TENSORFLOW_USE_SYCL
   absl::Status EmitCubDeviceRadixSort(const HloCustomCallInstruction* instr);
   absl::Status EmitCholeskyThunk(const HloInstruction* instr);
-#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM
+#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM || TENSORFLOW_USE_SYCL
   absl::Status EmitCustomCallThunk(const HloCustomCallInstruction* instr);
   absl::Status EmitFftThunk(const HloFftInstruction* instr);
   absl::Status EmitFusion(const HloFusionInstruction* instr);
@@ -165,9 +167,9 @@ class IrEmitterUnnested : public IrEmitter {
       const HloRngGetAndUpdateStateInstruction* instr);
 
   absl::Status EmitSort(const HloSortInstruction* sort);
-#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM
+#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM || TENSORFLOW_USE_SYCL
   absl::Status EmitTriangularSolveCustomCall(const HloInstruction* instr);
-#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM
+#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM || TENSORFLOW_USE_SYCL
   absl::Status EmitTopKCustomCall(const HloCustomCallInstruction* instr);
   absl::Status EmitTritonCustomCall(const HloCustomCallInstruction* instr);
 
diff --git a/xla/service/gpu/kernels/BUILD b/xla/service/gpu/kernels/BUILD
index 4652f8cafd..4145b64073 100644
--- a/xla/service/gpu/kernels/BUILD
+++ b/xla/service/gpu/kernels/BUILD
@@ -149,26 +149,27 @@ cc_library(
         "@tsl//tsl/platform:errors",
         "@tsl//tsl/platform:logging",
         "@tsl//tsl/platform:statusor",
-    ] + if_gpu_is_configured([
-        ":topk_kernel_gpu",
-    ]),
+    ]
+    # ] + if_gpu_is_configured([
+    #     ":topk_kernel_gpu",
+    # ]),
 )
 
-gpu_kernel_library(
-    name = "topk_kernel_gpu",
-    srcs = if_gpu_is_configured([
-        "topk_kernel_bfloat16.cu.cc",
-        "topk_kernel_float.cu.cc",
-        "topk_kernel.cu.h",
-    ]),
-    hdrs = if_gpu_is_configured(["topk_kernel_common.h"]),
-    compatible_with = [],
-    deps = [
-        "//xla:types",
-        "//xla/stream_executor/gpu:gpu_types_header",
-        "@tsl//tsl/lib/math:math_util",
-    ],
-)
+# gpu_kernel_library(
+#     name = "topk_kernel_gpu",
+#     srcs = if_gpu_is_configured([
+#         "topk_kernel_bfloat16.cu.cc",
+#         "topk_kernel_float.cu.cc",
+#         "topk_kernel.cu.h",
+#     ]),
+#     hdrs = if_gpu_is_configured(["topk_kernel_common.h"]),
+#     compatible_with = [],
+#     deps = [
+#         "//xla:types",
+#         "//xla/stream_executor/gpu:gpu_types_header",
+#         "@tsl//tsl/lib/math:math_util",
+#     ],
+# )
 
 xla_test(
     name = "topk_kernel_test",
@@ -215,9 +216,10 @@ cc_library(
         "@com_google_absl//absl/status:statusor",
         "@com_google_absl//absl/strings",
         "@tsl//tsl/platform:statusor",
-    ] + if_gpu_is_configured([
-        ":topk_kernel_gpu",
-    ]),
+    ]
+    # ] + if_gpu_is_configured([
+    #     ":topk_kernel_gpu",
+    # ]),
 )
 
 xla_test(
diff --git a/xla/service/gpu/launch_dimensions.cc b/xla/service/gpu/launch_dimensions.cc
index b31b0e532c..dd106364c5 100644
--- a/xla/service/gpu/launch_dimensions.cc
+++ b/xla/service/gpu/launch_dimensions.cc
@@ -95,6 +95,9 @@ struct BlockSizes {
 BlockSizes GetBlockSizes(LaunchDimensionsConfig dim_config,
                          const se::DeviceDescription& gpu_device_info,
                          const Shape& shape, int64_t num_elements) {
+#if !TENSORFLOW_USE_SYCL
+  // TODO: It will set 128 threads per block by default. We prefer to use
+  // the max value (1024 on PVC). It can benefit instructions like scatter.
   if (!dim_config.row_vectorized && !dim_config.few_waves) {
     BlockSizes result;
     const int kWarpSchedulers = 4;
@@ -105,6 +108,7 @@ BlockSizes GetBlockSizes(LaunchDimensionsConfig dim_config,
         num_elements, result.threads_per_block_x * result.threads_per_block_y);
     return result;
   }
+#endif
 
   int64_t threads_per_block_row_vectorized =
       ThreadsPerBlockRowVectorized(shape, gpu_device_info, dim_config);
diff --git a/xla/service/gpu/llvm_gpu_backend/BUILD b/xla/service/gpu/llvm_gpu_backend/BUILD
index cacdbf6781..a3e4868a99 100644
--- a/xla/service/gpu/llvm_gpu_backend/BUILD
+++ b/xla/service/gpu/llvm_gpu_backend/BUILD
@@ -6,6 +6,10 @@ load(
     "@tsl//tsl/platform/default:cuda_build_defs.bzl",
     "if_cuda_is_configured",
 )
+load(
+    "@local_config_sycl//sycl:build_defs.bzl",
+    "if_sycl_is_configured",
+)
 load("//xla:xla.bzl", "xla_cc_test")
 load("//xla/tsl:tsl.bzl", "internal_visibility")
 
@@ -44,6 +48,7 @@ cc_library(
         "//xla/service/gpu:metrics",
         "//xla/service/llvm_ir:llvm_command_line_options",
         "//xla/service/llvm_ir:llvm_type_conversion_util",
+        "//xla/service/llvm_ir:llvm_util",
         "//xla/stream_executor:device_description",
         "//xla/tsl/util:env_var",
         "@com_google_absl//absl/base",
@@ -89,6 +94,8 @@ cc_library(
         "@local_config_rocm//rocm:rocm_headers",
         "@llvm-project//llvm:AMDGPUCodeGen",
         "@llvm-project//llvm:AMDGPUAsmParser",
+    ]) + if_sycl_is_configured([
+        "@llvm_spir//:llvm_spir_translator",
     ]),
 )
 
diff --git a/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc b/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc
index a3ed5f71fb..69a3977d7d 100644
--- a/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc
+++ b/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc
@@ -80,6 +80,8 @@ limitations under the License.
 #include "xla/service/gpu/metrics.h"
 #include "xla/service/llvm_ir/llvm_command_line_options.h"
 #include "xla/service/llvm_ir/llvm_type_conversion_util.h"
+#include "xla/service/llvm_ir/llvm_util.h"
+#include "xla/status_macros.h"
 #include "xla/stream_executor/device_description.h"
 #include "xla/tsl/util/env_var.h"
 #include "xla/util.h"
@@ -100,6 +102,11 @@ limitations under the License.
 #include "rocm/rocm_config.h"
 #endif
 
+#if TENSORFLOW_USE_SYCL
+#include "LLVMSPIRVLib.h"
+#include "LLVMSPIRVOpts.h"
+#endif
+
 #if GOOGLE_CUDA
 #include "third_party/gpus/cuda/include/cuda.h"
 #include "xla/stream_executor/cuda/cuda_asm_compiler.h"
@@ -452,10 +459,13 @@ absl::Status LinkAndOptimizeModule(
   llvm::CGSCCAnalysisManager cgam;
   llvm::ModuleAnalysisManager mam;
 
-  fam.registerPass([&] { return target_machine->getTargetIRAnalysis(); });
+  if (target_machine)
+    fam.registerPass([&] { return target_machine->getTargetIRAnalysis(); });
 
+  // SYCL: customized config
   llvm::PipelineTuningOptions pto;
-  pto.SLPVectorization = true;
+  pto.SLPVectorization = false;
+  pto.LoopVectorization = false;
   pto.InlinerThreshold = inline_threshold;
 
   llvm::PassInstrumentationCallbacks pic;
@@ -1131,5 +1141,107 @@ absl::StatusOr<std::vector<uint8_t>> CompileToHsaco(
 
 }  // namespace amdgpu
 
+namespace {
+std::unique_ptr<llvm::TargetMachine> SPIRGetTargetMachine(
+    llvm::Triple target_triple, se::GpuComputeCapability gpu_version,
+    const DebugOptions& debug_options) {
+  return nullptr;
+}
+
+Status SPIRTargetModuleLinker(llvm::Module* module,
+                              se::GpuComputeCapability gpu_version,
+                              const DebugOptions& debug_options,
+                              const std::string& device_bitcode_dir_path) {
+  return OkStatus();
+}
+
+StatusOr<std::string> EmitModuleToSpir(llvm::Module* module,
+                                       se::GpuComputeCapability gpu_version,
+                                       const DebugOptions& debug_options) {
+  SPIRV::TranslatorOpts::ExtensionsStatusMap ExtensionsStatus;
+  SPIRV::TranslatorOpts opts(SPIRV::VersionNumber::MaximumVersion,
+                             ExtensionsStatus);
+  opts.enableAllExtensions();  // enable all SPIR-V extension first
+
+  std::ostringstream oss;
+  std::string err;
+  bool success = llvm::writeSpirv(module, opts, oss, err);
+  if (!success) {
+    return xla::Internal("Fails to convert LLVM as SPIR-V: %s", err);
+  }
+  return oss.str();
+}
+
+void SPIRBackendInit(const DebugOptions& debug_options) {
+
+  FeedLLVMWithFlags({"-slp-vectorize-hor=false"});
+
+  bool vec = true;
+  tsl::ReadBoolFromEnvVar("VECTORIZE", true, &vec);
+  if (vec) {
+    FeedLLVMWithFlags({
+        "-slp-min-reg-size=64",
+        "-slp-max-reg-size=64",
+    });
+  } else {
+    // TODO: sycl-opt disables all LLVM vectorization passes. Evaluate if it is
+    // needed.
+    FeedLLVMWithFlags({"-sycl-opt=1"});
+  }
+
+  llvm_ir::InitializeLLVMCommandLineOptions(
+      debug_options.xla_backend_extra_options());
+
+  llvm::PassRegistry* registry = llvm::PassRegistry::getPassRegistry();
+  InitializePasses(registry);
+}
+}  // namespace
+
+namespace spir {
+absl::StatusOr<std::vector<uint8_t>> CompileToSpir(
+    llvm::Module* module, se::GpuComputeCapability gpu_version,
+    const DebugOptions& debug_options) {
+  std::string libdevice_dir_path;
+  static absl::once_flag backend_init_flag;
+  absl::call_once(backend_init_flag, SPIRBackendInit, debug_options);
+
+  std::string spir;
+  {
+    XLA_SCOPED_LOGGING_TIMER("Compile module " + module->getName().str());
+
+    // If the module has no functions or globals, there's nothing to compile.
+    if (module->empty() && module->global_empty()) {
+      VLOG(2) << "Module '" << module->getName().str()
+              << "' is empty. Skipping compilation.";
+      return std::vector<uint8_t>();
+    }
+
+    // No SPIR target machine?
+    llvm::Triple default_target_triple("spir64-unknown-unknown");
+    std::unique_ptr<llvm::TargetMachine> target_machine =
+        SPIRGetTargetMachine(default_target_triple, gpu_version, debug_options);
+
+    bool opt = true;
+    tsl::ReadBoolFromEnvVar("SYCL_LLVM_OPT", true, &opt);
+    if (opt) {
+      // Link with libdevice, and optimize the LLVM module.
+      TF_RETURN_IF_ERROR(LinkAndOptimizeModule(
+          module, gpu_version, debug_options, libdevice_dir_path,
+          SPIRTargetModuleLinker, default_target_triple, target_machine.get(),
+          kDefaultInlineThreshold));
+    }
+
+#if 0
+    LOG(ERROR) << "Optimized IR before converting to spir\n" << llvm_ir::DumpToString(module);
+#endif
+
+    // Lower optimized LLVM module to SPIR.
+    TF_ASSIGN_OR_RETURN(spir,
+                        EmitModuleToSpir(module, gpu_version, debug_options));
+  }
+  return std::vector<uint8_t>(spir.begin(), spir.end());
+}
+}  // namespace spir
+
 }  // namespace gpu
 }  // namespace xla
diff --git a/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.h b/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.h
index 3d67bf043e..be34da59d0 100644
--- a/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.h
+++ b/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.h
@@ -73,6 +73,12 @@ absl::StatusOr<std::vector<uint8_t>> CompileToHsaco(
     const std::string& module_config_cache_key);
 }  // namespace amdgpu
 
+namespace spir {
+absl::StatusOr<std::vector<uint8_t>> CompileToSpir(
+    llvm::Module* module, se::GpuComputeCapability gpu_version,
+    const DebugOptions& debug_options);
+}  // namespace spir
+
 }  // namespace gpu
 }  // namespace xla
 
diff --git a/xla/service/gpu/make_batch_pointers.cc b/xla/service/gpu/make_batch_pointers.cc
index f2516742e1..77c6deefb2 100644
--- a/xla/service/gpu/make_batch_pointers.cc
+++ b/xla/service/gpu/make_batch_pointers.cc
@@ -44,6 +44,7 @@ namespace make_batch_pointers {
 void* kernel();  // returns a pointer to a CUDA C++ device function
 }  // namespace make_batch_pointers
 
+#if !TENSORFLOW_USE_SYCL
 absl::Status MakeBatchPointers(se::Stream* stream,
                                se::DeviceMemoryBase base_ptr,
                                size_t stride_bytes, size_t n,
@@ -71,5 +72,14 @@ absl::Status MakeBatchPointers(se::Stream* stream,
 #endif
   return absl::OkStatus();
 }
+#else
+absl::Status MakeBatchPointers(se::Stream* stream,
+                               const se::DeviceMemoryBase& base_ptr,
+                               size_t stride_bytes, size_t n,
+                               se::DeviceMemoryBase& ptrs_out) {
+  ptrs_out = base_ptr;
+  return absl::OkStatus();
+}
+#endif // !TENSORFLOW_USE_SYCL
 
 }  // namespace xla::gpu
diff --git a/xla/service/gpu/make_batch_pointers.h b/xla/service/gpu/make_batch_pointers.h
index 6e437fafdc..b95a2cabfa 100644
--- a/xla/service/gpu/make_batch_pointers.h
+++ b/xla/service/gpu/make_batch_pointers.h
@@ -49,11 +49,17 @@ namespace xla::gpu {
 //    driver and slow down *all* work on the GPU.  So to do this right, we'd
 //    need to allocate the host memory as pinned, one alloc per stream.  Then
 //    we'd need to manage this memory without leaks.  This becomes complex!
+#if !TENSORFLOW_USE_SYCL
 absl::Status MakeBatchPointers(se::Stream* stream,
                                se::DeviceMemoryBase base_ptr,
                                size_t stride_bytes, size_t n,
                                se::DeviceMemoryBase ptrs_out);
-
+#else
+absl::Status MakeBatchPointers(se::Stream* stream,
+                               const se::DeviceMemoryBase& base_ptr,
+                               size_t stride_bytes, size_t n,
+                               se::DeviceMemoryBase& ptrs_out);
+#endif
 }  // namespace xla::gpu
 
 #endif  // XLA_SERVICE_GPU_MAKE_BATCH_POINTERS_H_
diff --git a/xla/service/gpu/matmul_utils.h b/xla/service/gpu/matmul_utils.h
index 22d7f17813..5d64dfd606 100644
--- a/xla/service/gpu/matmul_utils.h
+++ b/xla/service/gpu/matmul_utils.h
@@ -40,6 +40,23 @@ limitations under the License.
 #include "rocm/rocm_config.h"
 #endif
 
+namespace stream_executor {
+namespace cuda {
+namespace BlasLt {
+enum class Epilogue {
+  kDefault = 1,                   // No special postprocessing
+  kReLU = 2,                      // Apply point-wise ReLU function
+  kBias = 4,                      // Add broadcasted bias vector
+  kBiasThenReLU = kBias | kReLU,  // Apply bias and then ReLU transform
+  kGELU = 32,                // Apply GELU point-wise transform to the results
+  kGELUWithAux = 32 | 1024,  // Apply GELU with auxiliary output.
+  kBiasThenGELU = kBias | kGELU,  // Apply bias and then approximate GELU.
+  kBiasThenGELUWithAux = kBiasThenGELU | 1024,
+};
+}
+}  // namespace cuda
+}  // namespace stream_executor
+
 namespace xla {
 namespace gpu {
 
diff --git a/xla/service/gpu/model/gpu_performance_model_base.cc b/xla/service/gpu/model/gpu_performance_model_base.cc
index 2d0fc0ab3d..5940bcf7ce 100644
--- a/xla/service/gpu/model/gpu_performance_model_base.cc
+++ b/xla/service/gpu/model/gpu_performance_model_base.cc
@@ -77,8 +77,12 @@ int GetCoalescingWasteFactor(PrimitiveType element_type,
 // (1830 MHz) to saturate the memory bandwidth (3.35 TB/s).
 float AdjustBandwidth(const se::DeviceDescription& gpu_device_info,
                       float bandwidth, int64_t num_blocks) {
+#if TENSORFLOW_USE_SYCL
+  float per_block_bandwidth = gpu_device_info.clock_rate_ghz() * 1.0e9f * 64;
+#else
   float per_block_bandwidth = gpu_device_info.clock_rate_ghz() * 1.0e9f *
                               gpu_device_info.memory_transactions_per_clock();
+#endif
   float max_bandwidth = num_blocks * per_block_bandwidth;
 
   return std::min(bandwidth, max_bandwidth);
@@ -176,6 +180,11 @@ LaunchDimensions GpuPerformanceModelBase::EstimateFusionLaunchDimensions(
   VLOG(5) << "Using fallback launch dimensions estimate for "
           << fusion_analysis.fusion().ToString();
   int64_t num_threads_per_block = 128;
+#if TENSORFLOW_USE_SYCL
+  const se::DeviceDescription device_info = fusion_analysis.device_info();
+  num_threads_per_block =
+      RoundUpTo(device_info.threads_per_block_limit(), int64_t{32});
+#endif
   int64_t estimated_num_threads =
       ShapeUtil::ElementsInRecursive(fusion_analysis.fusion_root(0).shape());
   int64_t num_blocks =
diff --git a/xla/service/gpu/model/gpu_performance_model_base.h b/xla/service/gpu/model/gpu_performance_model_base.h
index bd33ce14b4..0cd63f3b33 100644
--- a/xla/service/gpu/model/gpu_performance_model_base.h
+++ b/xla/service/gpu/model/gpu_performance_model_base.h
@@ -139,7 +139,11 @@ class GpuPerformanceModelBase {
   };
 
   // Estimated values in the absence of easy ways to query them.
+#if TENSORFLOW_USE_SYCL
+  static constexpr absl::Duration kKernelLaunchOverhead = absl::Microseconds(5);
+#else
   static constexpr absl::Duration kKernelLaunchOverhead = absl::Microseconds(1);
+#endif
   static constexpr absl::Duration kNcclKernelLaunchOverhead =
       absl::Microseconds(5);
   static constexpr float kL2CacheSpeedup = 2.5;
diff --git a/xla/service/gpu/reduction_utils.h b/xla/service/gpu/reduction_utils.h
index 69a68aebe5..2ca2600fc9 100644
--- a/xla/service/gpu/reduction_utils.h
+++ b/xla/service/gpu/reduction_utils.h
@@ -103,4 +103,4 @@ bool AreReductionsMultiOutputFusionCompatible(
 
 }  // namespace gpu
 }  // namespace xla
-#endif  // XLA_SERVICE_GPU_REDUCTION_UTILS_H_
+#endif  // XLA_SERVICE_GPU_REDUCTION_UTILS_H_
\ No newline at end of file
diff --git a/xla/service/gpu/runtime/BUILD b/xla/service/gpu/runtime/BUILD
index c602ba586d..b6860821a1 100644
--- a/xla/service/gpu/runtime/BUILD
+++ b/xla/service/gpu/runtime/BUILD
@@ -180,56 +180,58 @@ xla_test(
 # have `if_nccl` and `if_gpu_configured` that do not compose. NCCL header included directly in
 # :nccl_api target and all other targets should use this header to launch collective operations.
 # This allows to minimize the spreading of #ifdef all over the XLA code base.
-alias(
-    name = "nccl_api",
-    actual = if_nccl(":_nccl_api_impl", ":_nccl_api_stub"),
-)
+# alias(
+#     name = "nccl_api",
+#     actual = if_nccl(":_nccl_api_impl", ":_nccl_api_stub"),
+# )
 
-cc_library(
-    name = "_nccl_api_impl",
-    srcs = if_gpu_is_configured(
-        ["nccl_api.cc"],
-        ["nccl_api_stub.cc"],
-    ),
-    hdrs = ["nccl_api.h"],
-    compatible_with = get_compatible_with_portable(),
-    deps = [
-        ":nccl_clique_key",
-        "//xla:shape_util",
-        "//xla:xla_data_proto_cc",
-        "//xla/service:collective_ops_utils",
-        "//xla/stream_executor",
-        "//xla/stream_executor/gpu:gpu_activation",
-        "//xla/tsl/concurrency:ref_count",
-        "@com_google_absl//absl/algorithm:container",
-        "@com_google_absl//absl/container:btree",
-        "@com_google_absl//absl/hash",
-        "@com_google_absl//absl/status",
-        "@com_google_absl//absl/status:statusor",
-        "@com_google_absl//absl/strings",
-        "@com_google_absl//absl/strings:str_format",
-        "@com_google_absl//absl/types:span",
-        "@tsl//tsl/platform:errors",
-        "@tsl//tsl/platform:logging",
-        "@tsl//tsl/platform:statusor",
-    ] + if_cuda_is_configured([
-        "@local_config_nccl//:nccl",
-        "//xla/stream_executor/cuda:cuda_driver",
-        "//xla/stream_executor/cuda:cuda_executor",
-    ]) + if_rocm_is_configured([
-        "@local_config_rocm//rocm:rocm_headers",
-        "@local_config_rocm//rocm:rccl",
-        "//xla/stream_executor/rocm:rocm_driver",
-        "//xla/stream_executor/rocm:rocm_executor",
-    ]) + if_gpu_is_configured([
-        "//xla/stream_executor/gpu:gpu_stream",
-    ]),
-)
+# cc_library(
+#     name = "_nccl_api_impl",
+#     srcs = if_gpu_is_configured(
+#         ["nccl_api.cc"],
+#         ["nccl_api_stub.cc"],
+#     ),
+#     hdrs = ["nccl_api.h"],
+#     compatible_with = get_compatible_with_portable(),
+#     deps = [
+#         "//xla:shape_util",
+#         "//xla:xla_data_proto_cc",
+#         "//xla/service:collective_ops_utils",
+#         "//xla/service/gpu:nccl_clique_key",
+#         "//xla/stream_executor",
+#         "//xla/stream_executor/gpu:gpu_activation",
+#         "@com_google_absl//absl/algorithm:container",
+#         "@com_google_absl//absl/container:btree",
+#         "@com_google_absl//absl/hash",
+#         "@com_google_absl//absl/status",
+#         "@com_google_absl//absl/status:statusor",
+#         "@com_google_absl//absl/strings",
+#         "@com_google_absl//absl/strings:str_format",
+#         "@com_google_absl//absl/types:span",
+#         "@tsl//tsl/concurrency:ref_count",
+#         "@tsl//tsl/platform:errors",
+#         "@tsl//tsl/platform:logging",
+#         "@tsl//tsl/platform:statusor",
+#     ] + if_cuda_is_configured([
+#         "@local_config_nccl//:nccl",
+#         "//xla/stream_executor/cuda:cuda_driver",
+#         "//xla/stream_executor/cuda:cuda_executor",
+#     ]) + if_rocm_is_configured([
+#         "@local_config_rocm//rocm:rccl",
+#         "//xla/stream_executor/rocm:rocm_driver",
+#         "//xla/stream_executor/rocm:rocm_executor",
+#     ]) + if_gpu_is_configured([
+#         "//xla/stream_executor/gpu:gpu_stream",
+#     ]),
+# )
 
 cc_library(
-    name = "_nccl_api_stub",
-    srcs = ["nccl_api_stub.cc"],
-    hdrs = ["nccl_api.h"],
+    name = "nccl_api",
+    srcs = ["ccl_api.cc"],
+    hdrs = [
+        "nccl_api.h",
+        "ccl_api.h",
+    ],
     compatible_with = get_compatible_with_portable(),
     deps = [
         ":nccl_clique_key",
@@ -237,7 +239,9 @@ cc_library(
         "//xla:xla_data_proto_cc",
         "//xla/service:collective_ops_utils",
         "//xla/stream_executor",
+        "//xla/stream_executor/gpu:gpu_stream",
         "//xla/tsl/concurrency:ref_count",
+        "@intel_extension_for_openxla//xla/service/gpu:ccl_ops",
         "@com_google_absl//absl/status",
         "@com_google_absl//absl/status:statusor",
         "@com_google_absl//absl/types:span",
@@ -286,6 +290,7 @@ cc_library(
     hdrs = ["nccl_clique_key.h"],
     compatible_with = get_compatible_with_portable(),
     deps = [
+        "//xla:executable_run_options",
         "//xla/service:global_device_id",
         "@com_google_absl//absl/algorithm:container",
         "@com_google_absl//absl/status",
@@ -656,6 +661,7 @@ cc_library(
         "//xla/stream_executor:lazy_op_runner",
         "@com_google_absl//absl/base:core_headers",
         "@com_google_absl//absl/container:flat_hash_map",
+        "@intel_extension_for_openxla//xla/service/gpu:xetla_gpu_fused_mha_runner",
         "@com_google_absl//absl/status",
         "@com_google_absl//absl/synchronization",
         "@tsl//tsl/platform:errors",
@@ -1138,7 +1144,7 @@ cc_library(
     srcs = ["for_all_thunks.cc"],
     hdrs = ["for_all_thunks.h"],
     deps = [
-        ":command_buffer_thunk",
+        # ":command_buffer_thunk",
         ":conditional_thunk",
         ":dynamic_slice_thunk",
         ":sequential_thunk",
diff --git a/xla/service/gpu/runtime/ccl_api.cc b/xla/service/gpu/runtime/ccl_api.cc
new file mode 100644
index 0000000000..0ec895d732
--- /dev/null
+++ b/xla/service/gpu/runtime/ccl_api.cc
@@ -0,0 +1,318 @@
+/* Copyright (c) 2024 Intel Corporation
+
+Copyright 2019 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "xla/service/gpu/runtime/ccl_api.h"
+
+#include <cstddef>
+#include <cstdint>
+
+#include "absl/status/status.h"
+#include "absl/status/statusor.h"
+#include "xla/tsl/concurrency/ref_count.h"
+#include "xla/service/collective_ops_utils.h"
+#include "xla/service/gpu/ccl_ops.h"
+#include "xla/service/gpu/runtime/nccl_api.h"
+#include "xla/service/gpu/runtime/nccl_clique_key.h"
+#include "xla/stream_executor/device_memory.h"
+#include "xla/stream_executor/gpu/gpu_stream.h"
+#include "xla/stream_executor/stream.h"
+
+namespace xla::gpu {
+//==-----------------------------------------------------------------------===//
+// NcclApi::PersistentPlanAllocator
+//==-----------------------------------------------------------------------===//
+
+using PersistentPlanAllocator = NcclApi::PersistentPlanAllocator;
+using ScopedPersistentPlanAllocator = NcclApi::ScopedPersistentPlanAllocator;
+
+PersistentPlanAllocator::PersistentPlanAllocator(int64_t,
+                                                 se::DeviceMemoryAllocator*,
+                                                 se::Stream*) {
+  // Suppress clang unused private field warnings.
+  (void)device_ordinal_;
+  (void)allocator_;
+  (void)stream_;
+}
+
+PersistentPlanAllocator::~PersistentPlanAllocator() = default;
+
+absl::StatusOr<se::DeviceMemoryBase>
+PersistentPlanAllocator::AllocateAndInitialize(void*, size_t) {
+  return absl::UnimplementedError("XLA compiled without NCCL support");
+}
+
+absl::Status PersistentPlanAllocator::Deallocate(se::DeviceMemoryBase mem) {
+  return absl::UnimplementedError("XLA compiled without NCCL support");
+}
+
+ScopedPersistentPlanAllocator::ScopedPersistentPlanAllocator(
+    NcclCommHandle, tsl::RCReference<PersistentPlanAllocator>) {
+  // Suppress clang unused private field warnings.
+  (void)comm_;
+  (void)recover_;
+  (void)allocator_;
+}
+
+ScopedPersistentPlanAllocator::~ScopedPersistentPlanAllocator() = default;
+
+//===----------------------------------------------------------------------===//
+// CclApi
+//===----------------------------------------------------------------------===//
+
+static absl::Status UnimplementedError(std::string mes = "") {
+  return absl::UnimplementedError("XLA compiled without CCL support: " + mes);
+}
+
+CclApi::CclApi() {}
+
+absl::StatusOr<NcclCliqueId> CclApi::GetUniqueId() { return NcclCliqueId(); }
+
+absl::StatusOr<NcclCliqueId> CclApi::GetId(const NcclCliqueKey& key,
+                                           const RunId& id) {
+  std::string new_id =
+      id.ToString() + "=" + GlobalDeviceIdsToString(key.devices());
+  TF_RET_CHECK(new_id.size() < NcclCliqueId::kSize)
+      << "Run ID length must < kSize(" << NcclCliqueId::kSize << ").";
+  new_id.resize(NcclCliqueId::kSize);
+
+  return NcclCliqueId().FromString(new_id);
+}
+
+absl::StatusOr<std::vector<CclApi::OwnedNcclComm>> CclApi::CommInitRanks(
+    int32_t nranks, const NcclCliqueId& clique_id,
+    absl::Span<const DeviceRank> ranks, const Config& config){
+  VLOG(1) << "Initialize NCCL communicator for " << ranks.size()
+          << " devices; hash(id)=" << absl::HashOf(clique_id);
+
+  std::vector<OwnedNcclComm> comms;
+  comms.reserve(ranks.size());
+
+  for (size_t i = 0; i < ranks.size(); ++i) {
+    VLOG(1) << "Initialize NCCL communicator for rank #" << ranks[i].rank
+            << " of " << nranks << "; hash(id)=" << absl::HashOf(clique_id);
+
+  NcclCommHandle comm = reinterpret_cast<NcclCommHandle>(
+      new ccl::communicator(nranks, ranks[i].rank, clique_id.ToString()));
+
+  comms.emplace_back(comm, NcclCommDeleter{this});
+}
+
+  return comms;
+}
+
+absl::StatusOr<std::vector<CclApi::OwnedNcclComm>> CclApi::CommSplit(
+    absl::Span<const NcclCommHandle> comms, int32_t color,
+    absl::Span<const int32_t> keys, std::optional<Config> config){
+  // Don't need now
+  return UnimplementedError("CommSplit");
+}
+
+absl::Status CclApi::CommAbort(NcclCommHandle) {
+  // Don't need now
+  return UnimplementedError("CommAbort");
+}
+
+absl::Status CclApi::CommFinalize(NcclCommHandle) {
+  // Don't need now
+  return UnimplementedError("CommFinalize");
+}
+
+absl::Status CclApi::CommDestroy(NcclCommHandle comm) {
+  delete reinterpret_cast<ncclComm_t>(comm);
+  return absl::OkStatus();
+}
+
+absl::StatusOr<int32_t> CclApi::CommCount(NcclCommHandle) {
+  // Don't need now
+  return UnimplementedError("CommCount");
+}
+
+absl::Status CclApi::CommGetAsyncError(NcclCommHandle comm) {
+  return absl::OkStatus();
+}
+
+absl::Status CclApi::GroupStart() {
+  // Don't need now
+  return UnimplementedError("GroupStart");
+}
+
+absl::Status CclApi::GroupEnd() {
+  // Don't need now
+  return UnimplementedError("GroupEnd");
+}
+
+absl::Status CclApi::AllReduce(se::DeviceMemoryBase send_buffer,
+                               se::DeviceMemoryBase recv_buffer,
+                               PrimitiveType dtype, size_t count,
+                               ReductionKind reduction_kind,
+                               NcclCommHandle comm, se::Stream* stream) {
+  se::gpu::GpuStreamHandle gpu_stream = se::gpu::AsGpuStreamValue(stream);
+
+  const void* send_buffer_ = send_buffer.opaque();
+  void* recv_buffer_ = recv_buffer.opaque();
+  size_t element_count = count * (primitive_util::IsComplexType(dtype) ? 2 : 1);
+
+  auto comm_ = reinterpret_cast<ncclComm_t>(comm);
+
+  sycl_allreduce(send_buffer_, recv_buffer_, element_count, dtype,
+                 reduction_kind, gpu_stream, comm_);
+  return absl::OkStatus();
+}
+
+absl::Status CclApi::Broadcast(se::DeviceMemoryBase send_buffer,
+                               se::DeviceMemoryBase recv_buffer,
+                               PrimitiveType dtype, size_t count,
+                               size_t root, NcclCommHandle comm,
+                               se::Stream* stream){
+  se::gpu::GpuStreamHandle gpu_stream = se::gpu::AsGpuStreamValue(stream);
+
+  const void* send_buffer_ = send_buffer.opaque();
+  void* recv_buffer_ = recv_buffer.opaque();
+  size_t element_count = count * (primitive_util::IsComplexType(dtype) ? 2 : 1);
+
+  auto comm_ = reinterpret_cast<ncclComm_t>(comm);
+
+  sycl_broadcast(send_buffer_, recv_buffer_, element_count, dtype,
+                 root, gpu_stream, comm_);
+
+  return absl::OkStatus();
+}
+
+absl::Status CclApi::ReduceScatter(se::DeviceMemoryBase send_buffer,
+                                   se::DeviceMemoryBase recv_buffer,
+                                   PrimitiveType dtype, size_t count,
+                                   ReductionKind reduction_kind,
+                                   NcclCommHandle comm, se::Stream* stream) {
+  se::gpu::GpuStreamHandle gpu_stream = se::gpu::AsGpuStreamValue(stream);
+
+  const void* send_buffer_ = send_buffer.opaque();
+  void* recv_buffer_ = recv_buffer.opaque();
+  size_t element_count = count * (primitive_util::IsComplexType(dtype) ? 2 : 1);
+
+  auto comm_ = reinterpret_cast<ncclComm_t>(comm);
+
+  int num_participants = comm_->nranks;
+  TF_RET_CHECK(element_count % num_participants == 0)
+      << "Source buffer was not an exact multiple of the number of "
+         "participants.";
+  int64_t recv_count = element_count / num_participants;
+  VLOG(3) << absl::StreamFormat(
+      "Calling ncclReduceScatter(send_buffer=%p, recv_buffer=%p, "
+      "recvcount=%d, "
+      "comm=%p, stream=%p)",
+      send_buffer_, recv_buffer_, recv_count, static_cast<const void*>(comm_),
+      gpu_stream);
+
+  sycl_reduce_scatter(send_buffer_, recv_buffer_, recv_count, dtype,
+                      reduction_kind, gpu_stream, comm_);
+  return absl::OkStatus();
+}
+
+absl::Status CclApi::AllGather(se::DeviceMemoryBase send_buffer,
+                               se::DeviceMemoryBase recv_buffer,
+                               PrimitiveType dtype, size_t count,
+                               NcclCommHandle comm, se::Stream* stream) {
+  se::gpu::GpuStreamHandle gpu_stream = se::gpu::AsGpuStreamValue(stream);
+
+  const void* send_buffer_ = send_buffer.opaque();
+  void* recv_buffer_ = recv_buffer.opaque();
+  size_t element_count = count * (primitive_util::IsComplexType(dtype) ? 2 : 1);
+
+  auto comm_ = reinterpret_cast<ncclComm_t>(comm);
+
+  sycl_allgather(send_buffer_, recv_buffer_, element_count, dtype, gpu_stream,
+                 comm_);
+  return absl::OkStatus();
+}
+
+absl::Status CclApi::AllToAll(bool has_split_dimension,
+                              std::vector<const void*>& send_buffers,
+                              std::vector<void*>& recv_buffers,
+                              size_t element_count, PrimitiveType element_type,
+                              NcclCommHandle comm, se::Stream* stream) {
+  auto comm_ = reinterpret_cast<ncclComm_t>(comm);
+
+  se::gpu::GpuStreamHandle gpu_stream = se::gpu::AsGpuStreamValue(stream);
+
+  element_count = element_count * (primitive_util::IsComplexType(element_type) ? 2 : 1);
+
+  if (has_split_dimension) {
+    sycl_alltoall_split(send_buffers, recv_buffers, element_count, element_type,
+                        gpu_stream, comm_);
+  } else {
+    sycl_alltoall(send_buffers, recv_buffers, element_count, element_type,
+                  gpu_stream, comm_);
+  }
+
+  return absl::OkStatus();
+}
+
+absl::Status CclApi::CollectivePermute(se::DeviceMemoryBase src_addr,
+                                       se::DeviceMemoryBase dest_addr,
+                                       size_t element_count,
+                                       PrimitiveType element_type,
+                                       const std::optional<int64_t> source_id,
+                                       const std::optional<int64_t> target_id,
+                                       NcclCommHandle comm,
+                                       se::Stream* stream) {
+  auto comm_ = reinterpret_cast<ncclComm_t>(comm);
+
+  se::gpu::GpuStreamHandle gpu_stream = se::gpu::AsGpuStreamValue(stream);
+
+  element_count = element_count * (primitive_util::IsComplexType(element_type) ? 2 : 1);
+
+  sycl_collective_permute(src_addr.opaque(), dest_addr.opaque(), element_count,
+                          element_type, source_id, target_id, gpu_stream,
+                          comm_);
+
+  return absl::OkStatus();
+}
+
+absl::Status CclApi::Send(se::DeviceMemoryBase, PrimitiveType, size_t, int32_t,
+                          NcclCommHandle, se::Stream*) {
+  // Don't need now
+  return UnimplementedError("Send");
+}
+
+absl::Status CclApi::Recv(se::DeviceMemoryBase, PrimitiveType, size_t, int32_t,
+                          NcclCommHandle, se::Stream*) {
+  // Don't need now
+  return UnimplementedError("Recv");
+}
+
+absl::StatusOr<CclApi::NcclRegisteredBufferHandle> CclApi::RegisterBuffer(
+    NcclCommHandle, se::DeviceMemoryBase) {
+  // Don't need now
+  return UnimplementedError("RegisterBuffer");
+}
+
+absl::StatusOr<CclApi::NcclRegisteredBufferHandle> CclApi::DeregisterBuffer(
+    NcclCommHandle, CclApi::NcclRegisteredBufferHandle) {
+  // Don't need now
+  return UnimplementedError("DeregisterBuffer");
+}
+
+ncclComm_t CastCCLComm(CclApi::NcclCommHandle comm) {
+  return reinterpret_cast<ncclComm_t>(comm);
+}
+
+NcclApi* NcclApi::Default() {
+  static auto* ccl_api = new CclApi();
+  return ccl_api;
+}
+
+}  // namespace xla::gpu
diff --git a/xla/service/gpu/runtime/ccl_api.h b/xla/service/gpu/runtime/ccl_api.h
new file mode 100644
index 0000000000..bb90373e1e
--- /dev/null
+++ b/xla/service/gpu/runtime/ccl_api.h
@@ -0,0 +1,120 @@
+/* Copyright (c) 2024 Intel Corporation
+
+Copyright 2019 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+#ifndef XLA_SERVICE_GPU_RUNTIME_CCL_API_H_
+#define XLA_SERVICE_GPU_RUNTIME_CCL_API_H_
+
+#include <cstddef>
+#include <cstdint>
+
+#include "absl/status/status.h"
+#include "absl/status/statusor.h"
+#include "xla/tsl/concurrency/ref_count.h"
+#include "xla/service/collective_ops_utils.h"
+#include "xla/service/gpu/ccl_ops.h"
+#include "xla/service/gpu/runtime/nccl_api.h"
+#include "xla/service/gpu/runtime/nccl_clique_key.h"
+#include "xla/stream_executor/device_memory.h"
+#include "xla/stream_executor/gpu/gpu_stream.h"
+#include "xla/stream_executor/stream.h"
+
+namespace xla::gpu {
+//===----------------------------------------------------------------------===//
+// CclApi
+//===----------------------------------------------------------------------===//
+
+class CclApi final : public NcclApi {
+ public:
+  CclApi();
+
+  absl::StatusOr<NcclCliqueId> GetUniqueId() final;
+
+  absl::StatusOr<NcclCliqueId> GetId(const NcclCliqueKey& key,
+                                     const RunId& id) final;
+
+//   absl::StatusOr<OwnedNcclComm> CommInitRank(int32_t nranks,
+//                                              const NcclCliqueId& clique_id,
+//                                              int32_t rank);
+
+  absl::StatusOr<std::vector<OwnedNcclComm>> CommInitRanks(
+      int32_t nranks, const NcclCliqueId& clique_id,
+      absl::Span<const DeviceRank> ranks, const Config& config) final;
+
+  absl::StatusOr<std::vector<OwnedNcclComm>> CommSplit(
+      absl::Span<const NcclCommHandle> comms, int32_t color,
+      absl::Span<const int32_t> keys, std::optional<Config> config) final;
+
+  absl::Status CommAbort(NcclCommHandle) final;
+  absl::Status CommFinalize(NcclCommHandle) final;
+  absl::Status CommDestroy(NcclCommHandle comm) final;
+  absl::StatusOr<int32_t> CommCount(NcclCommHandle) final;
+  absl::Status CommGetAsyncError(NcclCommHandle comm) final;
+
+  absl::Status GroupStart() final;
+
+  absl::Status GroupEnd() final;
+
+  absl::Status AllReduce(se::DeviceMemoryBase send_buffer,
+                         se::DeviceMemoryBase recv_buffer, PrimitiveType dtype,
+                         size_t count, ReductionKind reduction_kind,
+                         NcclCommHandle comm, se::Stream* stream) final;
+
+  absl::Status Broadcast(se::DeviceMemoryBase send_buffer,
+                         se::DeviceMemoryBase recv_buffer,
+                         PrimitiveType dtype, size_t count,
+                         size_t root, NcclCommHandle comm,
+                         se::Stream* stream) final;
+
+  absl::Status ReduceScatter(se::DeviceMemoryBase send_buffer,
+                             se::DeviceMemoryBase recv_buffer,
+                             PrimitiveType dtype, size_t count,
+                             ReductionKind reduction_kind, NcclCommHandle comm,
+                             se::Stream* stream) final;
+
+  absl::Status AllGather(se::DeviceMemoryBase send_buffer,
+                         se::DeviceMemoryBase recv_buffer, PrimitiveType dtype,
+                         size_t count, NcclCommHandle comm,
+                         se::Stream* stream) final;
+
+  absl::Status AllToAll(bool has_split_dimension,
+                        std::vector<const void*>& send_buffers,
+                        std::vector<void*>& recv_buffers, size_t element_count,
+                        PrimitiveType element_type, NcclCommHandle comm,
+                        se::Stream* stream);
+
+  absl::Status CollectivePermute(se::DeviceMemoryBase src_addr,
+                                 se::DeviceMemoryBase dest_addr,
+                                 size_t element_count, PrimitiveType element_type,
+                                 const std::optional<int64_t> source_id,
+                                 const std::optional<int64_t> target_id,
+                                 NcclCommHandle comm, se::Stream* stream);
+
+  absl::Status Send(se::DeviceMemoryBase, PrimitiveType, size_t, int32_t,
+                    NcclCommHandle, se::Stream*) final;
+  absl::Status Recv(se::DeviceMemoryBase, PrimitiveType, size_t, int32_t,
+                    NcclCommHandle, se::Stream*) final;
+
+  absl::StatusOr<NcclRegisteredBufferHandle> RegisterBuffer(
+      NcclCommHandle, se::DeviceMemoryBase) final;
+
+  absl::StatusOr<NcclRegisteredBufferHandle> DeregisterBuffer(
+      NcclCommHandle, NcclRegisteredBufferHandle) final;
+};
+
+ncclComm_t CastCCLComm(CclApi::NcclCommHandle);
+
+}  // namespace xla::gpu
+#endif  // XLA_SERVICE_GPU_RUNTIME_CCL_API_H_
diff --git a/xla/service/gpu/runtime/cholesky_thunk.cc b/xla/service/gpu/runtime/cholesky_thunk.cc
index 1b6112f2f7..a7af374f62 100644
--- a/xla/service/gpu/runtime/cholesky_thunk.cc
+++ b/xla/service/gpu/runtime/cholesky_thunk.cc
@@ -54,6 +54,7 @@ absl::Status DoPotrfBatched(const se::GpuAsmOpts& asm_opts,
   se::DeviceMemory<T*> as(params->workspace_buffer);
 #endif
 
+#if !TENSORFLOW_USE_SYCL
   CHECK_GE(as.size(), params->batch_size);
   CHECK_GE(infos.size(), params->batch_size);
 
@@ -66,6 +67,10 @@ absl::Status DoPotrfBatched(const se::GpuAsmOpts& asm_opts,
   // Now that we've set up the `as` array, we can call cusolver.
   return context.PotrfBatched(params->uplo, params->n, as, params->n, infos,
                               params->batch_size);
+#else // !TENSORFLOW_USE_SYCL
+  return context.PotrfBatched(params->uplo, params->n, as, params->n, infos,
+                              params->batch_size, a_base);
+#endif // !TENSORFLOW_USE_SYCL
 }
 
 template <typename T>
diff --git a/xla/service/gpu/runtime/custom_call_thunk.cc b/xla/service/gpu/runtime/custom_call_thunk.cc
index e40f8000d4..fc259dc8d2 100644
--- a/xla/service/gpu/runtime/custom_call_thunk.cc
+++ b/xla/service/gpu/runtime/custom_call_thunk.cc
@@ -38,7 +38,7 @@ limitations under the License.
 #include "xla/stream_executor/stream.h"
 #include "xla/util.h"
 
-#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM
+#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM || TENSORFLOW_USE_SYCL
 #include "xla/stream_executor/gpu/gpu_stream.h"
 #endif
 
@@ -92,7 +92,7 @@ absl::Status CustomCallThunk::ExecuteCustomCall(const ExecuteParams& params) {
     }
   }
 
-#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM
+#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM || TENSORFLOW_USE_SYCL
   auto gpu_stream = se::gpu::AsGpuStreamValue(params.stream);
   XlaCustomCallStatus custom_call_status;
   call_target_(gpu_stream, buffers.data(), opaque_.data(), opaque_.size(),
@@ -103,11 +103,11 @@ absl::Status CustomCallThunk::ExecuteCustomCall(const ExecuteParams& params) {
   } else {
     return absl::OkStatus();
   }
-#else   //  GOOGLE_CUDA || TENSORFLOW_USE_ROCM
+#else   //  GOOGLE_CUDA || TENSORFLOW_USE_ROCM || TENSORFLOW_USE_SYCL
   return Unavailable(
       "Custom calls on GPU are not supported in this configuration. Please "
       "build with --config=cuda or --config=rocm");
-#endif  //   GOOGLE_CUDA || TENSORFLOW_USE_ROCM
+#endif  //   GOOGLE_CUDA || TENSORFLOW_USE_ROCM || TENSORFLOW_USE_SYCL
 }
 
 absl::Status CustomCallThunk::ExecuteFfiHandler(
diff --git a/xla/service/gpu/runtime/custom_call_thunk.h b/xla/service/gpu/runtime/custom_call_thunk.h
index 542583f41a..df029ba22b 100644
--- a/xla/service/gpu/runtime/custom_call_thunk.h
+++ b/xla/service/gpu/runtime/custom_call_thunk.h
@@ -38,7 +38,7 @@ limitations under the License.
 #include "xla/stream_executor/device_memory_allocator.h"
 #include "xla/stream_executor/stream.h"
 
-#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM
+#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM || TENSORFLOW_USE_SYCL
 #include "xla/stream_executor/gpu/gpu_types.h"
 #endif
 
@@ -58,11 +58,11 @@ namespace gpu {
 // compiler is allowed to create.
 class CustomCallThunk : public Thunk {
  public:
-#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM
+#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM || TENSORFLOW_USE_SYCL
   using Stream = stream_executor::gpu::GpuStreamHandle;
-#else   //  GOOGLE_CUDA || TENSORFLOW_USE_ROCM
+#else   //  GOOGLE_CUDA || TENSORFLOW_USE_ROCM || TENSORFLOW_USE_SYCL
   using Stream = void*;
-#endif  //  GOOGLE_CUDA || TENSORFLOW_USE_ROCM
+#endif  //  GOOGLE_CUDA || TENSORFLOW_USE_ROCM || TENSORFLOW_USE_SYCL
 
   using CustomCallTarget = std::function<void(Stream, void**, const char*,
                                               size_t, XlaCustomCallStatus*)>;
diff --git a/xla/service/gpu/runtime/for_all_thunks.cc b/xla/service/gpu/runtime/for_all_thunks.cc
index d10d82d315..6bb8dcbc30 100644
--- a/xla/service/gpu/runtime/for_all_thunks.cc
+++ b/xla/service/gpu/runtime/for_all_thunks.cc
@@ -19,7 +19,7 @@ limitations under the License.
 #include <optional>
 
 #include "absl/functional/function_ref.h"
-#include "xla/service/gpu/runtime/command_buffer_thunk.h"
+// #include "xla/service/gpu/runtime/command_buffer_thunk.h"
 #include "xla/service/gpu/runtime/conditional_thunk.h"
 #include "xla/service/gpu/runtime/dynamic_slice_thunk.h"
 #include "xla/service/gpu/runtime/sequential_thunk.h"
@@ -39,13 +39,14 @@ void ForAllThunks(absl::FunctionRef<void(const Thunk*)> fn,
       ForAllThunks(fn, tensorflow::down_cast<const DynamicSliceThunk*>(thunk)
                            ->embedded_thunk());
       break;
-    case Thunk::kCommandBuffer:
-      if (const std::optional<ThunkSequence>& sequence =
-              tensorflow::down_cast<const CommandBufferThunk*>(thunk)->thunks();
-          sequence.has_value()) {
-        ForAllThunks(fn, &sequence.value());
-      }
-      break;
+    // SYCL TODO: don't support CommandBufferThunk
+    // case Thunk::kCommandBuffer:
+    //   if (const std::optional<ThunkSequence>& sequence =
+    //           tensorflow::down_cast<const CommandBufferThunk*>(thunk)->thunks();
+    //       sequence.has_value()) {
+    //     ForAllThunks(fn, &sequence.value());
+    //   }
+    //   break;
     case Thunk::kConditional:
       for (const std::unique_ptr<SequentialThunk>& branch :
            tensorflow::down_cast<const ConditionalThunk*>(thunk)
diff --git a/xla/service/gpu/runtime/fused_mha_thunk.cc b/xla/service/gpu/runtime/fused_mha_thunk.cc
index 4370178b40..7fa0a74e10 100644
--- a/xla/service/gpu/runtime/fused_mha_thunk.cc
+++ b/xla/service/gpu/runtime/fused_mha_thunk.cc
@@ -32,6 +32,10 @@ limitations under the License.
 #include "tsl/platform/errors.h"
 #include "tsl/platform/statusor.h"
 
+#if TENSORFLOW_USE_SYCL
+#include "xla/service/gpu/xetla_gpu_fused_mha_runner.h"
+#endif
+
 namespace xla {
 namespace gpu {
 
@@ -77,10 +81,15 @@ std::optional<se::DeviceMemoryBase> AssignBufferIfNotNull(
 }
 
 absl::Status FusedMHAThunk::Initialize(const InitializeParams& params) {
+#ifdef TENSORFLOW_USE_SYCL
+  // SYCL does not need runner
+  return absl::OkStatus();
+#else
   se::dnn::LazyOpRunner<se::dnn::FusedMHAOp>* lazy_runner =
       GetOrCreateRunner(params.stream).AsFusedMHARunner();
   TF_ASSIGN_OR_RETURN(auto config, config_.AsDnnFusedMHAOpConfig());
   return lazy_runner->GetOrCreateRunner(config, params.stream).status();
+#endif
 }
 
 absl::Status FusedMHAThunk::ExecuteOnStream(const ExecuteParams& params) {
@@ -100,6 +109,13 @@ absl::Status FusedMHAThunk::ExecuteOnStream(const ExecuteParams& params) {
       AssignBufferIfNotNull(buffer_allocations, bias_buffer_);
   std::optional<se::DeviceMemoryBase> activation_buffer =
       AssignBufferIfNotNull(buffer_allocations, activation_buffer_);
+
+#if TENSORFLOW_USE_SYCL
+  TF_RETURN_IF_ERROR(RunXetlaGpuFMHA(config_, lhs_bmm1_buffer, rhs_bmm1_buffer,
+                                     rhs_bmm2_buffer, output_buffer,
+                                     scratch_buffer, bias_buffer,
+                                     activation_buffer, params.stream));
+#else 
   std::optional<se::DeviceMemoryBase> seqlen_q_buffer =
       AssignBufferIfNotNull(buffer_allocations, seqlen_q_buffer_);
   std::optional<se::DeviceMemoryBase> seqlen_k_buffer =
@@ -110,7 +126,7 @@ absl::Status FusedMHAThunk::ExecuteOnStream(const ExecuteParams& params) {
                                 rhs_bmm2_buffer, output_buffer, scratch_buffer,
                                 bias_buffer, activation_buffer, seqlen_q_buffer,
                                 seqlen_k_buffer, params.stream, opts));
-
+#endif
   if (!params.stream->ok()) {
     return Internal("FusedMHAThunk::ExecuteOnStream failed.");
   }
@@ -212,6 +228,13 @@ absl::Status FusedMHABackwardThunk::ExecuteOnStream(
       AssignBufferIfNotNull(buffer_allocations, seqlen_k_buffer_);
   RunFusedMHABackwardOptions opts;
 
+#if TENSORFLOW_USE_SYCL
+  TF_RETURN_IF_ERROR(RunXetlaGpuFMHABackward(
+      config_, bmm1_grad_gemm1_rhs_buffer, bmm1_grad_gemm2_rhs_buffer,
+      bmm2_grad_gemm1_lhs_buffer, bmm2_grad_gemm2_rhs_buffer, d_output_buffer,
+      scratch_buffer, d_bmm1_lhs_buffer, d_bmm1_rhs_buffer, d_bmm2_rhs_buffer,
+      d_s_buffer, d_bias_buffer, fwd_output_buffer, bias_buffer, params.stream));
+#else
   opts.runner_cache = &GetOrCreateRunner(params.stream);
 
   TF_RETURN_IF_ERROR(RunGpuFMHABackward(
@@ -220,6 +243,7 @@ absl::Status FusedMHABackwardThunk::ExecuteOnStream(
       scratch_buffer, d_bmm1_lhs_buffer, d_bmm1_rhs_buffer, d_bmm2_rhs_buffer,
       d_s_buffer, d_bias_buffer, fwd_output_buffer, bias_buffer,
       seqlen_q_buffer, seqlen_k_buffer, params.stream, opts));
+#endif
   if (!params.stream->ok()) {
     return Internal("FusedMHABackwardThunk::ExecuteOnStream failed.");
   }
diff --git a/xla/service/gpu/runtime/nccl_api.h b/xla/service/gpu/runtime/nccl_api.h
index 76747b64f7..c406f743da 100644
--- a/xla/service/gpu/runtime/nccl_api.h
+++ b/xla/service/gpu/runtime/nccl_api.h
@@ -146,6 +146,10 @@ class NcclApi {
   // https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/api/comms.html#ncclgetuniqueid
   virtual absl::StatusOr<NcclCliqueId> GetUniqueId() = 0;
 
+  // Temporary function to get unique id for non-NCCL backend.
+  virtual absl::StatusOr<NcclCliqueId> GetId(const NcclCliqueKey& key,
+                                             const RunId& id) = 0;
+                                             
   // Creates new communicators for given devices.
   //
   // This API doesn't have a corresponding API in NCCL and implemented as
diff --git a/xla/service/gpu/runtime/nccl_clique.cc b/xla/service/gpu/runtime/nccl_clique.cc
index f43e662b70..d2a82d8211 100644
--- a/xla/service/gpu/runtime/nccl_clique.cc
+++ b/xla/service/gpu/runtime/nccl_clique.cc
@@ -73,8 +73,11 @@ absl::StatusOr<const NcclCliqueIdCallback*> GetNcclCliqueIdCallback(
       << "If non-local devices are taking part of a collective API on "
          "GPU, the nccl_clique_id_callback must be provided by the client.";
 
-  static auto* local_callback = new NcclCliqueIdCallback(
-      [](const NcclCliqueKey&) { return NcclApi::Default()->GetUniqueId(); });
+  static auto* local_callback =
+      new NcclCliqueIdCallback([](const NcclCliqueKey& key, const RunId& id) {
+        // return NcclApi::Default()->GetUniqueId();
+        return NcclApi::Default()->GetId(key, id);
+      });
   return local_callback;
 }
 
@@ -263,7 +266,7 @@ static absl::StatusOr<std::shared_ptr<NcclClique::Lock>> InitializeNcclClique(
   // gives access to clique communicators.
   auto initialize = [&](absl::Span<const NcclApi::DeviceRank* const> args)
       -> absl::StatusOr<NcclClique::Lock> {
-    TF_ASSIGN_OR_RETURN(auto clique_id, clique_id_callback(clique_key));
+    TF_ASSIGN_OR_RETURN(auto clique_id, clique_id_callback(clique_key, run_id));
 
     std::vector<NcclApi::DeviceRank> ranks;
     ranks.reserve(args.size());
diff --git a/xla/service/gpu/runtime/nccl_clique.h b/xla/service/gpu/runtime/nccl_clique.h
index e84fabbc9a..ad0bd1fce2 100644
--- a/xla/service/gpu/runtime/nccl_clique.h
+++ b/xla/service/gpu/runtime/nccl_clique.h
@@ -30,6 +30,7 @@ limitations under the License.
 #include "absl/strings/str_format.h"
 #include "xla/executable_run_options.h"
 #include "xla/service/gpu/runtime/nccl_api.h"
+#include "xla/service/gpu/runtime/ccl_api.h"
 #include "xla/service/gpu/runtime/nccl_clique_key.h"
 #include "xla/service/lockable.h"
 #include "xla/stream_executor/stream_executor.h"
diff --git a/xla/service/gpu/runtime/nccl_clique_key.h b/xla/service/gpu/runtime/nccl_clique_key.h
index 86c8d69b53..0dc2f64156 100644
--- a/xla/service/gpu/runtime/nccl_clique_key.h
+++ b/xla/service/gpu/runtime/nccl_clique_key.h
@@ -26,6 +26,7 @@ limitations under the License.
 
 #include "absl/status/statusor.h"
 #include "absl/types/span.h"
+#include "xla/executable_run_options.h"
 #include "xla/service/global_device_id.h"
 #include "tsl/lib/gtl/int_type.h"
 
@@ -159,7 +160,8 @@ H AbslHashValue(H h, const NcclCliqueId& id) {
 
 // A callback to get a unique clique id (see `ncclUniqueId` documentation).
 using NcclCliqueIdCallback =  // NOLINT
-    std::function<absl::StatusOr<NcclCliqueId>(const NcclCliqueKey&)>;
+    std::function<absl::StatusOr<NcclCliqueId>(const NcclCliqueKey&,
+                                               const RunId&)>;
 
 }  // namespace xla::gpu
 
diff --git a/xla/service/gpu/spir_compiler.cc b/xla/service/gpu/spir_compiler.cc
new file mode 100644
index 0000000000..46246307af
--- /dev/null
+++ b/xla/service/gpu/spir_compiler.cc
@@ -0,0 +1,299 @@
+/* Copyright (c) 2023 Intel Corporation
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "xla/service/gpu/spir_compiler.h"
+
+#include <stdlib.h>
+
+#include <fstream>
+#include <memory>
+#include <string>
+#include <utility>
+#include <vector>
+
+#include "tsl/platform/path.h"
+#include "tsl/platform/status.h"
+#include "xla/tsl/util/env_var.h"
+#include "xla/hlo/ir/hlo_opcode.h"
+#include "xla/service/algebraic_simplifier.h"
+#include "xla/service/call_inliner.h"
+#include "xla/service/convert_mover.h"
+#include "xla/service/dot_dimension_merger.h"
+#include "xla/service/dump.h"
+#include "xla/service/float_normalization.h"
+#include "xla/service/float_support.h"
+#include "xla/service/gpu/backend_configs.pb.h"
+#include "xla/service/gpu/buffer_sharing.h"
+#include "xla/service/gpu/cublas_cudnn.h"
+#include "xla/service/gpu/cudnn_fused_conv_rewriter.h"
+#include "xla/service/gpu/cudnn_fused_mha_rewriter.h"
+#include "xla/service/gpu/cusolver_rewriter.h"
+#include "xla/service/gpu/gemm_impl_picker.h"
+#include "xla/service/gpu/gpu_conv_padding_legalization.h"
+#include "xla/service/gpu/gpu_conv_rewriter.h"
+#include "xla/service/gpu/gpu_layout_assignment.h"
+#include "xla/service/gpu/ir_emission_utils.h"
+#include "xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.h"
+#include "xla/service/gpu/move_copy_to_users.h"
+#include "xla/service/gpu/redundant_convert_mover.h"
+#include "xla/service/gpu/target_constants.h"
+#include "xla/service/gpu/triangular_solve_rewriter.h"
+#include "xla/service/hlo_constant_folding.h"
+#include "xla/service/hlo_cse.h"
+#include "xla/service/hlo_dce.h"
+#include "xla/service/hlo_pass_fix.h"
+#include "xla/service/hlo_pass_pipeline.h"
+#include "xla/service/hlo_verifier.h"
+#include "xla/service/layout_normalization.h"
+#include "xla/service/llvm_ir/llvm_util.h"
+#include "xla/service/reshape_decomposer.h"
+#include "xla/service/reshape_mover.h"
+#include "xla/service/tuple_simplifier.h"
+#include "xla/status_macros.h"
+#include "xla/stream_executor/sycl/hw_info.h"
+#include "xla/stream_executor/sycl/sycl_platform_id.h"
+#include "xla/types.h"
+#include "xla/util.h"
+
+namespace xla {
+namespace gpu {
+namespace {
+
+class ConvBfloat16Support : public FloatSupport {
+ public:
+  explicit ConvBfloat16Support()
+      : FloatSupport(BF16), is_conv_bf16_supported_(true) {}
+
+  bool SupportsLowPrecisionOperand(const HloInstruction& hlo,
+                                   int64_t operand_index) const override {
+    return (hlo.opcode() != HloOpcode::kConvolution) || is_conv_bf16_supported_;
+  }
+
+  bool SupportsLowPrecisionOutput(const HloInstruction& hlo) const override {
+    return (hlo.opcode() != HloOpcode::kConvolution) || is_conv_bf16_supported_;
+  }
+
+  bool SupportsMixedPrecisions(const HloInstruction& hlo) const override {
+    // Skip all HLOs other than convolutions.
+    return (hlo.opcode() != HloOpcode::kConvolution);
+  }
+
+ private:
+  bool is_conv_bf16_supported_;
+};
+
+}  // namespace
+
+int32_t SPIRCompiler::GetToolkitVersion() const {
+#if TENSORFLOW_USE_SYCL
+  return -1;
+#endif
+  LOG(FATAL) << "Failed to get SYCL version.";
+}
+
+
+absl::Status SPIRCompiler::OptimizeHloConvolutionCanonicalization(
+    HloModule* hlo_module, se::GpuComputeCapability gpu_version,
+    se::dnn::VersionInfo dnn_version,
+    se::DeviceMemoryAllocator* device_allocator) {
+  auto cuda_compute_capability =
+      std::get<se::CudaComputeCapability>(gpu_version);
+  // Convert convolutions into CustomCalls to onednn, then canonicalize them
+  // (GpuConvPaddingLegalization). Also expand cuSolver calls.
+  HloPassPipeline pipeline("conv_canonicalization");
+  pipeline.AddInvariantCheckerDebug<HloVerifier>(
+      /*layout_sensitive=*/false,
+      /*allow_mixed_precision=*/false);
+
+  // Convert upsupported bf16 convolutions to f32.
+  ConvBfloat16Support conv_bf16_support;
+  pipeline.AddPass<FloatNormalization>(&conv_bf16_support);
+
+  pipeline.AddPass<GpusolverRewriter>();
+  pipeline.AddPass<GpuConvRewriter>(cuda_compute_capability);
+  pipeline.AddPass<CudnnFusedConvRewriter>(cuda_compute_capability, dnn_version,
+                                           GetToolkitVersion());
+  pipeline.AddPass<GpuConvPaddingLegalization>();
+
+  // The conv padding/vectorization passes which we need to get rid of.  They
+  // also leave behind unnecessary tuple/get-tuple-element pairs that
+  // TupleSimplifier fixes.
+  pipeline.AddPass<CallInliner>();
+  pipeline.AddPass<TupleSimplifier>();
+
+  AlgebraicSimplifierOptions algsimp_options =
+      GetAlgebraicSimplifierOptions(hlo_module->config());
+  algsimp_options.set_enable_conv_operand_swap(false);
+  algsimp_options.set_enable_unconditional_reduce_of_concat_replacement(false);
+  pipeline.AddPass<HloPassFix<AlgebraicSimplifier>>(algsimp_options);
+
+  // tf2xla bridge, DepthwiseConvolutionConverter, GpuConvRewriter, and
+  // CudnnSimplifyPadding introduce reshapes and transposes.  Run ReshapeMover
+  // to a fixed point.  Include algsimp because ReshapeMover relies on it.
+  [&, &pipeline = pipeline.AddPass<HloPassFix<HloPassPipeline>>(
+          "reshape_mover_after_conv_canonicalization")] {
+    ReshapeMoverOptions reshape_mover_options;
+    reshape_mover_options.reshape_of_1d_broadcast_is_cheap = true;
+    pipeline.AddPass<HloPassFix<ReshapeMover>>(reshape_mover_options);
+    pipeline.AddPass<AlgebraicSimplifier>(algsimp_options);
+  }();
+
+  // The reshapes and transposes can possibly be eliminated using
+  // AlgebraicSimplifier. ConvertMover and ReshapeMover fight with each other.
+  // ConvertMover wants to move some converts down the graph, but ReshapeMover
+  // wants to move them up the graph. We run ConvertMover and algsimp to a fixed
+  // point.
+  [&, &pipeline = pipeline.AddPass<HloPassFix<HloPassPipeline>>(
+          "simplify_after_conv_canonicalization")] {
+    pipeline.AddPass<ConvertMover>();
+    pipeline.AddPass<AlgebraicSimplifier>(algsimp_options);
+  }();
+
+  // GpuConvRewriter, GpuConvPaddingLegalization and
+  // CudnnConvPadForTensorCores may add instructions which can be simplified
+  // by constant folding.
+  pipeline.AddPass<HloConstantFolding>();
+  TF_RETURN_IF_ERROR(pipeline.Run(hlo_module).status());
+
+  return absl::OkStatus();
+}
+
+absl::Status SPIRCompiler::OptimizeHloPostLayoutAssignment(
+    HloModule* hlo_module, se::StreamExecutor* stream_exec,
+    const CompileOptions& options, const TargetConfig& gpu_target_config,
+    tsl::thread::ThreadPool* thread_pool) {
+  HloPassPipeline pre_pipeline("spir post-layout_assignment part 1");
+
+  // This needs to run before GemmRewriter, which is part of
+  // OptimizeHloPostLayoutAssignment().
+  auto cuda_compute_capability = std::get<se::CudaComputeCapability>(
+      gpu_target_config.device_description.gpu_compute_capability());
+
+  // For frontend debugging.
+  FrontendAttributes frontend_attributes;
+  bool is_xetla_hardware_support = IsXetlaHardwareSupport();
+  if (is_xetla_hardware_support) {
+    frontend_attributes.mutable_map()->emplace("is_xetla_hardware_support",
+                                               "True");
+  }
+  hlo_module->add_frontend_attributes(frontend_attributes);
+  bool use_mha = true;
+  TF_CHECK_OK(tsl::ReadBoolFromEnvVar("MHA", true, &use_mha));
+  if (use_mha && is_xetla_hardware_support) {
+    HloPassPipeline mha_fusion_pipeline("multi-headed attention fusion");
+    const DebugOptions& debug_options = hlo_module->config().debug_options();
+    // The LayoutAssignment pass may leave behind kCopy instructions which are
+    // duplicate or NOPs, so remove them with algebraic simplification and CSE.
+    AlgebraicSimplifierOptions alg_sim_options;
+    alg_sim_options.set_supports_non_canonical_dots(false);
+    alg_sim_options.set_is_layout_sensitive(true);
+    alg_sim_options.set_enable_conv_operand_swap(false);
+    // "slow" minmax means we propagate nan.
+    alg_sim_options.set_minmax_propagate_nan(
+        !hlo_module->config().debug_options().xla_gpu_enable_fast_min_max());
+    alg_sim_options.set_enable_unconditional_reduce_of_concat_replacement(
+        false);
+    // SYCL TODO: debug_options should add xla_gpu_normalize_layouts here
+    // if (debug_options.xla_gpu_normalize_layouts()) {
+    //   mha_fusion_pipeline.AddPass<ReshapeDecomposer>();
+    //   mha_fusion_pipeline.AddPass<HloPassFix<MoveCopyToUsers>>();
+    //   mha_fusion_pipeline.AddPass<LayoutNormalization>();
+    // }
+    mha_fusion_pipeline.AddPass<HloCSE>(/*is_layout_sensitive=*/true);
+    mha_fusion_pipeline.AddPass<HloPassFix<AlgebraicSimplifier>>(
+        alg_sim_options);
+    mha_fusion_pipeline.AddPass<HloCSE>(/*is_layout_sensitive=*/true);
+
+    // Rewrite Multi-Headed Attention modules to Fused MHA custom-calls.
+    mha_fusion_pipeline.AddPass<RedundantConvertMover>();
+    mha_fusion_pipeline.AddPass<HloDCE>();
+    mha_fusion_pipeline.AddPass<CudnnFusedMHARewriter>(cuda_compute_capability,
+                                                       stream_exec);
+    mha_fusion_pipeline.AddPass<AlgebraicSimplifier>(alg_sim_options);
+    mha_fusion_pipeline.AddPass<HloDCE>();
+    mha_fusion_pipeline.AddPass<HloCSE>(/*is_layout_sensitive=*/true,
+                                        /*only_fusion_computations*/ false);
+    TF_RETURN_IF_ERROR(mha_fusion_pipeline.Run(hlo_module).status());
+  }
+
+  pre_pipeline.AddPass<DotDimensionMerger>();
+
+  // Padding a gemm operand that's a constant results in pad(constant).  Run
+  // constant-folding to simplify this into a new constant.
+  pre_pipeline.AddPass<HloConstantFolding>();
+  TF_RETURN_IF_ERROR(pre_pipeline.Run(hlo_module).status());
+
+  TF_RETURN_IF_ERROR(GpuCompiler::OptimizeHloPostLayoutAssignment(
+      hlo_module, stream_exec, options, gpu_target_config, thread_pool));
+
+  HloPassPipeline post_pipeline("spir post-layout_assignment part 2");
+
+  // Transform TriangularSolve ops into custom-calls, so we can add temp
+  // memory.
+  post_pipeline.AddPass<TriangularSolveRewriter>();
+
+  TF_RETURN_IF_ERROR(post_pipeline.Run(hlo_module).status());
+
+  return absl::OkStatus();
+}
+
+absl::Status SPIRCompiler::AddConvAndGemmAutotuningPasses(
+    HloPassPipeline* pipeline, HloModule* hlo_module,
+    AutotuneConfig& autotune_config, tsl::thread::ThreadPool* thread_pool) {
+  pipeline->AddPass<GemmAlgorithmPicker>(autotune_config);
+  return OkStatus();
+}
+
+SPIRCompiler::SPIRCompiler()
+    : GpuCompiler(stream_executor::sycl::kSyclPlatformId, spir::TargetTriple(),
+                  spir::DataLayout()) {}
+
+HloDataflowAnalysis::CanShareBuffer SPIRCompiler::GetCanShareBuffer() const {
+  return &CanShareBufferHint;
+}
+
+absl::StatusOr<GpuCompiler::BackendCompileResult>
+SPIRCompiler::CompileTargetBinary(const HloModuleConfig& module_config,
+                                  llvm::Module* llvm_module,
+                                  se::GpuComputeCapability gpu_version,
+                                  bool relocatable,
+                                  const HloModule* debug_module,
+                                  const CompileOptions& options) {
+  if (relocatable) {
+    return Unimplemented("relocatable target binary is not implemented");
+  }
+
+  std::vector<uint8_t> spir;
+  {
+    // This may print multiple lines per HLO compilation because of the
+    // parallelized compilation of LLVM modules.
+    XLA_SCOPED_LOGGING_TIMER_IF(
+        "SPIRCompiler::CompileTargetBinary - CompileToSpir",
+        !options.is_autotuning_compilation);
+    TF_ASSIGN_OR_RETURN(spir,
+                        spir::CompileToSpir(llvm_module, gpu_version,
+                                            module_config.debug_options()));
+  }
+
+  return BackendCompileResult{"", std::move(spir)};
+}
+
+/*static*/ SPIRCompiler* SPIRCompiler::CreateSPIRCompiler() {
+  static auto compiler = absl::make_unique<SPIRCompiler>();
+  return compiler.get();
+}
+
+}  // namespace gpu
+}  // namespace xla
diff --git a/xla/service/gpu/spir_compiler.h b/xla/service/gpu/spir_compiler.h
new file mode 100644
index 0000000000..febdeb611a
--- /dev/null
+++ b/xla/service/gpu/spir_compiler.h
@@ -0,0 +1,72 @@
+/* Copyright (c) 2023 Intel Corporation
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef XLA_SERVICE_GPU_SPIR_COMPILER_H_
+#define XLA_SERVICE_GPU_SPIR_COMPILER_H_
+
+#include <string>
+#include <utility>
+#include <vector>
+
+#include "absl/base/call_once.h"
+#include "llvm/IRReader/IRReader.h"
+#include "llvm/Support/SourceMgr.h"
+#include "xla/service/gpu/gpu_compiler.h"
+#include "xla/statusor.h"
+
+namespace xla {
+namespace gpu {
+
+// SPIRCompiler generates efficient GPU executables for NVPTX target.
+class SPIRCompiler : public GpuCompiler {
+ public:
+  SPIRCompiler();
+  ~SPIRCompiler() override {}
+
+  int32_t GetToolkitVersion() const override;
+
+  absl::Status OptimizeHloConvolutionCanonicalization(
+      HloModule* hlo_module, se::GpuComputeCapability gpu_version,
+      se::dnn::VersionInfo dnn_version,
+      se::DeviceMemoryAllocator* device_allocator) override;
+
+  absl::Status OptimizeHloPostLayoutAssignment(
+      HloModule* hlo_module, se::StreamExecutor* stream_exec,
+      const CompileOptions& options, const TargetConfig& gpu_target_config,
+      tsl::thread::ThreadPool* thread_pool) override;
+
+  absl::Status AddConvAndGemmAutotuningPasses(
+      HloPassPipeline* pipeline, HloModule* hlo_module,
+      AutotuneConfig& autotune_config,
+      tsl::thread::ThreadPool* thread_pool) override;
+
+  HloDataflowAnalysis::CanShareBuffer GetCanShareBuffer() const override;
+
+  absl::StatusOr<BackendCompileResult> CompileTargetBinary(
+      const HloModuleConfig& module_config, llvm::Module* llvm_module,
+      se::GpuComputeCapability gpu_version, bool relocatable,
+      const HloModule* debug_module, const CompileOptions& options) override;
+
+  static SPIRCompiler* CreateSPIRCompiler();
+
+ private:
+  SPIRCompiler(const SPIRCompiler&) = delete;
+  SPIRCompiler& operator=(const SPIRCompiler&) = delete;
+};
+
+}  // namespace gpu
+}  // namespace xla
+
+#endif  // XLA_SERVICE_GPU_SPIR_COMPILER_H_
diff --git a/xla/service/gpu/spir_compiler_registration.cc b/xla/service/gpu/spir_compiler_registration.cc
new file mode 100644
index 0000000000..a397ae7a17
--- /dev/null
+++ b/xla/service/gpu/spir_compiler_registration.cc
@@ -0,0 +1,27 @@
+/* Copyright (c) 2023 Intel Corporation
+
+Copyright 2019 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "xla/service/gpu/spir_compiler.h"
+#include "xla/stream_executor/sycl/sycl_platform_id.h"
+
+static bool InitCompilerModule() {
+  xla::Compiler::RegisterCompilerFactory(
+      stream_executor::sycl::kSyclPlatformId,
+      []() { return std::make_unique<xla::gpu::SPIRCompiler>(); });
+  return true;
+}
+static bool compiler_module_initialized = InitCompilerModule();
diff --git a/xla/service/gpu/stream_executor_util.cc b/xla/service/gpu/stream_executor_util.cc
index c278fbe8b4..f2d7b95d23 100644
--- a/xla/service/gpu/stream_executor_util.cc
+++ b/xla/service/gpu/stream_executor_util.cc
@@ -185,6 +185,13 @@ StreamExecutorConvLayoutsToXlaLayouts(const ConvolutionDimensionNumbers& dnums,
                            dnums.kernel_spatial_dimensions().end());
       filter_layout.push_back(dnums.kernel_input_feature_dimension());
       break;
+    case FilterLayout::kYXInputOutput:  // HWIO
+      filter_layout.insert(filter_layout.end(),
+                           dnums.kernel_spatial_dimensions().begin(),
+                           dnums.kernel_spatial_dimensions().end());
+      filter_layout.push_back(dnums.kernel_input_feature_dimension());
+      filter_layout.push_back(dnums.kernel_output_feature_dimension());
+      break;
     default:
       return Internal("Invalid filter layout %s for conv with dnums %s,",
                       FilterLayoutString(filter),
@@ -222,7 +229,7 @@ XlaConvShapesToStreamExecutorLayouts(const ConvolutionDimensionNumbers& dnums,
   Layout nhwc_input, nhwc_filter, nhwc_output;
   std::tie(nhwc_input, nhwc_filter, nhwc_output) =
       StreamExecutorConvLayoutsToXlaLayouts(dnums, DataLayout::kBatchYXDepth,
-                                            FilterLayout::kOutputYXInput,
+                                            FilterLayout::kYXInputOutput,
                                             DataLayout::kBatchYXDepth)
           .value();
 
@@ -271,7 +278,7 @@ XlaConvShapesToStreamExecutorLayouts(const ConvolutionDimensionNumbers& dnums,
           ConvolutionDimensionNumbersToString(dnums), vect_size);
     }
   } else if (LayoutUtil::Equal(filter.layout(), nhwc_filter)) {
-    filter_layout = FilterLayout::kOutputYXInput;
+    filter_layout = FilterLayout::kYXInputOutput;
   } else {
     return Internal(
         "Invalid filter layout %s for conv with dnums %s, expected one of (%s, "
diff --git a/xla/service/gpu/target_constants.h b/xla/service/gpu/target_constants.h
index 13190ae690..574a4120d7 100644
--- a/xla/service/gpu/target_constants.h
+++ b/xla/service/gpu/target_constants.h
@@ -67,7 +67,7 @@ inline const char* DataLayout() {
       "e-p:64:64:64-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-f32:"
       "32:32-f64:64:64-v16:16:16-v24:32:32-v32:32:32-v48:64:64-v64:64:64-v96:"
       "128:128-v128:128:128-v192:256:256-v256:256:256-v512:512:512-v1024:1024:"
-      "1024";
+      "1024-n8:16:32:64";
   return kDataLayout;
 }
 }  // namespace spir
diff --git a/xla/service/gpu/tests/dynamic_update_slice_inplace.hlo b/xla/service/gpu/tests/dynamic_update_slice_inplace.hlo
index 8fd8c1cbe6..2819cc840b 100644
--- a/xla/service/gpu/tests/dynamic_update_slice_inplace.hlo
+++ b/xla/service/gpu/tests/dynamic_update_slice_inplace.hlo
@@ -144,13 +144,13 @@ ENTRY main {
 // CHECK:      [[DUS1]].in_bounds-after:
 // CHECK-NEXT:   ret void
 // CHECK:      [[DUS0]].in_bounds-true
-// CHECK-DAG:    getelementptr bfloat, ptr %[[ARG1]], i64 %[[VAL_141:.*]]
-// CHECK-DAG:    getelementptr bfloat, ptr %[[ARG3]], i64 %[[VAL_141]]
-// CHECK-DAG:    getelementptr inbounds [10 x [11 x [12 x bfloat]]], ptr %[[ARG0]], i64 0, i64 %[[VAL_185:.*]], i64 %[[VAL_187:.*]], i64 %[[VAL_189:.*]]
+// CHECK-DAG:    getelementptr i16, ptr %[[ARG1]], i64 %[[VAL_141:.*]]
+// CHECK-DAG:    getelementptr i16, ptr %[[ARG3]], i64 %[[VAL_141]]
+// CHECK-DAG:    getelementptr inbounds [10 x [11 x [12 x i16]]], ptr %[[ARG0]], i64 0, i64 %[[VAL_185:.*]], i64 %[[VAL_187:.*]], i64 %[[VAL_189:.*]]
 // CHECK:      [[DUS1]].in_bounds-true
-// CHECK-DAG:    getelementptr bfloat, ptr %[[ARG1]], i64 %[[VAL_173:.*]]
-// CHECK-DAG:    getelementptr bfloat, ptr %[[ARG3]], i64 %[[VAL_173]]
-// CHECK-DAG:    getelementptr inbounds [8 x [11 x [12 x bfloat]]], ptr %[[ARG2]], i64 0, i64 %[[VAL_208:.*]], i64 %[[VAL_210:.*]], i64 %[[VAL_212:.*]]
+// CHECK-DAG:    getelementptr i16, ptr %[[ARG1]], i64 %[[VAL_173:.*]]
+// CHECK-DAG:    getelementptr i16, ptr %[[ARG3]], i64 %[[VAL_173]]
+// CHECK-DAG:    getelementptr inbounds [8 x [11 x [12 x i16]]], ptr %[[ARG2]], i64 0, i64 %[[VAL_208:.*]], i64 %[[VAL_210:.*]], i64 %[[VAL_212:.*]]
 
 HloModule MultipleInplaceDus, is_scheduled=true, input_output_alias={ {0}: (0, {}), {1}: (2, {}) }
 
@@ -194,13 +194,13 @@ ENTRY main {
 // CHECK:      [[DUS1]].in_bounds-after:
 // CHECK-NEXT:   ret void
 // CHECK:      [[DUS0]].in_bounds-true
-// CHECK-DAG:    getelementptr bfloat, ptr %[[ARG1]], i64 %[[VAL_247:.*]]
-// CHECK-DAG:    getelementptr bfloat, ptr %[[ARG3]], i64 %[[VAL_247]]
-// CHECK-DAG:    getelementptr inbounds [10 x [11 x [12 x bfloat]]], ptr %[[ARG0]], i64 0, i64 %[[VAL_291:.*]], i64 %[[VAL_293:.*]], i64 %[[VAL_295:.*]]
+// CHECK-DAG:    getelementptr i16, ptr %[[ARG1]], i64 %[[VAL_247:.*]]
+// CHECK-DAG:    getelementptr i16, ptr %[[ARG3]], i64 %[[VAL_247]]
+// CHECK-DAG:    getelementptr inbounds [10 x [11 x [12 x i16]]], ptr %[[ARG0]], i64 0, i64 %[[VAL_291:.*]], i64 %[[VAL_293:.*]], i64 %[[VAL_295:.*]]
 // CHECK:      [[DUS1]].in_bounds-true
-// CHECK-DAG:    getelementptr bfloat, ptr %[[ARG1]], i64 %[[VAL_279:.*]]
-// CHECK-DAG:    getelementptr bfloat, ptr %[[ARG3]], i64 %[[VAL_279]]
-// CHECK-DAG:    getelementptr inbounds [8 x [11 x [12 x bfloat]]], ptr %[[ARG2]], i64 0, i64 %[[VAL_314:.*]], i64 %[[VAL_316:.*]], i64 %[[VAL_318:.*]]
+// CHECK-DAG:    getelementptr i16, ptr %[[ARG1]], i64 %[[VAL_279:.*]]
+// CHECK-DAG:    getelementptr i16, ptr %[[ARG3]], i64 %[[VAL_279]]
+// CHECK-DAG:    getelementptr inbounds [8 x [11 x [12 x i16]]], ptr %[[ARG2]], i64 0, i64 %[[VAL_314:.*]], i64 %[[VAL_316:.*]], i64 %[[VAL_318:.*]]
 
 HloModule MultipleInplaceDusWithTransposeBitcastToTheRoot, is_scheduled=true, input_output_alias={ {0}: (0, {}), {1}: (2, {}) }
 
@@ -241,9 +241,9 @@ ENTRY main {
 // CHECK:      [[DUS0]].in_bounds-after:
 // CHECK-NEXT:   ret void
 // CHECK:      [[DUS0]].in_bounds-true
-// CHECK-DAG:    getelementptr bfloat, ptr %[[ARG1]], i64 %[[VAL_353:.*]]
-// CHECK-DAG:    getelementptr bfloat, ptr %[[ARG2]], i64 %[[VAL_353]]
-// CHECK-DAG:    getelementptr inbounds [10 x [11 x [12 x bfloat]]], ptr %[[ARG0]], i64 0, i64 %[[VAL_366:.*]], i64 %[[VAL_368:.*]], i64 %[[VAL_370:.*]]
+// CHECK-DAG:    getelementptr i16, ptr %[[ARG1]], i64 %[[VAL_353:.*]]
+// CHECK-DAG:    getelementptr i16, ptr %[[ARG2]], i64 %[[VAL_353]]
+// CHECK-DAG:    getelementptr inbounds [10 x [11 x [12 x i16]]], ptr %[[ARG0]], i64 0, i64 %[[VAL_366:.*]], i64 %[[VAL_368:.*]], i64 %[[VAL_370:.*]]
 
 HloModule SingleInplaceDusWithTransposeBitcastToTheRoot, is_scheduled=true, input_output_alias={ {}: (0, {}) }
 
@@ -280,9 +280,9 @@ ENTRY main {
 // CHECK:      [[DUS0]].in_bounds-after:
 // CHECK-NEXT:   ret void
 // CHECK:      [[DUS0]].in_bounds-true
-// CHECK-DAG:    getelementptr bfloat, ptr %[[ARG1]], i64 %[[VAL_408:.*]]
-// CHECK-DAG:    getelementptr bfloat, ptr %[[ARG2]], i64 %[[VAL_408:.*]]
-// CHECK-DAG:    getelementptr inbounds [10 x [11 x [12 x bfloat]]], ptr %[[ARG0]], i64 0, i64 %[[VAL_421:.*]], i64 %[[VAL_423:.*]], i64 %[[VAL_425:.*]]
+// CHECK-DAG:    getelementptr i16, ptr %[[ARG1]], i64 %[[VAL_408:.*]]
+// CHECK-DAG:    getelementptr i16, ptr %[[ARG2]], i64 %[[VAL_408:.*]]
+// CHECK-DAG:    getelementptr inbounds [10 x [11 x [12 x i16]]], ptr %[[ARG0]], i64 0, i64 %[[VAL_421:.*]], i64 %[[VAL_423:.*]], i64 %[[VAL_425:.*]]
 
 HloModule SingleInplaceDusWithReshapeBitcastToTheRoot, is_scheduled=true, input_output_alias={ {}: (0, {}) }
 
@@ -319,9 +319,9 @@ ENTRY main {
 // CHECK:      [[DUS0]].in_bounds-after:
 // CHECK-NEXT:   ret void
 // CHECK:      [[DUS0]].in_bounds-true
-// CHECK-DAG:    getelementptr bfloat, ptr %[[ARG1]], i64 %[[VAL_468:.*]]
-// CHECK-DAG:    getelementptr bfloat, ptr %[[ARG2]], i64 %[[VAL_468]]
-// CHECK-DAG:    getelementptr inbounds [10 x [6 x [2 x [11 x bfloat]]]], ptr %[[ARG0]], i64 0, i64 %[[VAL_483:.*]], i64 %[[VAL_485:.*]], i64 %[[VAL_487:.*]], i64 %[[VAL_489:.*]]
+// CHECK-DAG:    getelementptr i16, ptr %[[ARG1]], i64 %[[VAL_468:.*]]
+// CHECK-DAG:    getelementptr i16, ptr %[[ARG2]], i64 %[[VAL_468]]
+// CHECK-DAG:    getelementptr inbounds [10 x [6 x [2 x [11 x i16]]]], ptr %[[ARG0]], i64 0, i64 %[[VAL_483:.*]], i64 %[[VAL_485:.*]], i64 %[[VAL_487:.*]], i64 %[[VAL_489:.*]]
 
 HloModule SingleInplaceDusWithBitcastToTheRootAndFromTheParameter, is_scheduled=true, input_output_alias={ {}: (0, {}) }
 
diff --git a/xla/service/gpu/topk_specializer.cc b/xla/service/gpu/topk_specializer.cc
index bd01a076cc..a99d49e837 100644
--- a/xla/service/gpu/topk_specializer.cc
+++ b/xla/service/gpu/topk_specializer.cc
@@ -106,7 +106,10 @@ class SpecializeTopkVisitor : public DfsHloRewriteVisitor {
 absl::StatusOr<bool> TopkSpecializer::Run(
     HloModule* module,
     const absl::flat_hash_set<absl::string_view>& execution_threads) {
-  return SpecializeTopkVisitor().RunOnModule(module, execution_threads);
+  // SYCL TODO: disable this pass until sycl topk custom call is ready
+  // SYCL extension should implement custom_call_target="__gpu$TopK" through FFI
+  return false;
+  // return SpecializeTopkVisitor().RunOnModule(module, execution_threads);
 }
 
 }  // namespace gpu
diff --git a/xla/service/llvm_ir/llvm_util.cc b/xla/service/llvm_ir/llvm_util.cc
index 1992f0dea0..e72edb2840 100644
--- a/xla/service/llvm_ir/llvm_util.cc
+++ b/xla/service/llvm_ir/llvm_util.cc
@@ -227,16 +227,24 @@ llvm::Type* PrimitiveTypeToIrType(PrimitiveType element_type,
       return llvm::Type::getInt8Ty(module->getContext());
     case S16:
     case U16:
+    case BF16:
+      // For BF16 we just need some type that is 16 bits wide so that it will
+      // take up the right amount of space in memory. LLVM does not have a BF16
+      // type (the LLVM half type is IEEE 16 bit floating point, not bfloat), so
+      // we can't map it directly to an LLVM type. We will not map a BF16
+      // addition to an addition on this type (int16_t) - this is just the type
+      // used for storage.
+      // SYCL: revert a883b54870bc7ce24e510f941d458a354de67d66 to avoid 
+      // Unexpected llvm intrinsic like llvm.fabs.bf16 & llvm.floor.bf16
       return llvm::Type::getInt16Ty(module->getContext());
     case F8E5M2:
     case F8E5M2FNUZ:
     case F8E4M3FN:
     case F8E4M3B11FNUZ:
     case F8E4M3FNUZ:
-      // We represent F8 as an int since there is no LLVM F8 dtype.
+      // Similarly as with BF16, we represent F8 as an int since there is no
+      // LLVM F8 dtype.
       return llvm::Type::getInt8Ty(module->getContext());
-    case BF16:
-      return llvm::Type::getBFloatTy(module->getContext());
     case F16:
       return llvm::Type::getHalfTy(module->getContext());
     case S32:
diff --git a/xla/service/scatter_promotion.cc b/xla/service/scatter_promotion.cc
new file mode 100644
index 0000000000..200845ff17
--- /dev/null
+++ b/xla/service/scatter_promotion.cc
@@ -0,0 +1,142 @@
+/* Copyright 2022 The OpenXLA Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "xla/service/scatter_promotion.h"
+
+#include <memory>
+#include <optional>
+#include <string>
+#include <utility>
+
+#include "xla/service/hlo_creation_utils.h"
+
+namespace xla {
+namespace {
+
+std::optional<PrimitiveType> Get16bitFloatOperandType(
+    const HloInstruction* instr) {
+  std::optional<PrimitiveType> type;
+  for (const HloInstruction* operand : instr->operands()) {
+    if (!type.has_value() && (operand->shape().element_type() == F16 ||
+                              operand->shape().element_type() == BF16)) {
+      type = operand->shape().element_type();
+    }
+  }
+  if (!type.has_value()) {
+    return std::nullopt;
+  }
+  return type;
+}
+
+bool IsScatter(const HloInstruction* inst) {
+  return inst->opcode() == HloOpcode::kScatter;
+}
+
+std::unique_ptr<HloInstruction> Clone16bitScatter(
+    const HloInstruction* inst, const Shape& shape,
+    absl::Span<HloInstruction* const> operands) {
+  // clone an scatter and also clone the attached computation to match the type.
+  std::unique_ptr<HloInstruction> new_inst =
+      inst->CloneWithNewOperands(shape, operands);
+  HloComputation* to_apply = new_inst->to_apply();
+  HloComputation* to_apply_promoted = [&]() {
+    PrimitiveType type = shape.element_type();
+    std::string name = absl::StrCat(to_apply->name(), "_promoted");
+    HloComputation::Builder promoted(name);
+    auto x = promoted.AddInstruction(HloInstruction::CreateParameter(
+        /*parameter_number=*/0, ShapeUtil::MakeShape(type, {}), "x"));
+    auto y = promoted.AddInstruction(HloInstruction::CreateParameter(
+        /*parameter_number=*/1, ShapeUtil::MakeShape(type, {}), "y"));
+    promoted.AddInstruction(HloInstruction::CreateBinary(
+        ShapeUtil::MakeShape(type, {}), to_apply->root_instruction()->opcode(),
+        x, y));
+    return inst->GetModule()->AddEmbeddedComputation(promoted.Build());
+  }();
+  new_inst->set_to_apply(to_apply_promoted);
+  return new_inst;
+}
+
+}  // namespace
+
+StatusOr<bool> ChangeMultiInputOpDataType::Run(
+    HloModule* module,
+    const absl::flat_hash_set<absl::string_view>& execution_threads) {
+  bool changed = false;
+  HloCloner default_cloner = [](const HloInstruction* inst, const Shape& shape,
+                                absl::Span<HloInstruction* const> operands) {
+    return inst->CloneWithNewOperands(shape, operands);
+  };
+  HloCloner cloner = cloner_ ? cloner_ : default_cloner;
+
+  for (HloComputation* comp :
+       module->MakeNonfusionComputations(execution_threads)) {
+    for (HloInstruction* instr : comp->MakeInstructionPostOrder()) {
+      std::optional<PrimitiveType> operand_type =
+          Get16bitFloatOperandType(instr);
+      if (!op_matcher_(instr) || !operand_type.has_value() ||
+          !instr->shape().IsArray() ||
+          instr->opcode() == HloOpcode::kParameter ||
+          (op_matcher_(instr) &&
+           instr->to_apply()->root_instruction()->operand_count() != 2)) {
+        continue;
+      }
+      const PrimitiveType from_type = *operand_type;
+      auto it = to_type_map_.find(from_type);
+      if (it == to_type_map_.end()) {
+        continue;
+      }
+
+      const PrimitiveType to_type = it->second;
+      absl::InlinedVector<HloInstruction*, 8> new_operands;
+
+      for (HloInstruction* operand : instr->mutable_operands()) {
+        const PrimitiveType cur_operand_type = operand->shape().element_type();
+        auto it = to_type_map_.find(cur_operand_type);
+        if (it == to_type_map_.end()) {
+          // Remain datatype of the operands which mismatch the types in
+          // from_ty.
+          new_operands.push_back(operand);
+        } else {
+          // Change datatype of the operands which match one of the types in
+          // from_ty.
+          new_operands.push_back(MakeConvertToHlo(operand, to_type));
+        }
+      }
+
+      Shape new_shape = instr->shape();
+      new_shape.set_element_type(to_type);
+      HloInstruction* new_instr =
+          comp->AddInstruction(cloner(instr, new_shape, new_operands));
+      TF_RETURN_IF_ERROR(comp->ReplaceInstruction(
+          instr, MakeConvertToHlo(new_instr, from_type)));
+      changed = true;
+    }
+  }
+  return changed;
+}
+
+// Promote 16-bit float scatter to 32-bit float types.
+// {{F16, F32}, {BF16, F32}}
+ScatterPromotion::ScatterPromotion(
+    absl::Span<std::pair<PrimitiveType, PrimitiveType> const> from_to_types)
+    : pass_(from_to_types, IsScatter, Clone16bitScatter) {}
+
+StatusOr<bool> ScatterPromotion::Run(
+    HloModule* module,
+    const absl::flat_hash_set<absl::string_view>& execution_threads) {
+  return pass_.Run(module, execution_threads);
+}
+
+}  // namespace xla
diff --git a/xla/service/scatter_promotion.h b/xla/service/scatter_promotion.h
new file mode 100644
index 0000000000..abb13b74f8
--- /dev/null
+++ b/xla/service/scatter_promotion.h
@@ -0,0 +1,106 @@
+/* Copyright 2024 The OpenXLA Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef XLA_SERVICE_SCATTER_PROMOTION_H_
+#define XLA_SERVICE_SCATTER_PROMOTION_H_
+
+#include <functional>
+#include <memory>
+#include <utility>
+
+#include "xla/service/hlo_pass_interface.h"
+
+namespace xla {
+
+// Changes `from_ty op(from_ty a, from_ty b)` into
+// `from_ty convert(op(to_ty convert(a), to_ty convert(b)))`.
+//
+// This path is different from "xla/service/change_op_data_type.h"
+// ChangeOpDataType. ChangeOpDataType only considers ops that match `op_matcher`
+// and where all operands have type `from_ty`.  ChangeOpDataType will not do the
+// correct thing for ops like dynamic-slice where only some of the arguments
+// should be converted; it's up to you to avoid matching such ops with
+// `op_matcher`.
+//
+// The ChangeMultiInputOpDataType pass support multiple input which not all
+// operands have type `from_ty. It will apply the transform only to the operands
+// match one of the types in from_ty.
+//
+// It uses provided `cloner` to clone an instruction with shape and converted
+// operands. If the cloner is not provided, it will uses `CloneWithNewOperands`.
+class ChangeMultiInputOpDataType : public HloModulePass {
+ public:
+  using HloCloner = std::function<std::unique_ptr<HloInstruction>(
+      const HloInstruction*, const Shape&, absl::Span<HloInstruction* const>)>;
+  ChangeMultiInputOpDataType(
+      absl::Span<std::pair<PrimitiveType, PrimitiveType> const> from_to_types,
+      HloPredicate op_matcher, HloCloner cloner = nullptr)
+      : op_matcher_(op_matcher), cloner_(cloner) {
+    for (const std::pair<PrimitiveType, PrimitiveType>& pair : from_to_types) {
+      to_type_map_[pair.first] = pair.second;
+    }
+  }
+
+  ChangeMultiInputOpDataType(PrimitiveType from_ty, PrimitiveType to_ty,
+                             HloPredicate op_matcher,
+                             HloCloner cloner = nullptr)
+      : op_matcher_(op_matcher), cloner_(cloner) {
+    to_type_map_[from_ty] = to_ty;
+  }
+
+  absl::string_view name() const override {
+    return "change-multi-input-op-data-type";
+  }
+  absl::StatusOr<bool> Run(
+      HloModule* module,
+      const absl::flat_hash_set<absl::string_view>& execution_threads) override;
+
+ private:
+  // map with key = from_type and value = to_type.
+  absl::flat_hash_map<PrimitiveType, PrimitiveType> to_type_map_;
+  HloPredicate op_matcher_;
+  HloCloner cloner_;
+};
+
+// This pass rewrites scatter operations to promote 16 bit integer scatter
+// to 32-bit to avoid slow 16-bit atomic.
+//
+// Scatter invoke `EmitAtomicOperationUsingCAS` function for atomicCAS.
+// The function is in "xla/service/gpu/ir_emitter_nested.cc".
+// if the element type is smaller than 32 bits, XLA will use int32_t for the
+// atomicCAS operation. A 32-bit location holds two 16-bit data.
+// In this case, atomicCAS reads and writes 32 bit values from the memory.
+// When competition intensifies, it will be really slow.
+//
+// This pass lets us run the fp16/bf16 scatter as "convert to fp32,
+// run in fp32, then convert back to fp16/bf16".
+class ScatterPromotion : public HloModulePass {
+ public:
+  explicit ScatterPromotion(
+      absl::Span<std::pair<PrimitiveType, PrimitiveType> const> from_to_types);
+  absl::string_view name() const override { return "scatter-promotion"; }
+
+  using HloPassInterface::Run;
+  absl::StatusOr<bool> Run(
+      HloModule* module,
+      const absl::flat_hash_set<absl::string_view>& execution_threads) override;
+
+ private:
+  ChangeMultiInputOpDataType pass_;
+};
+
+}  // namespace xla
+
+#endif  // XLA_SERVICE_SCATTER_PROMOTION_H_
diff --git a/xla/stream_executor/blas.h b/xla/stream_executor/blas.h
index 3f78c95648..01c3c6da38 100644
--- a/xla/stream_executor/blas.h
+++ b/xla/stream_executor/blas.h
@@ -139,6 +139,8 @@ constexpr AlgorithmType kDefaultBlasGemm = -2;
 constexpr AlgorithmType kDefaultBlasGemv = -3;
 constexpr AlgorithmType kNoAlgorithm = -4;
 constexpr AlgorithmType kRuntimeAutotuning = -5;
+constexpr AlgorithmType kXetlaGemm = -6;
+constexpr AlgorithmType kOneDnnGemm = -7;
 
 // blas uses -1 to represent the default algorithm. This happens to match up
 // with the CUBLAS_GEMM_DFALT constant, so cuda_blas.cc is using static_cast
diff --git a/xla/stream_executor/cuda/cuda_driver.cc b/xla/stream_executor/cuda/cuda_driver.cc
index 0b1018f180..c52f712fb2 100644
--- a/xla/stream_executor/cuda/cuda_driver.cc
+++ b/xla/stream_executor/cuda/cuda_driver.cc
@@ -1417,6 +1417,14 @@ struct BitPatternToValue {
       "Feature not supported on CUDA platform (LoadHsaco)");
 }
 
+/* static */ absl::Status GpuDriver::LoadLevelzero(GpuContext* context,
+                                                  const char* spir_contents,
+                                                  const size_t size,
+                                                  CUmodule* module) {
+  return absl::InternalError(
+      "Feature not supported on CUDA platform (LoadLevelzero)");
+}
+
 /* static */ absl::Status GpuDriver::SynchronousMemsetUint8(
     GpuContext* context, CUdeviceptr location, uint8_t value, size_t size) {
   ScopedActivateContext activation(context);
diff --git a/xla/stream_executor/cuda/cuda_executor.cc b/xla/stream_executor/cuda/cuda_executor.cc
index b78d54c116..4012de9b1c 100644
--- a/xla/stream_executor/cuda/cuda_executor.cc
+++ b/xla/stream_executor/cuda/cuda_executor.cc
@@ -206,6 +206,12 @@ absl::Status GpuExecutor::LoadModuleFromHsaco(const char* hsaco,
       "Feature not supported on CUDA platform (LoadModuleFromHsaco)");
 }
 
+absl::Status GpuExecutor::LoadModuleFromSpir(const char* spir, const size_t size,
+                                             CUmodule* module) {
+  return absl::InternalError(
+      "Feature not supported on CUDA platform (LoadModuleFromSpir)");
+}
+
 absl::Status GpuExecutor::GetKernel(const MultiKernelLoaderSpec& spec,
                                     Kernel* kernel) {
   GpuKernel* cuda_kernel = AsGpuKernel(kernel);
diff --git a/xla/stream_executor/gpu/BUILD b/xla/stream_executor/gpu/BUILD
index da5145b822..1104185932 100644
--- a/xla/stream_executor/gpu/BUILD
+++ b/xla/stream_executor/gpu/BUILD
@@ -10,6 +10,10 @@ load(
     "if_rocm",
     "if_rocm_is_configured",
 )
+load(
+    "@local_config_sycl//sycl:build_defs.bzl",
+    "if_sycl_is_configured",
+)
 load(
     "@tsl//tsl/platform:build_config_root.bzl",
     "if_static",
@@ -171,6 +175,8 @@ gpu_only_cc_library(
         "//xla/stream_executor/cuda:cuda_conditional_kernels",
     ]) + if_rocm_is_configured([
         "//xla/stream_executor/rocm:hip_conditional_kernels",
+    ]) + if_sycl_is_configured([
+        "@intel_extension_for_openxla//xla/stream_executor/sycl:sycl_conditional_kernels",
     ]),
 )
 
@@ -422,6 +428,8 @@ gpu_only_cc_library(
         "@local_config_cuda//cuda:cuda_headers",
     ]) + if_rocm_is_configured([
         "@local_config_rocm//rocm:rocm_headers",
+    ]) + if_sycl_is_configured([
+        "@local_config_sycl//sycl:sycl_headers",
     ]),
 )
 
@@ -662,6 +670,38 @@ xla_test(
     ]),
 )
 
+tsl_gpu_library(
+    name = "gpu_malloc_allocator_header",
+    hdrs = ["gpu_malloc_allocator.h"],
+    deps = [
+        "//xla/stream_executor:stream_executor_h",
+        "@com_google_absl//absl/base:core_headers",
+        "@com_google_absl//absl/container:flat_hash_map",
+        "@xla//xla/tsl/framework:allocator",
+        "@xla//xla/tsl/framework:device_id",
+        "@tsl//tsl/platform:mutex",
+    ],
+)
+
+tsl_gpu_library(
+    name = "gpu_malloc_allocator",
+    srcs = [
+        "gpu_malloc_allocator.cc",
+    ],
+    hdrs = ["gpu_malloc_allocator.h"],
+    deps = [
+        ":gpu_init_impl",
+        "//xla/stream_executor:stream_executor_h",
+        "@com_google_absl//absl/base:core_headers",
+        "@com_google_absl//absl/container:flat_hash_map",
+        "@com_google_absl//absl/strings",
+        "@xla//xla/tsl/framework:allocator",
+        "@xla//xla/tsl/framework:device_id",
+        "@tsl//tsl/platform:logging",
+        "@tsl//tsl/platform:mutex",
+    ],
+)
+
 cc_library(
     name = "gpu_blas_lt",
     srcs = ["gpu_blas_lt.cc"],
diff --git a/xla/stream_executor/gpu/gpu_driver.h b/xla/stream_executor/gpu/gpu_driver.h
index ceab15bba6..85bba89e4a 100644
--- a/xla/stream_executor/gpu/gpu_driver.h
+++ b/xla/stream_executor/gpu/gpu_driver.h
@@ -624,6 +624,9 @@ class GpuDriver {
   // (supported on ROCm only)
   static absl::Status LoadHsaco(GpuContext* context, const char* hsaco_contents,
                                 GpuModuleHandle* module);
+  static absl::Status LoadLevelzero(GpuContext* context,
+                                    const char* spir_contents, const size_t size,
+                                    GpuModuleHandle* module);
 
   // Retrieves a named kernel from a loaded module, and places the resulting
   // handle into function (outparam) on success. Neither kernel_name nor
diff --git a/xla/stream_executor/gpu/gpu_executor.h b/xla/stream_executor/gpu/gpu_executor.h
index 31398f60ce..1416d78400 100644
--- a/xla/stream_executor/gpu/gpu_executor.h
+++ b/xla/stream_executor/gpu/gpu_executor.h
@@ -353,6 +353,11 @@ class GpuExecutor : public StreamExecutorCommon {
   absl::Status LoadModuleFromHsaco(const char* hsaco, GpuModuleHandle* module)
       TF_EXCLUSIVE_LOCKS_REQUIRED(in_memory_modules_mu_);
 
+  // (supported on SYCL only)
+  absl::Status LoadModuleFromSpir(const char* spir, const size_t size,
+                                  GpuModuleHandle* module)
+      TF_EXCLUSIVE_LOCKS_REQUIRED(in_memory_modules_mu_);
+
   absl::Status Launch(Stream* stream, const ThreadDim& thread_dims,
                       const BlockDim& block_dims,
                       const std::optional<ClusterDim>& cluster_dims,
diff --git a/xla/stream_executor/gpu/gpu_helpers.h b/xla/stream_executor/gpu/gpu_helpers.h
index c86f49140a..5560795dd4 100644
--- a/xla/stream_executor/gpu/gpu_helpers.h
+++ b/xla/stream_executor/gpu/gpu_helpers.h
@@ -53,14 +53,18 @@ T* GpuMemoryMutable(DeviceMemory<T>* mem) {
 static_assert(
     sizeof(std::complex<float>) == sizeof(GpuComplexType),
     "std::complex<float> and GpuComplexType should have the same size");
+#if !TENSORFLOW_USE_SYCL
 static_assert(offsetof(GpuComplexType, x) == 0,
               "The real part of GpuComplexType should appear first.");
+#endif // !TENSORFLOW_USE_SYCL
 static_assert(
     sizeof(std::complex<double>) == sizeof(GpuDoubleComplexType),
     "std::complex<double> and GpuDoubleComplexType should have the same "
     "size");
+#if !TENSORFLOW_USE_SYCL
 static_assert(offsetof(GpuDoubleComplexType, x) == 0,
               "The real part of GpuDoubleComplexType should appear first.");
+#endif // !TENSORFLOW_USE_SYCL
 
 // Type traits to get CUDA complex types from std::complex<>.
 
diff --git a/xla/stream_executor/gpu/gpu_malloc_allocator.cc b/xla/stream_executor/gpu/gpu_malloc_allocator.cc
new file mode 100644
index 0000000000..e0494d7501
--- /dev/null
+++ b/xla/stream_executor/gpu/gpu_malloc_allocator.cc
@@ -0,0 +1,154 @@
+/* Copyright (c) 2024 Intel Corporation
+
+Copyright 2021 The OpenXLA Authors.
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+    http://www.apache.org/licenses/LICENSE-2.0
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "xla/stream_executor/gpu/gpu_malloc_allocator.h"
+
+#include <atomic>
+#include <cstddef>
+#include <map>
+#include <memory>
+#include <optional>
+#include <string>
+#include <vector>
+
+#include "absl/strings/str_cat.h"
+#include "absl/strings/str_join.h"
+#include "xla/stream_executor/gpu/gpu_init.h"     // IWYU pragma: keep
+#include "xla/stream_executor/stream_executor.h"  // IWYU pragma: keep
+#include "xla/tsl/framework/allocator.h"
+#include "xla/tsl/framework/device_id.h"
+#include "tsl/platform/logging.h"
+#include "tsl/platform/mutex.h"
+
+namespace stream_executor {
+
+std::atomic<int> GpuMallocAllocator::number_instantiated_(0);
+
+GpuMallocAllocator::GpuMallocAllocator(StreamExecutor* executor,
+                                       tsl::PlatformDeviceId platform_device_id,
+                                       bool compute_stats)
+    : stream_exec_(executor),
+      name_(absl::StrCat("gpu", platform_device_id.value())) {
+  ++number_instantiated_;
+
+  if (compute_stats) {
+    stats_ = std::make_unique<tsl::AllocatorStats>();
+  }  // If not set, it means we do not compute stats.
+
+  VLOG(1) << Name() << " GPUMallocAsync initialized on platform: "
+          << platform_device_id.value();
+}
+
+GpuMallocAllocator::~GpuMallocAllocator() {}
+
+void* GpuMallocAllocator::AllocateRaw(size_t alignment, size_t num_bytes) {
+  CHECK(stream_exec_ != nullptr)
+      << "A stream executor must be added to the default gpu allocator";
+  // The lock is only needed when stats are enabled, but it must be around
+  // the cuMemAllocFromPoolAsync call as well to ensure consistency of the stats
+  // update.
+  std::unique_lock<tsl::mutex> lock(lock_, std::defer_lock);
+  if (stats_) {
+    lock.lock();
+  }
+  int64_t count = num_bytes / sizeof(uint8_t);
+  void* ptr = (stream_exec_->AllocateArray<uint8_t>(count, 0)).opaque();
+
+  if (ptr == nullptr) {
+    LOG(ERROR) << Name() << " failed to allocate " << num_bytes << " bytes: ";
+    if (stats_) {
+      LOG(ERROR) << "Stats: " << stats_->DebugString();
+    }
+
+    return nullptr;
+  }
+
+  // Update stats.
+  if (stats_) {
+    ++(stats_->num_allocs);
+    stats_->bytes_in_use += num_bytes;
+    if (stats_->bytes_in_use > stats_->peak_bytes_in_use) {
+      VLOG(9) << "New Peak memory usage of " << stats_->bytes_in_use
+              << " bytes.";
+    }
+    stats_->peak_bytes_in_use =
+        std::max(stats_->peak_bytes_in_use, stats_->bytes_in_use);
+    stats_->largest_alloc_size =
+        std::max<std::size_t>(stats_->largest_alloc_size, num_bytes);
+    bool ptr_inserted = size_map_.emplace(ptr, num_bytes).second;
+    DCHECK(ptr_inserted);
+  }
+  VLOG(1) << Name() << " Allocated " << num_bytes << " at " << ptr;
+  return ptr;
+}
+
+void GpuMallocAllocator::DeallocateRaw(void* ptr) {
+  if (ptr == nullptr) return;
+  // The lock is only needed when stats are enabled, but it must be around
+  // the cuMemFreeAsync call as well to ensure consistency of the stats update.
+  std::unique_lock<tsl::mutex> lock(lock_, std::defer_lock);
+  if (stats_) {
+    lock.lock();
+  }
+  auto device_memory_base = DeviceMemoryBase(ptr);
+  stream_exec_->Deallocate(&device_memory_base);
+
+  // Updates the stats.
+  if (stats_) {
+    DCHECK(size_map_.contains(ptr));
+    size_t size = size_map_[ptr];
+    stats_->bytes_in_use -= size;
+    size_map_.erase(ptr);
+  }
+
+  VLOG(1) << Name() << " Freed ptr: " << ptr;
+}
+
+bool GpuMallocAllocator::TracksAllocationSizes() const {
+  return static_cast<bool>(stats_);
+}
+
+size_t GpuMallocAllocator::RequestedSize(const void* ptr) const {
+  if (!stats_ || !ptr) return 0;
+  tsl::mutex_lock l(lock_);
+  return size_map_.at(ptr);
+}
+
+size_t GpuMallocAllocator::AllocatedSize(const void* ptr) const {
+  if (!stats_ || !ptr) return 0;
+  tsl::mutex_lock l(lock_);
+  return size_map_.at(ptr);
+}
+
+std::optional<tsl::AllocatorStats> GpuMallocAllocator::GetStats() {
+  if (!stats_) return std::nullopt;
+  tsl::mutex_lock l(lock_);
+  return *stats_;
+}
+
+bool GpuMallocAllocator::ClearStats() {
+  if (!stats_) return false;
+  tsl::mutex_lock l(lock_);
+  stats_->num_allocs = 0;
+  stats_->peak_bytes_in_use = stats_->bytes_in_use;
+  stats_->largest_alloc_size = 0;
+  return true;
+}
+
+void GpuMallocAllocator::SetStreamAndPreallocateMemory(void* stream) {
+  LOG(FATAL) <<  // Crash OK.
+      "Unimplemented. ";
+}
+
+}  // namespace stream_executor
diff --git a/xla/stream_executor/gpu/gpu_malloc_allocator.h b/xla/stream_executor/gpu/gpu_malloc_allocator.h
new file mode 100644
index 0000000000..f82130a56b
--- /dev/null
+++ b/xla/stream_executor/gpu/gpu_malloc_allocator.h
@@ -0,0 +1,83 @@
+/* Copyright (c) 2024 Intel Corporation
+
+Copyright 2021 The OpenXLA Authors.
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+    http://www.apache.org/licenses/LICENSE-2.0
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef XLA_STREAM_EXECUTOR_GPU_GPU_MALLOC_ALLOCATOR_H_
+#define XLA_STREAM_EXECUTOR_GPU_GPU_MALLOC_ALLOCATOR_H_
+
+#include <atomic>
+#include <cstddef>
+#include <memory>
+#include <optional>
+#include <string>
+
+#include "absl/base/thread_annotations.h"
+#include "absl/container/flat_hash_map.h"
+#include "xla/stream_executor/stream_executor.h"  // IWYU pragma: keep
+#include "xla/tsl/framework/allocator.h"
+#include "xla/tsl/framework/device_id.h"
+#include "tsl/platform/mutex.h"
+
+namespace stream_executor {
+
+class GpuMallocAllocator : public tsl::Allocator {
+ public:
+  explicit GpuMallocAllocator(StreamExecutor* executor,
+                              tsl::PlatformDeviceId platform_device_id,
+                              bool compute_stats = true);
+  ~GpuMallocAllocator() override;
+  std::string Name() override { return name_; }
+  void* AllocateRaw(size_t alignment,
+                    size_t num_bytes) override ABSL_NO_THREAD_SAFETY_ANALYSIS;
+  void DeallocateRaw(void* ptr) override ABSL_NO_THREAD_SAFETY_ANALYSIS;
+
+  bool TracksAllocationSizes() const override;
+
+  size_t RequestedSize(const void* ptr) const override;
+
+  size_t AllocatedSize(const void* ptr) const override;
+
+  std::optional<tsl::AllocatorStats> GetStats() override;
+
+  bool ClearStats() override;
+
+  void SetStreamAndPreallocateMemory(void* stream) override;
+
+  static int GetInstantiatedCountTestOnly() { return number_instantiated_; }
+
+  tsl::AllocatorMemoryType GetMemoryType() const override {
+    return tsl::AllocatorMemoryType::kDevice;
+  }
+
+ private:
+  StreamExecutor* stream_exec_;
+
+  // Just a counter for the number of time this class is instantiated.
+  // Only useful for tests.
+  static std::atomic<int> number_instantiated_;
+
+  std::string name_;
+
+  GpuMallocAllocator(const GpuMallocAllocator&) = delete;
+  void operator=(const GpuMallocAllocator&) = delete;
+
+  // Stats.
+  // Structures mutable after construction
+  mutable tsl::mutex lock_;
+  std::unique_ptr<tsl::AllocatorStats> stats_ ABSL_PT_GUARDED_BY(lock_);
+  absl::flat_hash_map<const void*, size_t> size_map_ ABSL_GUARDED_BY(lock_);
+};
+
+}  // namespace stream_executor
+
+#endif  // XLA_STREAM_EXECUTOR_GPU_GPU_MALLOC_ALLOCATOR_H_
diff --git a/xla/stream_executor/gpu/gpu_timer_kernel.h b/xla/stream_executor/gpu/gpu_timer_kernel.h
index cb0c5d1a3c..94aae0dd3e 100644
--- a/xla/stream_executor/gpu/gpu_timer_kernel.h
+++ b/xla/stream_executor/gpu/gpu_timer_kernel.h
@@ -25,8 +25,15 @@ namespace stream_executor::gpu {
 // Returns true if the current backend and GPU supports the delay kernel for
 // time measurement. It might return an error if checking for the support at
 // runtime failed.
+#if !TENSORFLOW_USE_SYCL
 absl::StatusOr<bool> DelayKernelIsSupported(GpuStream* stream);
-
+#else
+// SYCL TODO: this file will be removed in subsequent xla versions
+// and only supported for CUDA
+absl::StatusOr<bool> DelayKernelIsSupported(GpuStream* stream){
+    return false;
+}
+#endif
 // Launches the delay kernel on the given stream. The caller is responsible for
 // keeping the returned semaphore alive until the kernel finished executing.
 // Setting the semaphore to `kRelease` makes the kernel quit.
diff --git a/xla/stream_executor/gpu/gpu_types.h b/xla/stream_executor/gpu/gpu_types.h
index 3964b0777e..81a9228911 100644
--- a/xla/stream_executor/gpu/gpu_types.h
+++ b/xla/stream_executor/gpu/gpu_types.h
@@ -18,7 +18,12 @@ limitations under the License.
 #ifndef XLA_STREAM_EXECUTOR_GPU_GPU_TYPES_H_
 #define XLA_STREAM_EXECUTOR_GPU_GPU_TYPES_H_
 
-#if TENSORFLOW_USE_ROCM
+#if TENSORFLOW_USE_SYCL
+
+#include <sycl/sycl.hpp>
+#include <complex>
+
+#elif TENSORFLOW_USE_ROCM
 
 #define __HIP_DISABLE_CPP_FUNCTIONS__
 
@@ -40,7 +45,34 @@ namespace gpu {
 // current CUDA/HIP version.
 struct UnsupportedGpuFeature {};
 
-#if TENSORFLOW_USE_ROCM
+#if TENSORFLOW_USE_SYCL
+typedef struct SYCLEventWrapper {
+  ::sycl::event* event;
+  ::sycl::queue* queue;
+} EventWrapper;
+
+using GpuContextHandle = const void*;
+using GpuStreamHandle = ::sycl::queue*;
+using GpuEventHandle = EventWrapper*;
+using GpuFunctionHandle = ::sycl::kernel*;
+using GpuFunctionAttribute = const void*;
+using GpuDeviceHandle = ::sycl::device*;
+using GpuDevicePtr = void*;
+using GpuDeviceAttribute = const void*;
+using GpuDeviceProperty = const void*;
+using GpuModuleHandle = ze_module_handle_t;
+using GpuStatus = const void*;
+using GpuFuncCachePreference = const void*;
+using GpuSharedMemConfig = const void*;
+using GpuComplexType = std::complex<float>;
+using GpuDoubleComplexType = std::complex<double>;
+using GpuRngHandle = const void*;
+using GpuGraphHandle = const void*;
+using GpuGraphExecHandle = const void*;
+using GpuGraphNodeHandle = const void*;
+using GpuGraphConditionalHandle = UnsupportedGpuFeature;
+
+#elif TENSORFLOW_USE_ROCM
 
 using GpuStreamHandle = hipStream_t;
 using GpuEventHandle = hipEvent_t;
diff --git a/xla/stream_executor/gpu/redzone_allocator.cc b/xla/stream_executor/gpu/redzone_allocator.cc
index f15b543167..55f75472f2 100644
--- a/xla/stream_executor/gpu/redzone_allocator.cc
+++ b/xla/stream_executor/gpu/redzone_allocator.cc
@@ -254,6 +254,7 @@ static absl::StatusOr<RedzoneCheckStatus> CheckRedzonesForBuffer(
   return RedzoneCheckStatus::OK();
 }
 
+#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM
 absl::StatusOr<RedzoneCheckStatus> RedzoneAllocator::CheckRedzones() const {
   StreamExecutor* executor = stream_->parent();
 
@@ -280,6 +281,7 @@ absl::StatusOr<RedzoneCheckStatus> RedzoneAllocator::CheckRedzones() const {
 
   return RedzoneCheckStatus::OK();
 }
+#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM
 
 std::string RedzoneCheckStatus::RedzoneFailureMsg() const {
   return absl::StrFormat(
diff --git a/xla/stream_executor/kernel_spec.h b/xla/stream_executor/kernel_spec.h
index 934714ef41..8c95c6b8e6 100644
--- a/xla/stream_executor/kernel_spec.h
+++ b/xla/stream_executor/kernel_spec.h
@@ -167,9 +167,12 @@ class CudaCubinInMemory : public KernelLoaderSpec {
                     absl::string_view kernel_name);
 
   absl::Span<const uint8_t> cubin_bytes() const { return cubin_bytes_; }
+  const int size() const { return size_; }
 
  private:
   absl::Span<const uint8_t> cubin_bytes_;
+  // SYCL: this is needed only for SPIRV
+  int size_;
 
   CudaCubinInMemory(const CudaCubinInMemory &) = delete;
   void operator=(const CudaCubinInMemory &) = delete;
diff --git a/xla/stream_executor/rocm/rocm_driver.cc b/xla/stream_executor/rocm/rocm_driver.cc
index 0b0871a0fd..2eacf696c7 100644
--- a/xla/stream_executor/rocm/rocm_driver.cc
+++ b/xla/stream_executor/rocm/rocm_driver.cc
@@ -1171,6 +1171,14 @@ struct BitPatternToValue {
   return ret;
 }
 
+/* static */ absl::Status GpuDriver::LoadLevelzero(GpuContext* context,
+                                                  const char* spir_contents,
+                                                  const size_t size,
+                                                  hipModule_t* module) {
+  return absl::InternalError(
+      "Feature not supported on ROCm platform (LoadLevelzero)");
+}
+
 /* static */ absl::Status GpuDriver::SynchronousMemsetUint8(
     GpuContext* context, hipDeviceptr_t location, uint8 value, size_t size) {
   ScopedActivateContext activation{context};
diff --git a/xla/stream_executor/rocm/rocm_executor.cc b/xla/stream_executor/rocm/rocm_executor.cc
index dc9c30630d..35b458b21b 100644
--- a/xla/stream_executor/rocm/rocm_executor.cc
+++ b/xla/stream_executor/rocm/rocm_executor.cc
@@ -458,6 +458,11 @@ absl::Status GpuExecutor::LoadModuleFromHsaco(const char* hsaco,
   return absl::OkStatus();
 }
 
+tsl::Status GpuExecutor::LoadModuleFromSpir(const char* spir, const size_t size,
+                                            hipModule_t* module) {
+  LOG(FATAL) << "Feature not supported on ROCM platform (LoadModuleFromSpir)";
+}
+
 // This is a non-essential operation; if there's a failure, proceed without
 // logging an error. It's nearly certain that in case of failures, we'd never
 // get here in the first place; these are very low-impact routines.
diff --git a/xla/tsl/tsl.bzl b/xla/tsl/tsl.bzl
index 7ae66aa2ec..77545abc3b 100644
--- a/xla/tsl/tsl.bzl
+++ b/xla/tsl/tsl.bzl
@@ -36,6 +36,10 @@ load(
     "@local_config_tensorrt//:build_defs.bzl",
     "if_tensorrt",
 )
+load(
+    "@local_config_sycl//sycl:build_defs.bzl",
+    "if_sycl_is_configured",
+)
 
 # Internally this loads a macro, but in OSS this is a function
 # buildifier: disable=out-of-order-load
@@ -296,7 +300,7 @@ def tsl_copts(
         if_tensorrt(["-DGOOGLE_TENSORRT=1"]) +
         if_rocm(["-DTENSORFLOW_USE_ROCM=1"]) +
         # Compile in oneDNN based ops when building for x86 platforms
-        if_mkl(["-DINTEL_MKL"]) +
+        if_sycl_is_configured([], if_mkl(["-DINTEL_MKL"])) +
         # Enable additional ops (e.g., ops with non-NHWC data layout) and
         # optimizations for Intel builds using oneDNN if configured
         if_enable_mkl(["-DENABLE_MKL"]) +
