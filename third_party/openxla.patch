diff --git a/third_party/tsl/third_party/eigen3/eigen.patch b/third_party/tsl/third_party/eigen3/eigen.patch
new file mode 100644
index 0000000000..8954b1d2e3
--- /dev/null
+++ b/third_party/tsl/third_party/eigen3/eigen.patch
@@ -0,0 +1,13 @@
+diff --git a/Eigen/src/Core/util/Macros.h b/Eigen/src/Core/util/Macros.h
+index 69bbf2e73..251053b55 100644
+--- a/Eigen/src/Core/util/Macros.h
++++ b/Eigen/src/Core/util/Macros.h
+@@ -686,7 +686,7 @@
+ // For instance, if compiling with gcc and -std=c++17, then EIGEN_COMP_CXXVER
+ // is defined to 17.
+ #if EIGEN_CPLUSPLUS >= 202002L
+-  #define EIGEN_COMP_CXXVER 20
++  #define EIGEN_COMP_CXXVER 17
+ #elif EIGEN_CPLUSPLUS >= 201703L
+   #define EIGEN_COMP_CXXVER 17
+ #elif EIGEN_CPLUSPLUS >= 201402L
diff --git a/third_party/tsl/third_party/eigen3/workspace.bzl b/third_party/tsl/third_party/eigen3/workspace.bzl
index 027454e46d..ca5489940e 100644
--- a/third_party/tsl/third_party/eigen3/workspace.bzl
+++ b/third_party/tsl/third_party/eigen3/workspace.bzl
@@ -14,6 +14,7 @@ def repo():
     tf_http_archive(
         name = "eigen_archive",
         build_file = "//third_party/eigen3:eigen_archive.BUILD",
+        patch_file = ["//third_party/eigen3:eigen.patch"],
         sha256 = EIGEN_SHA256,
         strip_prefix = "eigen-{commit}".format(commit = EIGEN_COMMIT),
         urls = tf_mirror_urls("https://gitlab.com/libeigen/eigen/-/archive/{commit}/eigen-{commit}.tar.gz".format(commit = EIGEN_COMMIT)),
diff --git a/third_party/tsl/third_party/gpus/cuda_configure.bzl b/third_party/tsl/third_party/gpus/cuda_configure.bzl
index 1665388439..e88f55019d 100644
--- a/third_party/tsl/third_party/gpus/cuda_configure.bzl
+++ b/third_party/tsl/third_party/gpus/cuda_configure.bzl
@@ -26,7 +26,7 @@
   * `PYTHON_BIN_PATH`: The python binary path
 """
 
-load("//third_party/clang_toolchain:download_clang.bzl", "download_clang")
+load("@xla//third_party/tsl/third_party/clang_toolchain:download_clang.bzl", "download_clang")
 load(
     "@bazel_tools//tools/cpp:lib_cc_configure.bzl",
     "escape_string",
@@ -39,7 +39,7 @@ load(
     "setup_vc_env_vars",
 )
 load(
-    "//third_party/remote_config:common.bzl",
+    "@xla//third_party/tsl/third_party/remote_config:common.bzl",
     "config_repo_label",
     "err_out",
     "execute",
diff --git a/third_party/tsl/third_party/grpc/c_ares.patch b/third_party/tsl/third_party/grpc/c_ares.patch
new file mode 100644
index 0000000000..752965f933
--- /dev/null
+++ b/third_party/tsl/third_party/grpc/c_ares.patch
@@ -0,0 +1,200 @@
+diff --git a/bazel/grpc_deps.bzl b/bazel/grpc_deps.bzl
+index c2f2f43820..72de3cea94 100644
+--- a/bazel/grpc_deps.bzl
++++ b/bazel/grpc_deps.bzl
+@@ -182,9 +182,9 @@ def grpc_deps():
+         http_archive(
+             name = "com_github_cares_cares",
+             build_file = "@com_github_grpc_grpc//third_party:cares/cares.BUILD",
+-            sha256 = "e8c2751ddc70fed9dc6f999acd92e232d5846f009ee1674f8aee81f19b2b915a",
+-            strip_prefix = "c-ares-e982924acee7f7313b4baa4ee5ec000c5e373c30",
+-            url = "https://github.com/c-ares/c-ares/archive/e982924acee7f7313b4baa4ee5ec000c5e373c30.tar.gz",
++            sha256 = "321700399b72ed0e037d0074c629e7741f6b2ec2dda92956abe3e9671d3e268e",
++            strip_prefix = "c-ares-1.19.1",
++            url = "https://github.com/c-ares/c-ares/releases/download/cares-1_19_1/c-ares-1.19.1.tar.gz",
+         )
+
+     if "com_google_absl" not in native.existing_rules():
+diff --git a/third_party/cares/cares.BUILD b/third_party/cares/cares.BUILD
+index 203712b182..2561b1a4bc 100644
+--- a/third_party/cares/cares.BUILD
++++ b/third_party/cares/cares.BUILD
+@@ -109,84 +109,95 @@ genrule(
+ cc_library(
+     name = "ares",
+     srcs = [
+-        "ares__close_sockets.c",
+-        "ares__get_hostent.c",
+-        "ares__read_line.c",
+-        "ares__timeval.c",
+-        "ares_cancel.c",
+-        "ares_create_query.c",
+-        "ares_data.c",
+-        "ares_destroy.c",
+-        "ares_expand_name.c",
+-        "ares_expand_string.c",
+-        "ares_fds.c",
+-        "ares_free_hostent.c",
+-        "ares_free_string.c",
+-        "ares_getenv.c",
+-        "ares_gethostbyaddr.c",
+-        "ares_gethostbyname.c",
+-        "ares_getnameinfo.c",
+-        "ares_getopt.c",
+-        "ares_getsock.c",
+-        "ares_init.c",
+-        "ares_library_init.c",
+-        "ares_llist.c",
+-        "ares_mkquery.c",
+-        "ares_nowarn.c",
+-        "ares_options.c",
+-        "ares_parse_a_reply.c",
+-        "ares_parse_aaaa_reply.c",
+-        "ares_parse_mx_reply.c",
+-        "ares_parse_naptr_reply.c",
+-        "ares_parse_ns_reply.c",
+-        "ares_parse_ptr_reply.c",
+-        "ares_parse_soa_reply.c",
+-        "ares_parse_srv_reply.c",
+-        "ares_parse_txt_reply.c",
+-        "ares_platform.c",
+-        "ares_process.c",
+-        "ares_query.c",
+-        "ares_search.c",
+-        "ares_send.c",
+-        "ares_strcasecmp.c",
+-        "ares_strdup.c",
+-        "ares_strsplit.c",
+-        "ares_strerror.c",
+-        "ares_timeout.c",
+-        "ares_version.c",
+-        "ares_writev.c",
+-        "bitncmp.c",
+-        "inet_net_pton.c",
+-        "inet_ntop.c",
+-        "windows_port.c",
++        "src/lib/ares__read_line.c",
++        "src/lib/ares__get_hostent.c",
++        "src/lib/ares__close_sockets.c",
++        "src/lib/ares__timeval.c",
++        "src/lib/ares_gethostbyaddr.c",
++        "src/lib/ares_getenv.c",
++        "src/lib/ares_free_string.c",
++        "src/lib/ares_free_hostent.c",
++        "src/lib/ares_fds.c",
++        "src/lib/ares_expand_string.c",
++        "src/lib/ares_create_query.c",
++        "src/lib/ares_cancel.c",
++        "src/lib/ares_android.c",
++        "src/lib/ares_parse_txt_reply.c",
++        "src/lib/ares_parse_srv_reply.c",
++        "src/lib/ares_parse_soa_reply.c",
++        "src/lib/ares_parse_ptr_reply.c",
++        "src/lib/ares_parse_ns_reply.c",
++        "src/lib/ares_parse_naptr_reply.c",
++        "src/lib/ares_parse_mx_reply.c",
++        "src/lib/ares_parse_caa_reply.c",
++        "src/lib/ares_options.c",
++        "src/lib/ares_nowarn.c",
++        "src/lib/ares_mkquery.c",
++        "src/lib/ares_llist.c",
++        "src/lib/ares_getsock.c",
++        "src/lib/ares_getnameinfo.c",
++        "src/lib/bitncmp.c",
++        "src/lib/ares_writev.c",
++        "src/lib/ares_version.c",
++        "src/lib/ares_timeout.c",
++        "src/lib/ares_strerror.c",
++        "src/lib/ares_strcasecmp.c",
++        "src/lib/ares_search.c",
++        "src/lib/ares_platform.c",
++        "src/lib/windows_port.c",
++        "src/lib/inet_ntop.c",
++        "src/lib/ares__sortaddrinfo.c",
++        "src/lib/ares__readaddrinfo.c",
++        "src/lib/ares_parse_uri_reply.c",
++        "src/lib/ares__parse_into_addrinfo.c",
++        "src/lib/ares_parse_a_reply.c",
++        "src/lib/ares_parse_aaaa_reply.c",
++        "src/lib/ares_library_init.c",
++        "src/lib/ares_init.c",
++        "src/lib/ares_gethostbyname.c",
++        "src/lib/ares_getaddrinfo.c",
++        "src/lib/ares_freeaddrinfo.c",
++        "src/lib/ares_expand_name.c",
++        "src/lib/ares_destroy.c",
++        "src/lib/ares_data.c",
++        "src/lib/ares__addrinfo_localhost.c",
++        "src/lib/ares__addrinfo2hostent.c",
++        "src/lib/inet_net_pton.c",
++        "src/lib/ares_strsplit.c",
++        "src/lib/ares_strdup.c",
++        "src/lib/ares_send.c",
++        "src/lib/ares_rand.c",
++        "src/lib/ares_query.c",
++        "src/lib/ares_process.c",
+     ],
+     hdrs = [
+-        "ares.h",
+         "ares_build.h",
+         "ares_config.h",
+-        "ares_data.h",
+-        "ares_dns.h",
+-        "ares_getenv.h",
+-        "ares_getopt.h",
+-        "ares_inet_net_pton.h",
+-        "ares_iphlpapi.h",
+-        "ares_ipv6.h",
+-        "ares_library_init.h",
+-        "ares_llist.h",
+-        "ares_nowarn.h",
+-        "ares_platform.h",
+-        "ares_private.h",
+-        "ares_rules.h",
+-        "ares_setup.h",
+-        "ares_strcasecmp.h",
+-        "ares_strdup.h",
+-        "ares_strsplit.h",
+-        "ares_version.h",
+-        "ares_writev.h",
+-        "bitncmp.h",
+-        "config-win32.h",
+-        "nameser.h",
+-        "setup_once.h",
++        "include/ares_version.h",
++        "include/ares.h",
++        "include/ares_rules.h",
++        "include/ares_dns.h",
++        "include/ares_nameser.h",
++        "src/tools/ares_getopt.h",
++        "src/lib/ares_strsplit.h",
++        "src/lib/ares_android.h",
++        "src/lib/ares_private.h",
++        "src/lib/ares_llist.h",
++        "src/lib/ares_platform.h",
++        "src/lib/ares_ipv6.h",
++        "src/lib/config-dos.h",
++        "src/lib/bitncmp.h",
++        "src/lib/ares_strcasecmp.h",
++        "src/lib/setup_once.h",
++        "src/lib/ares_inet_net_pton.h",
++        "src/lib/ares_data.h",
++        "src/lib/ares_getenv.h",
++        "src/lib/config-win32.h",
++        "src/lib/ares_strdup.h",
++        "src/lib/ares_iphlpapi.h",
++        "src/lib/ares_setup.h",
++        "src/lib/ares_writev.h",
++        "src/lib/ares_nowarn.h",
+     ],
+     copts = [
+         "-D_GNU_SOURCE",
+@@ -202,7 +213,7 @@ cc_library(
+         "//conditions:default": [],
+     }),
+     defines = ["CARES_STATICLIB"],
+-    includes = ["."],
++    includes = ["include", "."],
+     linkopts = select({
+         ":windows": ["-defaultlib:ws2_32.lib"],
+         "//conditions:default": [],
diff --git a/third_party/tsl/third_party/grpc/upb_platform_fix.patch b/third_party/tsl/third_party/grpc/upb_platform_fix.patch
index 6edd66067e..022c9d1557 100644
--- a/third_party/tsl/third_party/grpc/upb_platform_fix.patch
+++ b/third_party/tsl/third_party/grpc/upb_platform_fix.patch
@@ -11,3 +11,12 @@ index ad85b202..2311b2e4 100644
  )
 
  config_setting(
+@@ -24,7 +24,7 @@ exports_files([
+
+ CPPOPTS = [
+     # copybara:strip_for_google3_begin
+-    "-Werror",
++    # "-Werror",
+     "-Wno-long-long",
+     # copybara:strip_end
+ ]
diff --git a/third_party/tsl/third_party/llvm/build.patch b/third_party/tsl/third_party/llvm/build.patch
index 479e08cde8..33f585b709 100644
--- a/third_party/tsl/third_party/llvm/build.patch
+++ b/third_party/tsl/third_party/llvm/build.patch
@@ -44,3 +44,24 @@ index 7770284e5543..0b45127495dc 100644
          "//conditions:default": [
              "BLAKE3_NO_AVX2",
              "BLAKE3_NO_AVX512",
+diff --git a/mlir/lib/Bytecode/Reader/BytecodeReader.cpp b/mlir/lib/Bytecode/Reader/BytecodeReader.cpp
+index 177372c68046..40d49dc13b2f 100644
+--- a/mlir/lib/Bytecode/Reader/BytecodeReader.cpp
++++ b/mlir/lib/Bytecode/Reader/BytecodeReader.cpp
+@@ -1549,6 +1549,8 @@ private:
+   const std::shared_ptr<llvm::SourceMgr> &bufferOwnerRef;
+ };
+
++// TODO: Disable clang opt since it crashes with clang-17 compiler.
++#pragma clang optimize off
+ LogicalResult BytecodeReader::Impl::read(
+     Block *block, llvm::function_ref<bool(Operation *)> lazyOpsCallback) {
+   EncodingReader reader(buffer.getBuffer(), fileLoc);
+@@ -1628,6 +1630,7 @@ LogicalResult BytecodeReader::Impl::read(
+   // Finally, process the IR section.
+   return parseIRSection(*sectionDatas[bytecode::Section::kIR], block);
+ }
++#pragma clang optimize on
+
+ LogicalResult BytecodeReader::Impl::parseVersion(EncodingReader &reader) {
+   if (failed(reader.parseVarInt(version)))
diff --git a/third_party/tsl/third_party/llvm/opt.patch b/third_party/tsl/third_party/llvm/opt.patch
new file mode 100644
index 0000000000..49e5b3b7fc
--- /dev/null
+++ b/third_party/tsl/third_party/llvm/opt.patch
@@ -0,0 +1,18 @@
+diff --git a/llvm/lib/Transforms/Scalar/MemCpyOptimizer.cpp b/llvm/lib/Transforms/Scalar/MemCpyOptimizer.cpp
+index d87f2fb59814..d2411cb33ecf 100644
+--- a/llvm/lib/Transforms/Scalar/MemCpyOptimizer.cpp
++++ b/llvm/lib/Transforms/Scalar/MemCpyOptimizer.cpp
+@@ -2019,9 +2019,10 @@ PreservedAnalyses MemCpyOptPass::run(Function &F, FunctionAnalysisManager &AM) {
+   auto *PDT = &AM.getResult<PostDominatorTreeAnalysis>(F);
+   auto *MSSA = &AM.getResult<MemorySSAAnalysis>(F);
+ 
+-  bool MadeChange = runImpl(F, &TLI, AA, AC, DT, PDT, &MSSA->getMSSA());
+-  if (!MadeChange)
+-    return PreservedAnalyses::all();
++  // FIXME: Disable MemCpyOptPass to avoid llvm.memcpy issue
++ // bool MadeChange = runImpl(F, &TLI, AA, AC, DT, PDT, &MSSA->getMSSA());
++ // if (!MadeChange)
++ //   return PreservedAnalyses::all();
+ 
+   PreservedAnalyses PA;
+   PA.preserveSet<CFGAnalyses>();
diff --git a/third_party/tsl/third_party/llvm/spirv.patch b/third_party/tsl/third_party/llvm/spirv.patch
new file mode 100644
index 0000000000..8643e91813
--- /dev/null
+++ b/third_party/tsl/third_party/llvm/spirv.patch
@@ -0,0 +1,76 @@
+diff --git a/llvm/lib/Passes/PassBuilderPipelines.cpp b/llvm/lib/Passes/PassBuilderPipelines.cpp
+index 78e0e6353056..4f9f51164bfe 100644
+--- a/llvm/lib/Passes/PassBuilderPipelines.cpp
++++ b/llvm/lib/Passes/PassBuilderPipelines.cpp
+@@ -183,6 +183,10 @@ static cl::opt<bool> EnableGlobalAnalyses(
+     "enable-global-analyses", cl::init(true), cl::Hidden,
+     cl::desc("Enable inter-procedural analyses"));
+ 
++static cl::opt<bool>
++    SYCLOptimizationMode("sycl-opt", cl::init(false), cl::Hidden,
++                         cl::desc("Enable SYCL optimization mode."));
++
+ static cl::opt<bool>
+     RunPartialInlining("enable-partial-inlining", cl::init(false), cl::Hidden,
+                        cl::desc("Run Partial inlinining pass"));
+@@ -406,6 +410,7 @@ PassBuilder::buildO1FunctionSimplificationPipeline(OptimizationLevel Level,
+   // Form canonically associated expression trees, and simplify the trees using
+   // basic mathematical properties. For example, this will form (nearly)
+   // minimal multiplication trees.
++  if (!SYCLOptimizationMode) {
+   FPM.addPass(ReassociatePass());
+ 
+   // Add the primary loop simplification pipeline.
+@@ -477,7 +482,7 @@ PassBuilder::buildO1FunctionSimplificationPipeline(OptimizationLevel Level,
+   FPM.addPass(createFunctionToLoopPassAdaptor(std::move(LPM2),
+                                               /*UseMemorySSA=*/false,
+                                               /*UseBlockFrequencyInfo=*/false));
+-
++  }
+   // Delete small array after loop unroll.
+   FPM.addPass(SROAPass(SROAOptions::ModifyCFG));
+ 
+@@ -580,6 +585,7 @@ PassBuilder::buildFunctionSimplificationPipeline(OptimizationLevel Level,
+   // Form canonically associated expression trees, and simplify the trees using
+   // basic mathematical properties. For example, this will form (nearly)
+   // minimal multiplication trees.
++  if (!SYCLOptimizationMode) {
+   FPM.addPass(ReassociatePass());
+ 
+   if (EnableConstraintElimination)
+@@ -657,7 +663,7 @@ PassBuilder::buildFunctionSimplificationPipeline(OptimizationLevel Level,
+   FPM.addPass(createFunctionToLoopPassAdaptor(std::move(LPM2),
+                                               /*UseMemorySSA=*/false,
+                                               /*UseBlockFrequencyInfo=*/false));
+-
++  }
+   // Delete small array after loop unroll.
+   FPM.addPass(SROAPass(SROAOptions::ModifyCFG));
+ 
+@@ -715,6 +721,9 @@ PassBuilder::buildFunctionSimplificationPipeline(OptimizationLevel Level,
+ 
+   invokeScalarOptimizerLateEPCallbacks(FPM, Level);
+ 
++  if (SYCLOptimizationMode)
++    FPM.addPass(SimplifyCFGPass());
++  else
+   FPM.addPass(SimplifyCFGPass(SimplifyCFGOptions()
+                                   .convertSwitchRangeToICmp(true)
+                                   .hoistCommonInsts(true)
+@@ -1385,6 +1394,7 @@ PassBuilder::buildModuleOptimizationPipeline(OptimizationLevel Level,
+ 
+   invokeVectorizerStartEPCallbacks(OptimizePM, Level);
+ 
++  if (!SYCLOptimizationMode) {
+   LoopPassManager LPM;
+   // First rotate loops that may have been un-rotated by prior passes.
+   // Disable header duplication at -Oz.
+@@ -1408,7 +1418,7 @@ PassBuilder::buildModuleOptimizationPipeline(OptimizationLevel Level,
+   OptimizePM.addPass(InjectTLIMappings());
+ 
+   addVectorPasses(Level, OptimizePM, /* IsFullLTO */ false);
+-
++  }
+   // LoopSink pass sinks instructions hoisted by LICM, which serves as a
+   // canonicalization pass that enables other optimizations. As a result,
+   // LoopSink pass needs to be a very late IR pass to avoid undoing LICM
diff --git a/third_party/tsl/third_party/llvm/workspace.bzl b/third_party/tsl/third_party/llvm/workspace.bzl
index fa02d95b4a..405cee4be7 100644
--- a/third_party/tsl/third_party/llvm/workspace.bzl
+++ b/third_party/tsl/third_party/llvm/workspace.bzl
@@ -17,6 +17,7 @@ def repo(name):
         ],
         build_file = "//third_party/llvm:llvm.BUILD",
         patch_file = [
+            "//third_party/llvm:spirv.patch",
             "//third_party/llvm:generated.patch",  # Autogenerated, don't remove.
             "//third_party/llvm:build.patch",
             "//third_party/llvm:mathextras.patch",
diff --git a/third_party/tsl/tsl/framework/BUILD b/third_party/tsl/tsl/framework/BUILD
index e055a619f5..fa5e93af61 100644
--- a/third_party/tsl/tsl/framework/BUILD
+++ b/third_party/tsl/tsl/framework/BUILD
@@ -117,7 +117,8 @@ cc_library(
         "@com_google_absl//absl/types:optional",
     ] + if_static(
         extra_deps = [
-            ":allocator_registry_impl",
+            # Remove this to avoid duplicating compile cpu allocator part.
+            # ":allocator_registry_impl",
             "//tsl/lib/gtl:inlined_vector",
             "//tsl/platform:strcat",
             "//tsl/platform:stringprintf",
@@ -208,6 +209,7 @@ cc_library(
         "//tsl/profiler/lib:scoped_memory_debug_annotation",
         "//tsl/profiler/lib:traceme",
         "//tsl/protobuf:bfc_memory_map_proto_cc",
+        "@tsl//tsl/util:env_var",
         "@com_google_absl//absl/container:flat_hash_set",
         "@com_google_absl//absl/strings",
         "@com_google_absl//absl/types:optional",
diff --git a/third_party/tsl/tsl/framework/bfc_allocator.cc b/third_party/tsl/tsl/framework/bfc_allocator.cc
index e19c0018d2..0d22b94fb9 100644
--- a/third_party/tsl/tsl/framework/bfc_allocator.cc
+++ b/third_party/tsl/tsl/framework/bfc_allocator.cc
@@ -47,12 +47,13 @@ BFCAllocator::BFCAllocator(std::unique_ptr<SubAllocator> sub_allocator,
       sub_allocator_(std::move(sub_allocator)),
       name_(name),
       free_chunks_list_(kInvalidChunkHandle),
-      next_allocation_id_(1) {
-  if (opts.allow_growth) {
+      next_allocation_id_(1),
+      alloc_mode(AllocMode()) {
+  if (opts.allow_growth || alloc_mode == BFC_EXTEND_SMALL) {
     // 2MiB smallest initial allocation, unless total memory available
     // is less.
     curr_region_allocation_bytes_ =
-        RoundedBytes(std::min(total_memory, size_t{2 << 20}));
+        RoundedBytes(std::min(total_memory, kSmallSize));
   } else {
     curr_region_allocation_bytes_ = RoundedBytes(total_memory);
   }
@@ -126,34 +127,51 @@ bool BFCAllocator::Extend(size_t alignment, size_t rounded_bytes) {
   // allocation, keep multiplying by a power of two until that is
   // sufficient.
   bool increased_allocation = false;
-  while (rounded_bytes > curr_region_allocation_bytes_) {
-    curr_region_allocation_bytes_ *= 2;
-    increased_allocation = true;
+  if (alloc_mode == BFC_EXTEND_LARGE) {
+    while (rounded_bytes > curr_region_allocation_bytes_) {
+      curr_region_allocation_bytes_ *= 2;
+      increased_allocation = true;
+    }
+  } else {
+    // Requested bytes              --- Allocated bytes
+    // (0, kSmallSize]              --- kSmallBuffer
+    // (kSmallSize, kMinLargeAlloc) --- kLargeBuffer
+    // [kMinLargeAlloc, max]        --- round up to multiple of kRoundLarge
+    curr_region_allocation_bytes_ =
+        (rounded_bytes <= kSmallSize)
+            ? kSmallBuffer
+            : ((rounded_bytes < kMinLargeAlloc)
+                    ? kLargeBuffer
+                    : (kRoundLarge *
+                      ((rounded_bytes + kRoundLarge - 1) / kRoundLarge)));
   }
 
   // Try allocating.
   size_t bytes = std::min(curr_region_allocation_bytes_, available_bytes);
+  bytes = std::min(bytes, GetLimitAlloc());
+
   size_t bytes_received;
+  VLOG(1) << "BFCAllocator try to alloc " << bytes << "bytes!\r\n";
+  VLOG(1) << "BFCAllocator current available_bytes is " <<  available_bytes << "bytes!\r\n";
+  VLOG(1) << "BFCAllocator current rounded_bytes is " <<  rounded_bytes << "bytes!\r\n";
+  VLOG(1) << "BFCAllocator current curr_region_allocation_bytes_ is " <<  curr_region_allocation_bytes_ << "bytes!\r\n";
   void* mem_addr = sub_allocator_->Alloc(alignment, bytes, &bytes_received);
-  if (mem_addr == nullptr && !started_backpedal_) {
-    // Only backpedal once.
-    started_backpedal_ = true;
-
+  if (mem_addr == nullptr) {
     static constexpr float kBackpedalFactor = 0.9;
 
     // Try allocating less memory.
     while (mem_addr == nullptr) {
       bytes = RoundedBytes(bytes * kBackpedalFactor);
-      if (bytes < rounded_bytes) break;
+      if (bytes < rounded_bytes) {
+        return false;
+      }
       mem_addr = sub_allocator_->Alloc(alignment, bytes, &bytes_received);
     }
   }
 
-  if (mem_addr == nullptr) {
-    return false;
-  }
-
-  if (!increased_allocation) {
+  if (alloc_mode == BFC_EXTEND_SMALL) {
+    curr_region_allocation_bytes_ = kSmallSize;
+  } else if (!increased_allocation) {
     // Increase the region size of the next required allocation.
     curr_region_allocation_bytes_ *= 2;
   }
@@ -1253,4 +1271,35 @@ AllocatorMemoryType BFCAllocator::GetMemoryType() const {
   return sub_allocator_->GetMemoryType();
 }
 
+int64 AllocModeFromEnv() {
+  int64 alloc_mode_env = 1;
+  Status status = ReadInt64FromEnvVar("ITEX_ALLOC_MODE", 1, &alloc_mode_env);
+  if (!status.ok()) {
+    LOG(ERROR) << "Failed to read environment variable ITEX_ALLOC_MODE!"; 
+  }
+
+  return alloc_mode_env;
+}
+
+int64 BFCAllocator::AllocMode() {
+  static int64 alloc_mode = AllocModeFromEnv();
+  return alloc_mode;
+}
+
+// This function set the upper bound of memory allocation size, the
+// actual allocation size is the minimal value of this limit size
+// and the size want to get from system.
+size_t BFCAllocator::GetLimitAlloc() {
+  // set default limit to 4GB
+  int64 limit_size = 4 * 1024 - 1;
+  if (!is_arc_device_) {
+    limit_size = memory_limit_ / 1024 / 1024 * 0.75;
+  }
+  Status status = ReadInt64FromEnvVar("ITEX_LIMIT_MEMORY_SIZE_IN_MB",
+                                        limit_size, &limit_size);
+  if (!status.ok()) {
+    LOG(ERROR) << "Failed to read environment variable ITEX_LIMIT_MEMORY_SIZE_IN_MB!"; 
+  }
+  return limit_size * 1024 * 1024;
+}
 }  // namespace tsl
diff --git a/third_party/tsl/tsl/framework/bfc_allocator.h b/third_party/tsl/tsl/framework/bfc_allocator.h
index 47619856ab..213a901fdf 100644
--- a/third_party/tsl/tsl/framework/bfc_allocator.h
+++ b/third_party/tsl/tsl/framework/bfc_allocator.h
@@ -33,6 +33,10 @@ limitations under the License.
 #include "tsl/platform/strcat.h"
 #include "tsl/platform/thread_annotations.h"
 #include "tsl/platform/types.h"
+#include "tsl/util/env_var.h"
+
+#define BFC_EXTEND_LARGE 1
+#define BFC_EXTEND_SMALL 2
 
 namespace tensorflow {
 class MemoryDump;
@@ -97,6 +101,11 @@ class BFCAllocator : public Allocator {
 
   bool ClearStats() override;
 
+  // SYCL: set limit
+  void SetAllocateLimit(uint64_t limit_byte) { limit_byte_ = limit_byte; }
+
+  void SetArcDevice() {is_arc_device_ = true;}
+
   void SetTimingCounter(SharedCounter* sc) { timing_counter_ = sc; }
 
   void SetSafeFrontier(uint64 count) override;
@@ -108,6 +117,9 @@ class BFCAllocator : public Allocator {
   MemoryDump RecordMemoryMap();
 
  private:
+  // SYCL: set limit
+  uint64_t limit_byte_ = std::numeric_limits<uint64_t>::max();
+  bool is_arc_device_ = false;
   struct Bin;
 
   void* AllocateRawInternal(size_t alignment, size_t num_bytes,
@@ -152,6 +164,16 @@ class BFCAllocator : public Allocator {
                   int64_t req_bytes, int64_t alloc_bytes)
       TF_EXCLUSIVE_LOCKS_REQUIRED(lock_);
 
+  int64 AllocMode();
+  int64 alloc_mode;
+  size_t GetLimitAlloc();
+  
+  static constexpr size_t kSmallSize = 1048576;
+  static constexpr size_t kSmallBuffer = 2097152;
+  static constexpr size_t kLargeBuffer = 20971520;
+  static constexpr size_t kMinLargeAlloc = 10485760;
+  static constexpr size_t kRoundLarge = 2097152;
+
   // A ChunkHandle is an index into the chunks_ vector in BFCAllocator
   // kInvalidChunkHandle means an invalid chunk
   typedef size_t ChunkHandle;
@@ -579,10 +601,6 @@ class BFCAllocator : public Allocator {
   // The size of the current region allocation.
   size_t curr_region_allocation_bytes_;
 
-  // An indicator that expansion of a region has hit the limits
-  // of the available memory.
-  bool started_backpedal_ = false;
-
   // Whether the allocator will coalesce adjacent sub allocator provided
   // AllocationRegions. This may be disabled if discrete sub allocator
   // regions can't be treated as contiguous (e.g. if the allocation refers to
diff --git a/third_party/tsl/tsl/framework/contraction/BUILD b/third_party/tsl/tsl/framework/contraction/BUILD
index b52a7ae315..0934d4ed32 100644
--- a/third_party/tsl/tsl/framework/contraction/BUILD
+++ b/third_party/tsl/tsl/framework/contraction/BUILD
@@ -116,7 +116,7 @@ cc_library(
         "//tsl:linux_ppc64le": [],
         "//tsl:linux_s390x": [],
         "//tsl:macos_arm64": [],
-        "//conditions:default": ["@onednn//:mkl_dnn"],
+        "//conditions:default": [],
     }),
 )
 
diff --git a/third_party/tsl/workspace2.bzl b/third_party/tsl/workspace2.bzl
index 5bfb6fd033..78e69ea519 100644
--- a/third_party/tsl/workspace2.bzl
+++ b/third_party/tsl/workspace2.bzl
@@ -370,6 +370,7 @@ def _tf_repositories():
         patch_file = [
             "//third_party/grpc:generate_cc_env_fix.patch",
             "//third_party/grpc:register_go_toolchain.patch",
+            "//third_party/grpc:c_ares.patch",
         ],
         system_link_files = {
             "//third_party/systemlibs:BUILD": "bazel/BUILD",
diff --git a/xla/mlir_hlo/lhlo_gpu/IR/lhlo_gpu_ops_enums.td b/xla/mlir_hlo/lhlo_gpu/IR/lhlo_gpu_ops_enums.td
index 2c365d94de..e0e640468e 100644
--- a/xla/mlir_hlo/lhlo_gpu/IR/lhlo_gpu_ops_enums.td
+++ b/xla/mlir_hlo/lhlo_gpu/IR/lhlo_gpu_ops_enums.td
@@ -148,6 +148,7 @@ def FusedMhaDagSoftmaxDropout : I32EnumAttrCase<"SoftmaxDropout", 5>;
 def FusedMhaDagSoftmax : I32EnumAttrCase<"Softmax", 6>;
 def FusedMhaDagScaleBiasSoftmaxDropout : I32EnumAttrCase<"ScaleBiasSoftmaxDropout", 7>;
 def FusedMhaDagScaleBiasSoftmax : I32EnumAttrCase<"ScaleBiasSoftmax", 8>;
+def FusedMhaDagScaleSoftmax : I32EnumAttrCase<"ScaleSoftmax", 9>;
 
 def FusedMhaBackwardDagScaleBiasSoftmaxDropout : I32EnumAttrCase<"BackwardScaleBiasSoftmaxDropout", 0>;
 def FusedMhaBackwardDagScaleBiasSoftmax : I32EnumAttrCase<"BackwardScaleBiasSoftmax", 1>;
@@ -168,7 +169,8 @@ def FusedMhaDagSignature: I32EnumAttr<"FusedMhaDagSignature",
     FusedMhaDagSoftmaxDropout,
     FusedMhaDagSoftmax,
     FusedMhaDagScaleBiasSoftmaxDropout,
-    FusedMhaDagScaleBiasSoftmax]> {
+    FusedMhaDagScaleBiasSoftmax,
+    FusedMhaDagScaleSoftmax]> {
   let genSpecializedAttr = 0;
   let cppNamespace = "::mlir::lmhlo_gpu";
 }
diff --git a/xla/pjrt/c/pjrt_c_api_gpu_internal.cc b/xla/pjrt/c/pjrt_c_api_gpu_internal.cc
index fa10334d50..5820c94aed 100644
--- a/xla/pjrt/c/pjrt_c_api_gpu_internal.cc
+++ b/xla/pjrt/c/pjrt_c_api_gpu_internal.cc
@@ -45,7 +45,7 @@ limitations under the License.
 namespace pjrt {
 namespace gpu_plugin {
 
-#define PJRT_GPU_PLUGIN_PLATFORM_NAME "CUDA"
+#define PJRT_GPU_PLUGIN_PLATFORM_NAME "SYCL"
 
 PJRT_Error* PJRT_Client_Create(PJRT_Client_Create_Args* args) {
   PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(
diff --git a/xla/pjrt/c/pjrt_c_api_helpers.cc b/xla/pjrt/c/pjrt_c_api_helpers.cc
index c655dec9df..f591be7a4b 100644
--- a/xla/pjrt/c/pjrt_c_api_helpers.cc
+++ b/xla/pjrt/c/pjrt_c_api_helpers.cc
@@ -580,6 +580,15 @@ static std::string StructSizeErrorMsg(absl::string_view struct_name,
   return error_msg;
 }
 
+xla::Status CheckMatchingStructSizes(absl::string_view struct_name,
+                                     size_t expected_size, size_t actual_size) {
+  if (expected_size != actual_size) {
+    return tsl::errors::InvalidArgument(
+        StructSizeErrorMsg(struct_name, expected_size, actual_size));
+  }
+  return tsl::OkStatus();
+}
+
 xla::Status ActualStructSizeIsGreaterOrEqual(absl::string_view struct_name,
                                              size_t expected_size,
                                              size_t actual_size) {
diff --git a/xla/pjrt/c/pjrt_c_api_helpers.h b/xla/pjrt/c/pjrt_c_api_helpers.h
index 720ef577d5..36142466e7 100644
--- a/xla/pjrt/c/pjrt_c_api_helpers.h
+++ b/xla/pjrt/c/pjrt_c_api_helpers.h
@@ -154,6 +154,12 @@ xla::Status ValidateCreateOptions(
     const absl::flat_hash_map<std::string, PJRT_NamedValue_Type>&
         expected_name_and_types);
 
+// Helper function for checking C API argument struct sizes. Returns a non-OK
+// status if the expected and actual sizes aren't equal (i.e. no ABI
+// compatibility guarantees).
+xla::Status CheckMatchingStructSizes(absl::string_view struct_name,
+                                     size_t expected_size, size_t actual_size);
+
 // Helper function for checking the actual C API argument struct size is greater
 // than or equal to the expected size. The actual struct size can be larger if
 // it comes from a forwards-compatible caller built at a later version than this
diff --git a/xla/pjrt/event_pool.cc b/xla/pjrt/event_pool.cc
index 1db130633c..d1accf7fa7 100644
--- a/xla/pjrt/event_pool.cc
+++ b/xla/pjrt/event_pool.cc
@@ -53,8 +53,11 @@ StatusOr<EventPool::Handle> EventPool::AllocateEvent(
 }
 
 void EventPool::ThenRecordEvent(se::Stream* stream, EventPool::Handle& handle) {
-  absl::MutexLock lock(&mu_);
+  // We should not lock event pool mutex before submitting a sycl barrier to a
+  // stream, otherwise it may lead to a dead lock if there is a host task
+  // requiring this mutex in the same stream.
   stream->ThenRecordEvent(handle.event_.get());
+  absl::MutexLock lock(&mu_);
   handle.sequence_number_ = next_sequence_number_++;
 }
 
diff --git a/xla/pjrt/gpu/BUILD b/xla/pjrt/gpu/BUILD
index c98d792dd4..8e146237eb 100644
--- a/xla/pjrt/gpu/BUILD
+++ b/xla/pjrt/gpu/BUILD
@@ -40,6 +40,7 @@ cc_library(
     defines = if_cuda(["GOOGLE_CUDA=1"]) + if_rocm(["TENSORFLOW_USE_ROCM=1"]),
     visibility = ["//xla/pjrt:friends"],
     deps = [
+        "@intel_extension_for_openxla//xla/stream_executor/sycl:hw_info",
         ":gpu_helpers",
         ":gpu_metrics",
         ":gpu_topology",
diff --git a/xla/pjrt/gpu/gpu_helpers.cc b/xla/pjrt/gpu/gpu_helpers.cc
index d77ebf6bd4..000ad13fb4 100644
--- a/xla/pjrt/gpu/gpu_helpers.cc
+++ b/xla/pjrt/gpu/gpu_helpers.cc
@@ -37,11 +37,12 @@ namespace xla {
 StatusOr<LocalClient*> GetGpuXlaClient(
     const std::optional<std::string>& platform_name,
     const std::optional<std::set<int>>& allowed_devices) {
+  // SYCL: hardcode to xpu
   TF_ASSIGN_OR_RETURN(
       se::Platform * platform,
-      PlatformUtil::GetPlatform(platform_name ? *platform_name : "gpu"));
+      PlatformUtil::GetPlatform(platform_name ? *platform_name : "SYCL"));
   if (platform->VisibleDeviceCount() <= 0) {
-    return FailedPrecondition("No visible GPU devices.");
+    return FailedPrecondition("No visible XPU devices.");
   }
   LocalClientOptions options;
   options.set_platform(platform);
@@ -115,7 +116,8 @@ StatusOr<std::unique_ptr<tsl::BFCAllocator>> CreateBFCAllocator(
   opts.allow_growth = !preallocate;
   return std::make_unique<tsl::BFCAllocator>(
       std::move(sub_allocator), allocator_memory,
-      absl::StrCat("GPU_", device_ordinal, "_bfc"), opts);
+      // SYCL: hardcode to xpu
+      absl::StrCat("XPU_", device_ordinal, "_bfc"), opts);
 }
 
 // Builds a BFCAllocator for all local GPUs that uses collective memory.
@@ -171,7 +173,8 @@ std::unique_ptr<tsl::BFCAllocator> GetGpuHostAllocator(
   opts.allow_growth = true;
   return std::make_unique<tsl::BFCAllocator>(std::move(sub_allocator),
                                              kGpuHostMemoryLimitBytes,
-                                             /*name=*/"xla_gpu_host_bfc", opts);
+                                             // SYCL: hardcode to xpu
+                                             /*name=*/"xla_xpu_host_bfc", opts);
 }
 
 }  // namespace xla
diff --git a/xla/pjrt/gpu/se_gpu_pjrt_client.cc b/xla/pjrt/gpu/se_gpu_pjrt_client.cc
index 20ecb49c0c..4f3eed4745 100644
--- a/xla/pjrt/gpu/se_gpu_pjrt_client.cc
+++ b/xla/pjrt/gpu/se_gpu_pjrt_client.cc
@@ -81,6 +81,8 @@ limitations under the License.
 #include "tsl/platform/threadpool.h"
 #include "tsl/profiler/lib/connected_traceme.h"
 #include "tsl/profiler/lib/traceme.h"
+#include "xla/stream_executor/sycl/hw_info.h"
+#include "tsl/util/env_var.h"
 
 #if defined(GOOGLE_CUDA) || defined(TENSORFLOW_USE_ROCM)
 #include "xla/pjrt/compile_options.pb.h"
@@ -429,7 +431,7 @@ absl::string_view StreamExecutorGpuClient::platform_version() const {
 #elif GOOGLE_CUDA && defined(CUDART_VERSION)  // cuda
   return "cuda " STRINGIFY(CUDART_VERSION);
 #else
-  return "<unknown>";
+  return "sycl";
 #endif  // TENSORFLOW_USE_ROCM && defined(TF_ROCM_VERSION)
 }
 
@@ -753,12 +755,28 @@ GetStreamExecutorGpuDeviceAllocator(
     case GpuAllocatorConfig::Kind::kDefault:
     case GpuAllocatorConfig::Kind::kBFC: {
       LOG(INFO) << "Using BFC allocator.";
+      // SYCL: set allocate limit
+      int64_t default_limit_MB = -1;
+      TF_CHECK_OK(tsl::ReadInt64FromEnvVar(
+          "XLA_LIMIT_MEMORY_SIZE_IN_MB", default_limit_MB, &default_limit_MB));
+      if (IsARC() && default_limit_MB > 4095) {
+        default_limit_MB = 4095;
+        LOG(WARNING)
+            << "ARC or FLEX series allocation size should be less than 4096MB. "
+            << "Set as default 4095MB.";
+      }
+      uint64_t limit_byte =
+          default_limit_MB > 0
+              ? std::min(static_cast<uint64_t>(default_limit_MB * 1024 * 1024),
+                         GetMaxAllocateLimitByte())
+              : GetMaxAllocateLimitByte();
       for (const auto& ordinal_and_device : addressable_devices) {
         TF_ASSIGN_OR_RETURN(
             auto bfc_allocator,
             CreateBFCAllocator(ordinal_and_device.second->executor(),
                                allocator_config.memory_fraction,
                                allocator_config.preallocate));
+        bfc_allocator->SetAllocateLimit(limit_byte);
         allocators.emplace_back(std::move(bfc_allocator),
                                 ordinal_and_device.second->compute_stream(),
                                 /*memory_space=*/0);
@@ -951,6 +969,8 @@ StatusOr<std::unique_ptr<PjRtClient>> GetStreamExecutorGpuClient(
     const GpuClientOptions& options) {
 #if TENSORFLOW_USE_ROCM
   auto pjrt_platform_name = xla::RocmName();
+#elif TENSORFLOW_USE_SYCL
+  auto pjrt_platform_name = xla::XpuName();
 #else   // TENSORFLOW_USE_ROCM
   auto pjrt_platform_name = xla::CudaName();
 #endif  // TENSORFLOW_USE_ROCM
diff --git a/xla/pjrt/local_device_state.cc b/xla/pjrt/local_device_state.cc
index 20ef535279..825090293b 100644
--- a/xla/pjrt/local_device_state.cc
+++ b/xla/pjrt/local_device_state.cc
@@ -147,6 +147,7 @@ Status LocalDeviceState::ThenMemcpyDeviceToDevice(
 void LocalDeviceState::ThenExecuteCallback(se::Stream* stream,
                                            std::function<void()> callback) {
   tsl::profiler::TraceMe traceme("ThenExecuteCallback");
+  se::Stream* other_stream = stream;
   if (callback_stream_map_.has_value()) {
     // Prevent concurrent updates to the callback stream map.
     absl::MutexLock lock(&callback_stream_map_mu_);
@@ -157,10 +158,14 @@ void LocalDeviceState::ThenExecuteCallback(se::Stream* stream,
       callback_stream =
           callback_stream_map_->insert({stream, std::move(new_stream)}).first;
     }
-    callback_stream->second->ThenWaitFor(stream);
-    stream = callback_stream->second.get();
+    other_stream = callback_stream->second.get();
   }
-  stream->ThenDoHostCallback([this, callback{std::move(callback)}]() mutable {
+  // We should release mutex before submit barrier or host task to callback
+  // stream, otherwise will lead to dead lock.
+  if (other_stream != stream) {
+    other_stream->ThenWaitFor(stream);
+  }
+  other_stream->ThenDoHostCallback([this, callback{std::move(callback)}]() mutable {
     callback_thread_->Schedule(std::move(callback));
   });
 }
diff --git a/xla/pjrt/pjrt_compiler.h b/xla/pjrt/pjrt_compiler.h
index f947690936..e902a059e9 100644
--- a/xla/pjrt/pjrt_compiler.h
+++ b/xla/pjrt/pjrt_compiler.h
@@ -48,6 +48,10 @@ inline const char* TpuName() {
   static constexpr char kTpuName[] = "tpu";
   return kTpuName;
 }
+inline const char* XpuName() {
+  static constexpr char kXpuName[] = "xpu";
+  return kXpuName;
+}
 inline PjRtPlatformId CpuId() {
   static const PjRtPlatformId kCpuId = tsl::Fingerprint64(CpuName());
   return kCpuId;
@@ -64,6 +68,10 @@ inline PjRtPlatformId TpuId() {
   static const PjRtPlatformId kTpuId = tsl::Fingerprint64(TpuName());
   return kTpuId;
 }
+inline PjRtPlatformId XpuId() {
+  static const PjRtPlatformId kXpuId = tsl::Fingerprint64(XpuName());
+  return kXpuId;
+}
 
 class PjRtCompiler;
 class PjRtClient;
diff --git a/xla/service/BUILD b/xla/service/BUILD
index b69e6a1e7e..447a7dfff6 100644
--- a/xla/service/BUILD
+++ b/xla/service/BUILD
@@ -20,6 +20,10 @@ load(
     "if_rocm",
     "if_rocm_is_configured",
 )
+load(
+    "@local_config_sycl//sycl:build_defs.bzl",
+    "if_sycl_is_configured",
+)
 load("@tsl//tsl:tsl.bzl", "if_google", "if_libtpu")
 load("@tsl//tsl:tsl.default.bzl", "filegroup", "get_compatible_with_portable", "internal_hlo_deps")
 load(
@@ -1338,6 +1342,9 @@ cc_library(
     ]) + if_rocm_is_configured([
         "//xla/service/gpu:amdgpu_compiler",
         "//xla/stream_executor/rocm:stream_executor_rocm",
+    ]) + if_sycl_is_configured([
+        "//xla/service/gpu:spir_compiler",
+        "@intel_extension_for_openxla//xla/stream_executor/sycl:stream_executor_sycl",
     ]),
 )
 
@@ -3824,6 +3831,7 @@ cc_library(
         "//xla/stream_executor/cuda:cuda_platform_id",
         "//xla/stream_executor/host:host_platform_id",
         "//xla/stream_executor/rocm:rocm_platform_id",
+        "@intel_extension_for_openxla//xla/stream_executor/sycl:sycl_platform_id",
         "@com_google_absl//absl/container:flat_hash_map",
         "@com_google_absl//absl/memory",
         "@com_google_absl//absl/strings",
diff --git a/xla/service/algebraic_simplifier.h b/xla/service/algebraic_simplifier.h
index 6152b8c056..bbd739d400 100644
--- a/xla/service/algebraic_simplifier.h
+++ b/xla/service/algebraic_simplifier.h
@@ -27,6 +27,7 @@ limitations under the License.
 #include <vector>
 
 #include "absl/container/inlined_vector.h"
+#include "tsl/util/env_var.h"
 #include "xla/hlo/ir/dfs_hlo_visitor_with_default.h"
 #include "xla/hlo/ir/hlo_instruction.h"
 #include "xla/hlo/ir/hlo_module.h"
@@ -439,7 +440,9 @@ class AlgebraicSimplifierVisitor : public DfsHloRewriteVisitor {
   virtual bool IsValidLayout(const Shape& shape) { return true; }
   // Allow backend targets to determine whether a layout is inefficient.
   virtual bool ShouldStrengthReduceDotToReduce(const HloInstruction* hlo) {
-    return true;
+    bool llm_flag = false;
+    tsl::ReadBoolFromEnvVar("LLM", false, &llm_flag);
+    return !llm_flag;
   }
 
  protected:
diff --git a/xla/service/computation_placer.cc b/xla/service/computation_placer.cc
index d0274f5630..bc77594ad1 100644
--- a/xla/service/computation_placer.cc
+++ b/xla/service/computation_placer.cc
@@ -31,6 +31,7 @@ limitations under the License.
 #include "xla/stream_executor/cuda/cuda_platform_id.h"
 #include "xla/stream_executor/host/host_platform_id.h"
 #include "xla/stream_executor/rocm/rocm_platform_id.h"
+#include "xla/stream_executor/sycl/sycl_platform_id.h"
 #include "xla/types.h"
 #include "xla/util.h"
 #include "tsl/platform/errors.h"
@@ -216,6 +217,8 @@ static bool InitModule() {
       stream_executor::cuda::kCudaPlatformId, &CreateComputationPlacer);
   xla::ComputationPlacer::RegisterComputationPlacer(
       stream_executor::rocm::kROCmPlatformId, &CreateComputationPlacer);
+  xla::ComputationPlacer::RegisterComputationPlacer(
+      stream_executor::sycl::kSyclPlatformId, &CreateComputationPlacer);
   return true;
 }
 static bool module_initialized = InitModule();
diff --git a/xla/service/dump.cc b/xla/service/dump.cc
index 493980cd1b..5a7561a3c7 100644
--- a/xla/service/dump.cc
+++ b/xla/service/dump.cc
@@ -624,6 +624,9 @@ void DumpToFileInDirOrStdout(const HloModule& module, string_view file_prefix,
   if (opts.dumping_to_stdout()) return op->dump();
 
   mlir::OpPrintingFlags print_flags = mlir::OpPrintingFlags();
+
+  // Avoid printing large constant weight.
+  print_flags.elideLargeElementsAttrs(8);
   // Enable debug info so that it is easier to see the corresponding HLO node.
   if (file_prefix == "lmhlo") {
     print_flags.enableDebugInfo(/*enable=*/true,
diff --git a/xla/service/float8_fnuz_ir_emitter.cc b/xla/service/float8_fnuz_ir_emitter.cc
index 649729d873..873b1ee51b 100644
--- a/xla/service/float8_fnuz_ir_emitter.cc
+++ b/xla/service/float8_fnuz_ir_emitter.cc
@@ -19,6 +19,7 @@ limitations under the License.
 
 #include "llvm/IR/Constants.h"
 #include "llvm/IR/Intrinsics.h"
+#include "llvm/TargetParser/Triple.h"
 #include "xla/primitive_util.h"
 #include "xla/status_macros.h"
 #include "xla/util.h"
@@ -610,13 +611,17 @@ StatusOr<llvm::Value*> EmitF8fnuzToFloating(PrimitiveType input_type,
         llvm::Constant* result_lut_array =
             llvm::ConstantArray::get(result_lut_array_type, result_lut);
 
+        int addrspace = llvm::Triple(module->getTargetTriple()).isSPIR() ? 1 : 0;
         return new llvm::GlobalVariable(
             /*M=*/*module,
             /*Ty=*/result_lut_array_type,
             /*isConstant=*/true,
             /*Linkage=*/llvm::GlobalValue::PrivateLinkage,
             /*Initializer=*/result_lut_array,
-            /*Name=*/lut_name);
+            /*Name=*/lut_name,
+            /*InsertBefore*/nullptr,
+            /*TLMode=*/llvm::GlobalValue::NotThreadLocal,
+            /*AddressSpace=*/addrspace);
       });
 
   // Check for NaN, since it's a special case.
diff --git a/xla/service/gpu/BUILD b/xla/service/gpu/BUILD
index 3812f8ece3..e4e7dbfe24 100644
--- a/xla/service/gpu/BUILD
+++ b/xla/service/gpu/BUILD
@@ -36,6 +36,7 @@ load(
     "@tsl//tsl/platform/default:cuda_build_defs.bzl",
     "if_cuda_is_configured",
 )
+load("@local_config_sycl//sycl:build_defs.bzl", "if_sycl_is_configured")
 
 package(
     # copybara:uncomment default_applicable_licenses = ["//tensorflow:license"],
@@ -319,7 +320,8 @@ cc_library(
         ":launch_dimensions",
         ":matmul_utils",
         ":nccl_api",
-        ":nccl_collective_thunks",
+        # ":nccl_collective_thunks",
+        "@intel_extension_for_openxla//xla/service/gpu:ccl_collective_thunks",
         ":parallel_loop_emitter",
         ":reduction_utils",
         ":target_util",
@@ -352,9 +354,9 @@ cc_library(
         "//xla/service/gpu/fusions:tiling_util",
         "//xla/service/gpu/kernels:custom_kernel",
         "//xla/service/gpu/kernels:topk_custom_kernel",
-        "//xla/service/gpu/runtime3:command_buffer_cmd",
-        "//xla/service/gpu/runtime3:command_buffer_cmd_emitter",
-        "//xla/service/gpu/runtime3:command_buffer_thunk",
+        # "//xla/service/gpu/runtime3:command_buffer_cmd",
+        # "//xla/service/gpu/runtime3:command_buffer_cmd_emitter",
+        # "//xla/service/gpu/runtime3:command_buffer_thunk",
         "//xla/service/gpu/runtime3:conditional_thunk",
         "//xla/service/gpu/runtime3:convolution_thunk",
         "//xla/service/gpu/runtime3:copy_thunk",
@@ -364,8 +366,8 @@ cc_library(
         "//xla/service/gpu/runtime3:gemm_thunk",
         "//xla/service/gpu/runtime3:infeed_thunk",
         "//xla/service/gpu/runtime3:kernel_thunk",
-        "//xla/service/gpu/runtime3:nccl_all_gather_thunk",
-        "//xla/service/gpu/runtime3:nccl_all_reduce_thunk",
+        # "//xla/service/gpu/runtime3:nccl_all_gather_thunk",
+        # "//xla/service/gpu/runtime3:nccl_all_reduce_thunk",
         "//xla/service/gpu/runtime3:norm_thunk",
         "//xla/service/gpu/runtime3:outfeed_thunk",
         "//xla/service/gpu/runtime3:replica_id_thunk",
@@ -421,9 +423,9 @@ cc_library(
         "@tsl//tsl/platform:statusor",
         "@tsl//tsl/protobuf:dnn_proto_cc",
     ] + if_gpu_is_configured([
-        ":ir_emitter_triton",
+        # ":ir_emitter_triton",
         "//xla/service/gpu/runtime3:cholesky_thunk",
-        "//xla/service/gpu/runtime3:cub_sort_thunk",
+        # "//xla/service/gpu/runtime3:cub_sort_thunk",
         "//xla/service/gpu/runtime3:gpublas_lt_matmul_thunk",
         "//xla/service/gpu/runtime3:triangular_solve_thunk",
     ]),
@@ -925,55 +927,78 @@ cc_library(
 # :nccl_api target and all other targets should use this header to launch collective operations.
 # This allows to minimize the spreading of #ifdef all over the XLA code base.
 
-alias(
-    name = "nccl_api",
-    actual = if_nccl(":_nccl_api_impl", ":_nccl_api_stub"),
-)
+# alias(
+#     name = "nccl_api",
+#     actual = if_nccl(":_nccl_api_impl", ":_nccl_api_stub"),
+# )
+
+# cc_library(
+#     name = "_nccl_api_impl",
+#     srcs = if_cuda_is_configured(
+#         ["nccl_api.cc"],
+#         ["nccl_api_stub.cc"],
+#     ),
+#     hdrs = ["nccl_api.h"],
+#     compatible_with = get_compatible_with_portable(),
+#     defines = if_cuda_is_configured(["XLA_ENABLE_XCCL"]),  # TODO(ezhulenev): Remove!
+#     deps = [
+#         ":nccl_clique_key",
+#         "//xla:shape_util",
+#         "//xla:xla_data_proto_cc",
+#         "//xla/service:collective_ops_utils",
+#         "//xla/stream_executor",
+#         "@com_google_absl//absl/algorithm:container",
+#         "@com_google_absl//absl/hash",
+#         "@com_google_absl//absl/status",
+#         "@com_google_absl//absl/status:statusor",
+#         "@com_google_absl//absl/strings",
+#         "@com_google_absl//absl/strings:str_format",
+#         "@tsl//tsl/concurrency:ref_count",
+#         "@tsl//tsl/platform:logging",
+#         "@tsl//tsl/platform:statusor",
+#     ] + if_cuda_is_configured([
+#         "@local_config_nccl//:nccl",
+#         "//xla/stream_executor/cuda:cuda_driver",
+#         "//xla/stream_executor/cuda:cuda_executor",
+#     ]) + if_gpu_is_configured([
+#         "//xla/stream_executor/gpu:gpu_stream",
+#     ]),
+# )
+
+# cc_library(
+#     name = "_nccl_api_stub",
+#     srcs = ["nccl_api_stub.cc"],
+#     hdrs = ["nccl_api.h"],
+#     compatible_with = get_compatible_with_portable(),
+#     deps = [
+#         ":nccl_clique_key",
+#         "//xla:shape_util",
+#         "//xla:xla_data_proto_cc",
+#         "//xla/service:collective_ops_utils",
+#         "//xla/stream_executor",
+#         "@com_google_absl//absl/status",
+#         "@com_google_absl//absl/status:statusor",
+#         "@tsl//tsl/concurrency:ref_count",
+#         "@tsl//tsl/platform:logging",
+#     ],
+# )
 
 cc_library(
-    name = "_nccl_api_impl",
-    srcs = if_cuda_is_configured(
-        ["nccl_api.cc"],
-        ["nccl_api_stub.cc"],
-    ),
-    hdrs = ["nccl_api.h"],
+    name = "nccl_api",
+    srcs = ["ccl_api.cc"],
+    hdrs = [
+        "nccl_api.h",
+        "ccl_api.h",
+    ],
     compatible_with = get_compatible_with_portable(),
-    defines = if_cuda_is_configured(["XLA_ENABLE_XCCL"]),  # TODO(ezhulenev): Remove!
     deps = [
         ":nccl_clique_key",
         "//xla:shape_util",
         "//xla:xla_data_proto_cc",
         "//xla/service:collective_ops_utils",
         "//xla/stream_executor",
-        "@com_google_absl//absl/algorithm:container",
-        "@com_google_absl//absl/hash",
-        "@com_google_absl//absl/status",
-        "@com_google_absl//absl/status:statusor",
-        "@com_google_absl//absl/strings",
-        "@com_google_absl//absl/strings:str_format",
-        "@tsl//tsl/concurrency:ref_count",
-        "@tsl//tsl/platform:logging",
-        "@tsl//tsl/platform:statusor",
-    ] + if_cuda_is_configured([
-        "@local_config_nccl//:nccl",
-        "//xla/stream_executor/cuda:cuda_driver",
-        "//xla/stream_executor/cuda:cuda_executor",
-    ]) + if_gpu_is_configured([
         "//xla/stream_executor/gpu:gpu_stream",
-    ]),
-)
-
-cc_library(
-    name = "_nccl_api_stub",
-    srcs = ["nccl_api_stub.cc"],
-    hdrs = ["nccl_api.h"],
-    compatible_with = get_compatible_with_portable(),
-    deps = [
-        ":nccl_clique_key",
-        "//xla:shape_util",
-        "//xla:xla_data_proto_cc",
-        "//xla/service:collective_ops_utils",
-        "//xla/stream_executor",
+        "@intel_extension_for_openxla//xla/service/gpu:ccl_ops",
         "@com_google_absl//absl/status",
         "@com_google_absl//absl/status:statusor",
         "@tsl//tsl/concurrency:ref_count",
@@ -987,6 +1012,7 @@ cc_library(
     hdrs = ["nccl_clique_key.h"],
     compatible_with = get_compatible_with_portable(),
     deps = [
+        "//xla:executable_run_options",
         "//xla/service:global_device_id",
         "@com_google_absl//absl/algorithm:container",
         "@com_google_absl//absl/status",
@@ -1232,10 +1258,10 @@ cc_library(
         "//xla:status_macros",
         "//xla:util",
         "//xla/hlo/ir:hlo",
-        "//xla/mlir/runtime/ir:rt",
-        "//xla/mlir/runtime/transforms:compilation_pipeline_gpu",
-        "//xla/mlir/runtime/transforms:type_converter",
-        "//xla/runtime:executable",
+        # "//xla/mlir/runtime/ir:rt",
+        # "//xla/mlir/runtime/transforms:compilation_pipeline_gpu",
+        # "//xla/mlir/runtime/transforms:type_converter",
+        # "//xla/runtime:executable",
         "//xla/service:buffer_assignment",
         "//xla/service:executable",
         "//xla/service:hlo_execution_profile",
@@ -1244,7 +1270,8 @@ cc_library(
         "//xla/service:shaped_buffer",
         "//xla/service:stream_pool",
         "//xla/service:xla_debug_info_manager",
-        "//xla/service/gpu/runtime:executable",
+        # "//xla/service/gpu/runtime:executable",
+        # "//xla/service/gpu/runtime:support",
         "//xla/service/gpu/runtime:tracing",
         "//xla/stream_executor",
         "//xla/stream_executor:device_description",
@@ -1279,6 +1306,8 @@ cc_library(
         "@tsl//tsl/platform:statusor",
         "@tsl//tsl/profiler/lib:scoped_annotation",
         "@tsl//tsl/profiler/lib:traceme",
+        "@intel_extension_for_openxla//xla/service/gpu:xetla_gpu_fused_mha_runner",
+        "@intel_extension_for_openxla//xla/service/gpu:onednn_gpu_conv_runner",
     ] + if_gpu_is_configured([
         ":make_batch_pointers",
     ]) + if_cuda_is_configured([
@@ -2363,6 +2392,8 @@ cc_library(
         "//xla/stream_executor/rocm:rocblas_wrapper",
         "//xla/stream_executor/rocm:rocsolver_wrapper",
         "//xla/stream_executor/rocm:hipsolver_wrapper",
+    ]) + if_sycl_is_configured([
+        "@local_config_sycl//sycl:sycl_headers",
     ]),
 )
 
@@ -3037,6 +3068,7 @@ cc_library(
         "//xla/stream_executor/cuda:cuda_platform_id",
         "//xla/stream_executor/host:host_platform_id",
         "//xla/stream_executor/rocm:rocm_platform_id",
+        "@intel_extension_for_openxla//xla/stream_executor/sycl:sycl_platform_id",
         "@com_google_absl//absl/cleanup",
         "@com_google_absl//absl/status",
         "@llvm-project//llvm:Core",
@@ -3123,7 +3155,7 @@ cc_library(
         "//xla/service:hlo_ordering",
         "//xla/service:hlo_proto_cc",
         "//xla/service:logical_buffer",
-        "//xla/service/gpu/runtime:executable",
+        # "//xla/service/gpu/runtime:executable",
         "//xla/service/gpu/runtime3:conditional_thunk",
         "//xla/service/gpu/runtime3:sequential_thunk",
         "//xla/service/gpu/runtime3:while_thunk",
@@ -3344,6 +3376,7 @@ cc_library(
         "TENSORFLOW_USE_ROCM=1",
     ]),
     deps = if_gpu_is_configured([
+        "@intel_extension_for_openxla//xla/service/gpu:dot_expand_dims",
         ":address_computation_fusion_rewriter",
         ":alias_passthrough_params",
         ":all_reduce_blueconnect",
@@ -3422,9 +3455,9 @@ cc_library(
         "//xla:xla_proto_cc",
         "//xla/hlo/ir:hlo",
         "//xla/hlo/transforms:hlo_constant_splitter",
-        "//xla/mlir/backends/gpu/transforms:passes",
-        "//xla/mlir/runtime/transforms:compilation_pipeline_gpu",
-        "//xla/runtime:jit_executable",
+        # "//xla/mlir/backends/gpu/transforms:passes",
+        # "//xla/mlir/runtime/transforms:compilation_pipeline_gpu",
+        # "//xla/runtime:jit_executable",
         "//xla/service:algebraic_simplifier",
         "//xla/service:all_gather_broadcast_reorder",
         "//xla/service:all_gather_combiner",
@@ -3553,7 +3586,7 @@ cc_library(
         "//xla/service:hlo_ordering",
         "//xla/service:layout_assignment",
         "//xla/service:logical_buffer",
-        "//xla/service/gpu/runtime:executable",
+        # "//xla/service/gpu/runtime:executable",
         "//xla/stream_executor/rocm:rocm_platform_id",
         "@tsl//tsl/platform:numbers",
     ]) + xla_export_hlo_deps() + [
@@ -3759,6 +3792,60 @@ xla_cc_test(
     ],
 )
 
+cc_library(
+    name = "spir_compiler_impl",
+    srcs = [
+        "spir_compiler.cc",
+    ],
+    hdrs = [
+        "spir_compiler.h",
+    ],
+    deps = [
+        "@intel_extension_for_openxla//xla/service/gpu:redundant_convert_mover",
+        "@intel_extension_for_openxla//xla/stream_executor/sycl:hw_info",
+        "@intel_extension_for_openxla//xla/stream_executor/sycl:sycl_platform_id",
+        "@com_google_absl//absl/base",
+        "@com_google_absl//absl/container:node_hash_map",
+        "@com_google_absl//absl/types:optional",
+        "@llvm-project//llvm:IRReader",
+        "@llvm-project//llvm:Support",
+        "//xla/service:dot_dimension_merger",
+        "//xla/service:float_normalization",
+        "//xla/service:float_support",
+        "//xla/service:hlo_constant_folding",
+        "//xla/service:hlo_cse",
+        "//xla/service:hlo_dce",
+        "//xla/service:hlo_pass",
+        "//xla/service:hlo_pass_pipeline",
+        "//xla/service:hlo_proto_cc",
+        "//xla/service:hlo_verifier",
+        "//xla/service:llvm_compiler",
+        "//xla/service:reshape_mover",
+        "//xla/service:tuple_simplifier",
+        "//xla/service/gpu:cudnn_fused_conv_rewriter",
+        "//xla/service/gpu:cudnn_fused_mha_rewriter",
+        "//xla/service/gpu:cusolver_rewriter",
+        "//xla/service/gpu:gpu_compiler",
+        "//xla/service/gpu:gpu_conv_padding_legalization",
+        "//xla/service/gpu:target_constants",
+        "//xla/service/gpu:triangular_solve_rewriter",
+        "//xla/service/gpu/llvm_gpu_backend",
+    ],
+)
+
+cc_library(
+    name = "spir_compiler",
+    srcs = [
+        "spir_compiler_registration.cc",
+    ],
+    deps = [
+        ":spir_compiler_impl",
+        "@intel_extension_for_openxla//xla/stream_executor/sycl:sycl_platform_id",
+        "@tsl//tsl/platform:path",
+    ],
+    alwayslink = True,  # Contains compiler registration
+)
+
 xla_cc_test(
     name = "gpu_aot_compilation_test",
     srcs = if_gpu_is_configured([
diff --git a/xla/service/gpu/buffer_sharing.cc b/xla/service/gpu/buffer_sharing.cc
index abfcefdd3f..4041bdd1ca 100644
--- a/xla/service/gpu/buffer_sharing.cc
+++ b/xla/service/gpu/buffer_sharing.cc
@@ -215,6 +215,15 @@ std::optional<bool> CanShareBufferHint(const HloInstruction* user,
                 ->gemm_backend_config();
         return (config.beta() != 0.) && user->operand(2) == operand;
       }
+      // SYCL: inplace sum for onednn conv with side input.
+      if (user->custom_call_target() ==
+          kCudnnConvBiasActivationForwardCallTarget) {
+        CudnnConvBackendConfig config =
+            std::move(user->backend_config<GpuBackendConfig>())
+                ->cudnn_conv_backend_config();
+        return (config.side_input_scale() != 0.) &&
+               (user->operand(user->operand_count() - 1) == operand);
+      }
       // The operand of cholesky can be shared with the first output.
       if (user->custom_call_target() == kCusolverCholeskyCallTarget) {
         return user_index.size() == 1 && user_index[0] == 0;
diff --git a/xla/service/gpu/ccl_api.cc b/xla/service/gpu/ccl_api.cc
new file mode 100644
index 0000000000..f978e9b740
--- /dev/null
+++ b/xla/service/gpu/ccl_api.cc
@@ -0,0 +1,281 @@
+/* Copyright (c) 2024 Intel Corporation
+
+Copyright 2019 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "xla/service/gpu/ccl_api.h"
+
+#include <cstddef>
+#include <cstdint>
+
+#include "absl/status/status.h"
+#include "absl/status/statusor.h"
+#include "tsl/concurrency/ref_count.h"
+#include "xla/service/collective_ops_utils.h"
+#include "xla/service/gpu/ccl_ops.h"
+#include "xla/service/gpu/nccl_api.h"
+#include "xla/service/gpu/nccl_clique_key.h"
+#include "xla/stream_executor/device_memory.h"
+#include "xla/stream_executor/gpu/gpu_stream.h"
+#include "xla/stream_executor/stream.h"
+
+namespace xla::gpu {
+//==-----------------------------------------------------------------------===//
+// NcclApi::PersistentPlanAllocator
+//==-----------------------------------------------------------------------===//
+
+using PersistentPlanAllocator = NcclApi::PersistentPlanAllocator;
+using ScopedPersistentPlanAllocator = NcclApi::ScopedPersistentPlanAllocator;
+
+PersistentPlanAllocator::PersistentPlanAllocator(int64_t,
+                                                 se::DeviceMemoryAllocator*,
+                                                 se::Stream*) {
+  // Suppress clang unused private field warnings.
+  (void)device_ordinal_;
+  (void)allocator_;
+  (void)stream_;
+}
+
+PersistentPlanAllocator::~PersistentPlanAllocator() = default;
+
+absl::StatusOr<se::DeviceMemoryBase>
+PersistentPlanAllocator::AllocateAndInitialize(void*, size_t) {
+  return absl::UnimplementedError("XLA compiled without NCCL support");
+}
+
+absl::Status PersistentPlanAllocator::Deallocate(se::DeviceMemoryBase mem) {
+  return absl::UnimplementedError("XLA compiled without NCCL support");
+}
+
+ScopedPersistentPlanAllocator::ScopedPersistentPlanAllocator(
+    NcclCommHandle, tsl::RCReference<PersistentPlanAllocator>) {
+  // Suppress clang unused private field warnings.
+  (void)comm_;
+  (void)recover_;
+  (void)allocator_;
+}
+
+ScopedPersistentPlanAllocator::~ScopedPersistentPlanAllocator() = default;
+
+//===----------------------------------------------------------------------===//
+// CclApi
+//===----------------------------------------------------------------------===//
+
+static absl::Status UnimplementedError(std::string mes = "") {
+  return absl::UnimplementedError("XLA compiled without CCL support: " + mes);
+}
+
+CclApi::CclApi() {}
+
+absl::StatusOr<NcclCliqueId> CclApi::GetUniqueId() { return NcclCliqueId(); }
+
+absl::StatusOr<NcclCliqueId> CclApi::GetId(const NcclCliqueKey& key,
+                                           const RunId& id) {
+  std::string new_id =
+      id.ToString() + "=" + GlobalDeviceIdsToString(key.devices());
+  TF_RET_CHECK(new_id.size() < NcclCliqueId::kSize)
+      << "Run ID length must < kSize(" << NcclCliqueId::kSize << ").";
+  new_id.resize(NcclCliqueId::kSize);
+
+  return NcclCliqueId().FromString(new_id);
+}
+
+absl::StatusOr<CclApi::OwnedNcclComm> CclApi::CommInitRank(
+    int32_t nranks, const NcclCliqueId& clique_id, int32_t rank) {
+  VLOG(1) << "Initialize NCCL communicator for rank #" << rank << " of "
+          << nranks << "; hash(id)=" << absl::HashOf(clique_id.data());
+
+  if (rank < 0 || rank >= nranks)
+    return absl::InvalidArgumentError(absl::StrFormat(
+        "Invalid rank %d, it must be in [0, %d) range", rank, nranks));
+
+  NcclCommHandle comm = reinterpret_cast<NcclCommHandle>(
+      new ccl::communicator(nranks, rank, clique_id.ToString()));
+
+  return CclApi::OwnedNcclComm(comm, NcclCommDeleter{this});
+}
+
+absl::Status CclApi::CommAbort(NcclCommHandle) {
+  // Don't need now
+  return UnimplementedError("CommAbort");
+}
+
+absl::Status CclApi::CommFinalize(NcclCommHandle) {
+  // Don't need now
+  return UnimplementedError("CommFinalize");
+}
+
+absl::Status CclApi::CommDestroy(NcclCommHandle comm) {
+  delete reinterpret_cast<ncclComm_t>(comm);
+  return absl::OkStatus();
+}
+
+absl::StatusOr<int32_t> CclApi::CommCount(NcclCommHandle) {
+  // Don't need now
+  return UnimplementedError("CommCount");
+}
+
+absl::Status CclApi::CommGetAsyncError(NcclCommHandle comm) {
+  return absl::OkStatus();
+}
+
+absl::Status CclApi::GroupStart() {
+  // Don't need now
+  return UnimplementedError("GroupStart");
+}
+
+absl::Status CclApi::GroupEnd() {
+  // Don't need now
+  return UnimplementedError("GroupEnd");
+}
+
+absl::Status CclApi::AllReduce(se::DeviceMemoryBase send_buffer,
+                               se::DeviceMemoryBase recv_buffer,
+                               PrimitiveType dtype, size_t count,
+                               ReductionKind reduction_kind,
+                               NcclCommHandle comm, se::Stream* stream) {
+  se::gpu::GpuStreamHandle gpu_stream = se::gpu::AsGpuStreamValue(stream);
+
+  const void* send_buffer_ = send_buffer.opaque();
+  void* recv_buffer_ = recv_buffer.opaque();
+  int element_count = count * (primitive_util::IsComplexType(dtype) ? 2 : 1);
+
+  auto comm_ = reinterpret_cast<ncclComm_t>(comm);
+
+  sycl_allreduce(send_buffer_, recv_buffer_, element_count, dtype,
+                 reduction_kind, gpu_stream, comm_);
+  return absl::OkStatus();
+}
+
+absl::Status CclApi::ReduceScatter(se::DeviceMemoryBase send_buffer,
+                                   se::DeviceMemoryBase recv_buffer,
+                                   PrimitiveType dtype, size_t count,
+                                   ReductionKind reduction_kind,
+                                   NcclCommHandle comm, se::Stream* stream) {
+  se::gpu::GpuStreamHandle gpu_stream = se::gpu::AsGpuStreamValue(stream);
+
+  const void* send_buffer_ = send_buffer.opaque();
+  void* recv_buffer_ = recv_buffer.opaque();
+  int element_count = count * (primitive_util::IsComplexType(dtype) ? 2 : 1);
+
+  auto comm_ = reinterpret_cast<ncclComm_t>(comm);
+
+  int num_participants = comm_->nranks;
+  TF_RET_CHECK(element_count % num_participants == 0)
+      << "Source buffer was not an exact multiple of the number of "
+         "participants.";
+  int64_t recv_count = element_count / num_participants;
+  VLOG(3) << absl::StreamFormat(
+      "Calling ncclReduceScatter(send_buffer=%p, recv_buffer=%p, "
+      "recvcount=%d, "
+      "comm=%p, stream=%p)",
+      send_buffer_, recv_buffer_, recv_count, static_cast<const void*>(comm_),
+      gpu_stream);
+
+  sycl_reduce_scatter(send_buffer_, recv_buffer_, recv_count, dtype,
+                      reduction_kind, gpu_stream, comm_);
+  return absl::OkStatus();
+}
+
+absl::Status CclApi::AllGather(se::DeviceMemoryBase send_buffer,
+                               se::DeviceMemoryBase recv_buffer,
+                               PrimitiveType dtype, size_t count,
+                               NcclCommHandle comm, se::Stream* stream) {
+  se::gpu::GpuStreamHandle gpu_stream = se::gpu::AsGpuStreamValue(stream);
+
+  const void* send_buffer_ = send_buffer.opaque();
+  void* recv_buffer_ = recv_buffer.opaque();
+  int element_count = count * (primitive_util::IsComplexType(dtype) ? 2 : 1);
+
+  auto comm_ = reinterpret_cast<ncclComm_t>(comm);
+
+  sycl_allgather(send_buffer_, recv_buffer_, element_count, dtype, gpu_stream,
+                 comm_);
+  return absl::OkStatus();
+}
+
+absl::Status CclApi::AllToAll(bool has_split_dimension,
+                              std::vector<const void*>& send_buffers,
+                              std::vector<void*>& recv_buffers,
+                              int element_count, PrimitiveType element_type,
+                              NcclCommHandle comm, se::Stream* stream) {
+  auto comm_ = reinterpret_cast<ncclComm_t>(comm);
+
+  se::gpu::GpuStreamHandle gpu_stream = se::gpu::AsGpuStreamValue(stream);
+
+  if (has_split_dimension) {
+    sycl_alltoall_split(send_buffers, recv_buffers, element_count, element_type,
+                        gpu_stream, comm_);
+  } else {
+    sycl_alltoall(send_buffers, recv_buffers, element_count, element_type,
+                  gpu_stream, comm_);
+  }
+
+  return absl::OkStatus();
+}
+
+absl::Status CclApi::CollectivePermute(se::DeviceMemoryBase src_addr,
+                                       se::DeviceMemoryBase dest_addr,
+                                       int element_count,
+                                       PrimitiveType element_type,
+                                       const std::optional<int64_t> source_id,
+                                       const std::optional<int64_t> target_id,
+                                       NcclCommHandle comm,
+                                       se::Stream* stream) {
+  auto comm_ = reinterpret_cast<ncclComm_t>(comm);
+
+  se::gpu::GpuStreamHandle gpu_stream = se::gpu::AsGpuStreamValue(stream);
+
+  sycl_collective_permute(src_addr.opaque(), dest_addr.opaque(), element_count,
+                          element_type, source_id, target_id, gpu_stream,
+                          comm_);
+
+  return absl::OkStatus();
+}
+
+absl::Status CclApi::Send(se::DeviceMemoryBase, PrimitiveType, size_t, int32_t,
+                          NcclCommHandle, se::Stream*) {
+  // Don't need now
+  return UnimplementedError("Send");
+}
+
+absl::Status CclApi::Recv(se::DeviceMemoryBase, PrimitiveType, size_t, int32_t,
+                          NcclCommHandle, se::Stream*) {
+  // Don't need now
+  return UnimplementedError("Recv");
+}
+
+absl::StatusOr<CclApi::NcclRegisteredBufferHandle> CclApi::RegisterBuffer(
+    NcclCommHandle, se::DeviceMemoryBase) {
+  // Don't need now
+  return UnimplementedError("RegisterBuffer");
+}
+
+absl::StatusOr<CclApi::NcclRegisteredBufferHandle> CclApi::DeregisterBuffer(
+    NcclCommHandle, CclApi::NcclRegisteredBufferHandle) {
+  // Don't need now
+  return UnimplementedError("DeregisterBuffer");
+}
+
+ncclComm_t CastCCLComm(CclApi::NcclCommHandle comm) {
+  return reinterpret_cast<ncclComm_t>(comm);
+}
+
+NcclApi* NcclApi::Default() {
+  static auto* ccl_api = new CclApi();
+  return ccl_api;
+}
+
+}  // namespace xla::gpu
diff --git a/xla/service/gpu/ccl_api.h b/xla/service/gpu/ccl_api.h
new file mode 100644
index 0000000000..ed898746f7
--- /dev/null
+++ b/xla/service/gpu/ccl_api.h
@@ -0,0 +1,106 @@
+/* Copyright (c) 2024 Intel Corporation
+
+Copyright 2019 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+#ifndef XLA_SERVICE_GPU_CCL_API_H_
+#define XLA_SERVICE_GPU_CCL_API_H_
+
+#include <cstddef>
+#include <cstdint>
+
+#include "absl/status/status.h"
+#include "absl/status/statusor.h"
+#include "tsl/concurrency/ref_count.h"
+#include "xla/service/collective_ops_utils.h"
+#include "xla/service/gpu/ccl_ops.h"
+#include "xla/service/gpu/nccl_api.h"
+#include "xla/service/gpu/nccl_clique_key.h"
+#include "xla/stream_executor/device_memory.h"
+#include "xla/stream_executor/gpu/gpu_stream.h"
+#include "xla/stream_executor/stream.h"
+
+namespace xla::gpu {
+//===----------------------------------------------------------------------===//
+// CclApi
+//===----------------------------------------------------------------------===//
+
+class CclApi final : public NcclApi {
+ public:
+  CclApi();
+
+  absl::StatusOr<NcclCliqueId> GetUniqueId() final;
+
+  absl::StatusOr<NcclCliqueId> GetId(const NcclCliqueKey& key,
+                                     const RunId& id) final;
+
+  absl::StatusOr<OwnedNcclComm> CommInitRank(int32_t nranks,
+                                             const NcclCliqueId& clique_id,
+                                             int32_t rank) final;
+
+  absl::Status CommAbort(NcclCommHandle) final;
+  absl::Status CommFinalize(NcclCommHandle) final;
+  absl::Status CommDestroy(NcclCommHandle comm) final;
+  absl::StatusOr<int32_t> CommCount(NcclCommHandle) final;
+  absl::Status CommGetAsyncError(NcclCommHandle comm) final;
+
+  absl::Status GroupStart() final;
+
+  absl::Status GroupEnd() final;
+
+  absl::Status AllReduce(se::DeviceMemoryBase send_buffer,
+                         se::DeviceMemoryBase recv_buffer, PrimitiveType dtype,
+                         size_t count, ReductionKind reduction_kind,
+                         NcclCommHandle comm, se::Stream* stream) final;
+
+  absl::Status ReduceScatter(se::DeviceMemoryBase send_buffer,
+                             se::DeviceMemoryBase recv_buffer,
+                             PrimitiveType dtype, size_t count,
+                             ReductionKind reduction_kind, NcclCommHandle comm,
+                             se::Stream* stream) final;
+
+  absl::Status AllGather(se::DeviceMemoryBase send_buffer,
+                         se::DeviceMemoryBase recv_buffer, PrimitiveType dtype,
+                         size_t count, NcclCommHandle comm,
+                         se::Stream* stream) final;
+
+  absl::Status AllToAll(bool has_split_dimension,
+                        std::vector<const void*>& send_buffers,
+                        std::vector<void*>& recv_buffers, int element_count,
+                        PrimitiveType element_type, NcclCommHandle comm,
+                        se::Stream* stream);
+
+  absl::Status CollectivePermute(se::DeviceMemoryBase src_addr,
+                                 se::DeviceMemoryBase dest_addr,
+                                 int element_count, PrimitiveType element_type,
+                                 const std::optional<int64_t> source_id,
+                                 const std::optional<int64_t> target_id,
+                                 NcclCommHandle comm, se::Stream* stream);
+
+  absl::Status Send(se::DeviceMemoryBase, PrimitiveType, size_t, int32_t,
+                    NcclCommHandle, se::Stream*) final;
+  absl::Status Recv(se::DeviceMemoryBase, PrimitiveType, size_t, int32_t,
+                    NcclCommHandle, se::Stream*) final;
+
+  absl::StatusOr<NcclRegisteredBufferHandle> RegisterBuffer(
+      NcclCommHandle, se::DeviceMemoryBase) final;
+
+  absl::StatusOr<NcclRegisteredBufferHandle> DeregisterBuffer(
+      NcclCommHandle, NcclRegisteredBufferHandle) final;
+};
+
+ncclComm_t CastCCLComm(CclApi::NcclCommHandle);
+
+}  // namespace xla::gpu
+#endif  // XLA_SERVICE_GPU_CCL_API_H_
diff --git a/xla/service/gpu/compile_module_to_llvm_ir.cc b/xla/service/gpu/compile_module_to_llvm_ir.cc
index 80f74bb9fc..82d4f1a133 100644
--- a/xla/service/gpu/compile_module_to_llvm_ir.cc
+++ b/xla/service/gpu/compile_module_to_llvm_ir.cc
@@ -69,7 +69,7 @@ limitations under the License.
 #include "xla/service/gpu/ir_emitter_context.h"
 #include "xla/service/gpu/ir_emitter_unnested.h"
 #include "xla/service/gpu/metrics.h"
-#include "xla/service/gpu/runtime/executable.h"
+// #include "xla/service/gpu/runtime/executable.h"
 #include "xla/service/gpu/runtime3/conditional_thunk.h"
 #include "xla/service/gpu/runtime3/sequential_thunk.h"
 #include "xla/service/gpu/runtime3/while_thunk.h"
@@ -147,6 +147,7 @@ class DumpAfterPassIfEnabled : public mlir::PassInstrumentation {
   int pass_counter_ = 0;
 };
 
+#if 0
 // Lowers MLIR module to the XLA Gpu runtime custom calls.
 static absl::Status LowerToXlaGpuRuntime(
     mlir::ModuleOp module, llvm::StringRef entry_function_name,
@@ -193,7 +194,7 @@ static absl::Status LowerToXlaGpuRuntime(
 
   return absl::OkStatus();
 }
-
+#endif
 }  // namespace
 
 void ForAllThunks(const std::function<void(Thunk*)>& fn,
@@ -228,6 +229,7 @@ static void ForwardCollectiveAttrs(mlir::ModuleOp module,
   func->setAttr("num_partitions", b.getI64IntegerAttr(config.num_partitions()));
 }
 
+#if 0
 absl::StatusOr<GpuExecutable::OwnedGpuRuntimeProgram> LowerToJitRt(
     mlir::ModuleOp mlir_module, llvm::StringRef entry_function_name,
     llvm::ArrayRef<int64_t> buffer_sizes,
@@ -257,6 +259,7 @@ absl::StatusOr<GpuExecutable::OwnedGpuRuntimeProgram> LowerToJitRt(
       entry_function_name.str(), std::move(module_str), buffer_sizes.vec(),
       std::move(allocation_indices), module_config.debug_options());
 }
+#endif
 
 // Analyze the function signature to reconstruct a vector of BufferAllocation
 // objects, as well as other output information.
@@ -451,12 +454,13 @@ absl::StatusOr<CompileModuleResults> CompileModuleToLlvmIr(
     llvm::transform(
         results.allocations, std::back_inserter(buffer_sizes),
         [](const BufferAllocation& allocation) { return allocation.size(); });
-
+#if 0
     TF_ASSIGN_OR_RETURN(
         results.executable,
         LowerToJitRt(*mlir_module, entry_function.getName(), buffer_sizes,
                      ir_emitter->ConsumeThunkSequence(), hlo_module,
                      gpu_device_info.gpu_compute_capability()));
+#endif
   } else {
     auto thunk_sequence = ir_emitter->ConsumeThunkSequence();
     ForAllThunks([](Thunk* thunk) { thunk->ClearCompileTimeInfo(); },
diff --git a/xla/service/gpu/cublas_cudnn.cc b/xla/service/gpu/cublas_cudnn.cc
index fed9fe599b..fb946565d9 100644
--- a/xla/service/gpu/cublas_cudnn.cc
+++ b/xla/service/gpu/cublas_cudnn.cc
@@ -53,6 +53,7 @@ bool IsTriangularSolve(const HloInstruction& hlo) {
          hlo.custom_call_target() == kTriangularSolveCallTarget;
 }
 
+const absl::string_view kCudnnfQKVCallTarget = "__cudnn$fQKV";
 const absl::string_view kGemmCallTarget = "__cublas$gemm";
 const absl::string_view kCublasLtMatmulCallTarget = "__cublas$lt$matmul";
 const absl::string_view kCublasLtMatmulF8CallTarget = "__cublas$lt$matmul$f8";
@@ -83,6 +84,8 @@ const absl::string_view kCudnnfMHAScaleBiasMaskSoftmaxDropoutCallTarget =
     "__cudnn$fhmaScaleBiasMaskSoftmaxDropout";
 const absl::string_view kCudnnfMHAScaleBiasSoftmaxDropoutCallTarget =
     "__cudnn$fhmaScaleBiasSoftmaxDropout";
+const absl::string_view kCudnnfMHAScaleSoftmaxCallTarget =
+    "__cudnn$fmhaScaleSoftmax";
 const absl::string_view kCudnnfMHAScaleBiasSoftmaxCallTarget =
     "__cudnn$fhmaScaleBiasSoftmax";
 const absl::string_view kCudnnfMHAScaleMaskSoftmaxCallTarget =
@@ -155,6 +158,7 @@ bool IsFwdCustomCallTofMHA(const HloInstruction& hlo) {
          target == kCudnnfMHAScaleBiasMaskSoftmaxDropoutCallTarget ||
          target == kCudnnfMHAScaleMaskSoftmaxCallTarget ||
          target == kCudnnfMHAScaleMaskSoftmaxDropoutCallTarget ||
+         target == kCudnnfMHAScaleSoftmaxCallTarget ||target == kCudnnfMHAScaleSoftmaxCallTarget ||
          target == kCudnnfMHASoftmaxDropoutCallTarget ||
          target == kCudnnfMHAScaleBiasSoftmaxCallTarget ||
          target == kCudnnfMHAScaleBiasSoftmaxDropoutCallTarget;
@@ -187,6 +191,11 @@ bool MHACallHasDropout(const absl::string_view fmha_call_name) {
          fmha_call_name == kCudnnfMHAScaleMaskSoftmaxDropoutBackwardCallTarget;
 }
 
+bool IsCustomCallTofQKV(const HloInstruction& hlo) {
+  const auto& target = hlo.custom_call_target();
+  return target == kCudnnfQKVCallTarget;
+}
+
 bool IsCustomCallTofMHA(const HloInstruction& hlo) {
   return (IsFwdCustomCallTofMHA(hlo) || IsBwdCustomCallTofMHA(hlo));
 }
@@ -244,6 +253,8 @@ absl::StatusOr<CudnnfMHAKind> GetCudnnfMHAKind(
     return CudnnfMHAKind::kScaleMaskSoftmax;
   if (target == kCudnnfMHAScaleMaskSoftmaxDropoutCallTarget)
     return CudnnfMHAKind::kScaleMaskSoftmaxDropout;
+  if (target == kCudnnfMHAScaleSoftmaxCallTarget)
+    return CudnnfMHAKind::kScaleSoftmax;
   if (target == kCudnnfMHASoftmaxDropoutCallTarget)
     return CudnnfMHAKind::kSoftmaxDropout;
   if (target == kCudnnfMHASoftmaxCallTarget) return CudnnfMHAKind::kSoftmax;
@@ -293,6 +304,8 @@ std::string CudnnfMHAKindToString(CudnnfMHAKind kind) {
       return "fmha_bias_softmax_with_dropout";
     case CudnnfMHAKind::kScaleBiasSoftmax:
       return "fmha_bias_softmax";
+    case CudnnfMHAKind::kScaleSoftmax:
+       return "fmha_scale_softmax";
     // backward
     case CudnnfMHAKind::kBackwardBmmBmm:
       return "fused_batched_matmuls_backward";
diff --git a/xla/service/gpu/cublas_cudnn.h b/xla/service/gpu/cublas_cudnn.h
index cd9e78e805..de5ad6cde5 100644
--- a/xla/service/gpu/cublas_cudnn.h
+++ b/xla/service/gpu/cublas_cudnn.h
@@ -56,6 +56,7 @@ enum class CudnnfMHAKind {
   kScaleMaskSoftmaxDropout,
   kSoftmaxDropout,
   kSoftmax,
+  kScaleSoftmax,
   kScaleBiasSoftmax,
   kScaleBiasSoftmaxDropout,
   kBackwardBmmBmm,
@@ -176,6 +177,7 @@ bool IsCustomCallToDnnNorm(const HloInstruction& hlo);
 // 7. BMM1 - Softmax - BMM2
 // 8. BMM1 - scale - Bias - Softmax - BMM2
 // Forward calls
+extern const absl::string_view kCudnnfQKVCallTarget;
 extern const absl::string_view kCudnnfMHABmmBmmCallTarget;
 extern const absl::string_view kCudnnfMHASoftmaxCallTarget;
 extern const absl::string_view kCudnnfMHAScaleBiasMaskSoftmaxCallTarget;
@@ -185,6 +187,7 @@ extern const absl::string_view kCudnnfMHAScaleMaskSoftmaxDropoutCallTarget;
 extern const absl::string_view kCudnnfMHASoftmaxDropoutCallTarget;
 extern const absl::string_view kCudnnfMHAScaleBiasSoftmaxDropoutCallTarget;
 extern const absl::string_view kCudnnfMHAScaleBiasSoftmaxCallTarget;
+extern const absl::string_view kCudnnfMHAScaleSoftmaxCallTarget;
 // Backward calls
 extern const absl::string_view kCudnnfMHABmmBmmBackwardCallTarget;
 extern const absl::string_view kCudnnfMHASoftmaxBackwardCallTarget;
@@ -202,6 +205,8 @@ extern const absl::string_view kCudnnfMHAScaleBiasSoftmaxBackwardCallTarget;
 bool IsFwdCustomCallTofMHA(const HloInstruction& hlo);
 bool IsBwdCustomCallTofMHA(const HloInstruction& hlo);
 bool IsCustomCallTofMHA(const HloInstruction& hlo);
+bool IsCustomCallTofQKV(const HloInstruction& hlo);
+
 
 absl::StatusOr<CudnnfMHAKind> GetCudnnfMHAKind(
     const HloCustomCallInstruction* instr);
diff --git a/xla/service/gpu/cudnn_fused_conv_rewriter.cc b/xla/service/gpu/cudnn_fused_conv_rewriter.cc
index dca7a07a8d..58b2cbf7ab 100644
--- a/xla/service/gpu/cudnn_fused_conv_rewriter.cc
+++ b/xla/service/gpu/cudnn_fused_conv_rewriter.cc
@@ -846,6 +846,13 @@ absl::StatusOr<bool> FuseBiasOrSideInput(HloComputation* comp) {
         conv->CloneWithNewOperands(conv->shape(), new_operands));
     comp->parent()->SetAndUniquifyInstrName(new_conv, conv->name());
     TF_RETURN_IF_ERROR(new_conv->set_backend_config(gpu_config));
+#if TENSORFLOW_USE_SYCL
+    if (can_accept_side_input) {
+      xla::Cast<HloCustomCallInstruction>(new_conv)
+          ->set_output_to_operand_aliasing(
+              {{{}, {static_cast<long>(new_operands.size()) - 1, {}}}});
+    }
+#endif
     TF_ASSIGN_OR_RETURN(HloInstruction * new_instr,
                         MakeGetTupleElementHlo(new_conv, 0));
     TF_RETURN_IF_ERROR(comp->ReplaceInstruction(instr, new_instr));
diff --git a/xla/service/gpu/cudnn_fused_mha_rewriter.cc b/xla/service/gpu/cudnn_fused_mha_rewriter.cc
index 295f9ca78d..f4063622e4 100644
--- a/xla/service/gpu/cudnn_fused_mha_rewriter.cc
+++ b/xla/service/gpu/cudnn_fused_mha_rewriter.cc
@@ -328,16 +328,24 @@ bool IsSupportedPrimitiveType(const HloInstruction* bmm) {
 }
 
 bool IsContractingDimSupported(absl::Span<const int64_t> contracting_dims) {
+#if TENSORFLOW_USE_SYCL
+  return true;
+#else
   return absl::c_all_of(contracting_dims,
                         [](int64_t dim) { return dim == 64; });
+#endif
 }
 
 bool IsNonContractingDimSupported(
     const std::vector<int64_t>& non_contracting_dims, bool is_training) {
+#if TENSORFLOW_USE_SYCL
+  return true;
+#else
   // For training, cuDNN require non_contracting_dim to be Divisible by 64
   return absl::c_all_of(non_contracting_dims, [&](int64_t dim) {
     return dim <= 512 && (!is_training || dim % 64 == 0);
   });
+#endif
 }
 
 std::vector<int64_t> GetDimensionVector(absl::Span<const int64_t> dimensions,
@@ -426,6 +434,7 @@ absl::StatusOr<bool> IsSupportedBMM2(const HloInstruction* bmm_2,
   std::vector<int64_t> non_contracting_dims_bmm2 =
       GetDimensionVector(bmm_2->operand(operand_index)->shape().dimensions(),
                          non_contracting_dim_nums_bmm2);
+#if !TENSORFLOW_USE_SYCL
   // The non contracting dimension for BMM2 needs to be 64 for the input matrix.
   // The input matrix is the second argument to BMM2 i.e, rhs.
   if (!absl::c_all_of(non_contracting_dims_bmm2,
@@ -437,6 +446,7 @@ absl::StatusOr<bool> IsSupportedBMM2(const HloInstruction* bmm_2,
     }
     return false;
   }
+#endif
   return true;
 }
 
@@ -670,6 +680,7 @@ MatchFwdResult MatchBmm1UnfusedBiasSoftmaxBmm2(MatchFwdResult previous_result,
                 first_bmm_pattern, unfused_scaled_bmm_subpattern))))) {
     // bmm1 - (scale) - softmax
     match_result.matched_bmm_1 = bmm_1;
+    match_result.matched_scale = scale;
     match_result.matched_custom_call_name =
         has_dropout ? kCudnnfMHASoftmaxDropoutCallTarget
                     : kCudnnfMHASoftmaxCallTarget;
@@ -1370,6 +1381,9 @@ absl::StatusOr<HloInstruction*> ChangeCheckedDimToFastest(
                 minor_to_major_to_check),
             operand_bmm, perm),
         &operand_bmm->metadata());
+#if TENSORFLOW_USE_SYCL
+    bmm->ReplaceOperandWithDifferentShape(bmm_operand, operand_bmm);
+#endif
     *((DynCast<HloDotInstruction>(bmm))->mutable_dot_dimension_numbers()) =
         new_dot_dims_bmm;
   }
@@ -1860,12 +1874,15 @@ absl::StatusOr<bool> CudnnFusedMHARewriter::Run(
 #if CUDA_VERSION < 12000
     return false;
 #endif
+
+#if !TENSORFLOW_USE_SYCL
     if (!debug_options.xla_gpu_enable_cudnn_fmha() ||
         !IsComputeCapabilityAndCudnnSupported(
             compute_capability_, cudnn_version,
             stream_executor::dnn::VersionInfo(8, 8, 0))) {
       return false;
     }
+#endif  // !TENSORFLOW_USE_SYCL
     for (HloInstruction* instr : comp->MakeInstructionPostOrder()) {
       bool v_transposed = false;
       bool changed = false;
@@ -1936,6 +1953,8 @@ absl::StatusOr<bool> CudnnFusedMHARewriter::Run(
               matched_result.is_causal_mask,
               matched_result.is_flash_attention));
       any_changed |= changed;
+
+#if !TENSORFLOW_USE_SYCL
       if (matched_result.is_training) {
         MatchBwdResult matched_bwd_result =
             MatchBwdMHAPatternsForCanonicalization(
@@ -2019,6 +2038,7 @@ absl::StatusOr<bool> CudnnFusedMHARewriter::Run(
                 matched_bwd_result.matched_custom_call_name));
         any_changed |= changed;
       }
+#endif  // !TENSORFLOW_USE_SYCL
     }
   }
 
diff --git a/xla/service/gpu/cusolver_context.cc b/xla/service/gpu/cusolver_context.cc
index d63f7a7cc1..51023e44cc 100644
--- a/xla/service/gpu/cusolver_context.cc
+++ b/xla/service/gpu/cusolver_context.cc
@@ -40,6 +40,7 @@ limitations under the License.
 namespace xla {
 namespace gpu {
 
+#if !TENSORFLOW_USE_SYCL
 namespace {
 
 // Type traits to get CUDA complex types from std::complex<T>.
@@ -427,6 +428,19 @@ absl::Status GpuSolverContext::PotrfBatched(
 #endif
       ToDevicePointer(lapack_info), batch_size));
 }
+#else // !TENSORFLOW_USE_SYCL
 
+StatusOr<GpuSolverContext> GpuSolverContext::Create() {
+  return GpuSolverContext();
+}
+
+Status GpuSolverContext::SetStream(se::Stream* stream) {
+  gpu_stream_ = stream_executor::gpu::AsGpuStreamValue(stream);
+  return OkStatus();
+}
+
+GpuSolverContext::GpuSolverContext() {}
+
+#endif // !TENSORFLOW_USE_SYCL
 }  // namespace gpu
 }  // namespace xla
diff --git a/xla/service/gpu/cusolver_context.h b/xla/service/gpu/cusolver_context.h
index d17a570ef4..5602df2b38 100644
--- a/xla/service/gpu/cusolver_context.h
+++ b/xla/service/gpu/cusolver_context.h
@@ -26,6 +26,12 @@ limitations under the License.
 #define TENSORFLOW_USE_CUSOLVER_OR_HIPSOLVER \
   (!TENSORFLOW_USE_ROCM || TENSORFLOW_USE_HIPSOLVER)
 
+#if TENSORFLOW_USE_SYCL
+#include "oneapi/mkl/blas.hpp"
+#include "oneapi/mkl/lapack.hpp"
+#include "oneapi/mkl/dfti.hpp"
+#include "oneapi/mkl/exceptions.hpp"
+#else // TENSORFLOW_USE_SYCL
 #if !TENSORFLOW_USE_ROCM
 #include "third_party/gpus/cuda/include/cusolverDn.h"
 using gpusolverHandle_t = cusolverDnHandle_t;
@@ -41,16 +47,18 @@ using gpusolverHandle_t = hipsolverHandle_t;
 using gpusolverHandle_t = rocblas_handle;
 #endif  // TF_ROCM_VERSION >= 40500
 #endif  // TENSORFLOW_USE_ROCM
+#endif  // TENSORFLOW_USE_SYCL
 
 #include "xla/statusor.h"
 #include "xla/stream_executor/blas.h"
 #include "xla/stream_executor/stream_executor.h"
 #include "xla/xla_data.pb.h"
+#include "xla/stream_executor/gpu/gpu_types.h"
 
 namespace xla {
 namespace gpu {
-
 namespace se = ::stream_executor;
+#if !TENSORFLOW_USE_SYCL
 
 class GpuSolverContext {
  public:
@@ -104,6 +112,59 @@ class GpuSolverContext {
   std::unique_ptr<std::remove_pointer_t<gpusolverHandle_t>, Deleter> handle_;
 };
 
+#else // !TENSORFLOW_USE_SYCL
+class GpuSolverContext {
+ public:
+  static StatusOr<GpuSolverContext> Create();
+  Status SetStream(se::Stream* stream);
+
+  template <typename T>
+  Status PotrfBatched(se::blas::UpperLower uplo, int n, se::DeviceMemory<T*> as,
+                      int lda, se::DeviceMemory<int> lapack_info,
+                      int batch_size, T* a_base) {
+    T* scratch_data = static_cast<T*>(as.opaque());
+    int64_t scratchpad_size = as.size() / sizeof(T);
+
+    const int64_t stride_a = n * n;
+
+    oneapi::mkl::uplo params_uplo;
+    switch (uplo) {
+      case se::blas::UpperLower::kLower:
+        params_uplo = oneapi::mkl::uplo::L;
+        break;
+      case se::blas::UpperLower::kUpper:
+        params_uplo = oneapi::mkl::uplo::U;
+        break;
+      default:
+        params_uplo = static_cast<oneapi::mkl::uplo>(uplo);
+    }
+
+    try {
+      oneapi::mkl::lapack::potrf_batch(*gpu_stream_, params_uplo, n, a_base,
+                                       lda, stride_a, batch_size, scratch_data,
+                                       scratchpad_size);
+    } catch (oneapi::mkl::lapack::batch_error const& be) {
+      int i = 0;
+      auto& ids = be.ids();
+      for (auto const& e : be.exceptions()) {
+        try {
+          std::rethrow_exception(e);
+        } catch (oneapi::mkl::lapack::exception& e) {
+          LOG(ERROR) << "Exception " << ids[i++]
+                     << " in a batch says: " << e.what()
+                     << " (info code: " << e.info() << ")";
+        }
+      }
+    }
+    return OkStatus();
+  }
+
+ private:
+  explicit GpuSolverContext();
+  stream_executor::gpu::GpuStreamHandle gpu_stream_;
+};
+
+#endif // !TENSORFLOW_USE_SYCL
 }  // namespace gpu
 }  // namespace xla
 
diff --git a/xla/service/gpu/cusolver_rewriter.cc b/xla/service/gpu/cusolver_rewriter.cc
index fbf01f2f90..12a35d92e6 100644
--- a/xla/service/gpu/cusolver_rewriter.cc
+++ b/xla/service/gpu/cusolver_rewriter.cc
@@ -70,6 +70,7 @@ absl::StatusOr<HloInstruction*> CreateCholesky(GpuSolverContext* context,
   absl::c_iota(batch_dim_ids, 0);
   int64_t batch_size = absl::c_accumulate(batch_dims, 1, std::multiplies<>{});
 
+#if !TENSORFLOW_USE_SYCL
   // Find the workspace size.
   se::blas::UpperLower uplo = options.lower() ? se::blas::UpperLower::kLower
                                               : se::blas::UpperLower::kUpper;
@@ -77,7 +78,36 @@ absl::StatusOr<HloInstruction*> CreateCholesky(GpuSolverContext* context,
   TF_ASSIGN_OR_RETURN(
       workspace_size,
       context->PotrfBufferSize(a_shape.element_type(), uplo, n, n, batch_size));
-
+#else
+  // Find the workspace size.
+  int64_t workspace_size = 0;
+  oneapi::mkl::uplo uplo =
+      options.lower() ? oneapi::mkl::uplo::L : oneapi::mkl::uplo::U;
+  sycl::property_list propList{sycl::property::queue::in_order()};
+  sycl::queue queue(sycl::gpu_selector{}, propList);
+  switch (a_shape.element_type()) {
+    case F32:
+      workspace_size = oneapi::mkl::lapack::potrf_batch_scratchpad_size<float>(
+          queue, uplo, n, n, n * n, batch_size);
+      break;
+    case F64:
+      workspace_size = oneapi::mkl::lapack::potrf_batch_scratchpad_size<double>(
+          queue, uplo, n, n, n * n, batch_size);
+      break;
+    case C64:
+      workspace_size =
+          oneapi::mkl::lapack::potrf_batch_scratchpad_size<std::complex<float>>(
+              queue, uplo, n, n, n * n, batch_size);
+      break;
+    case C128:
+      workspace_size = oneapi::mkl::lapack::potrf_batch_scratchpad_size<
+          std::complex<double>>(queue, uplo, n, n, n * n, batch_size);
+      break;
+    default:
+      return InvalidArgument("Invalid type for cholesky %s",
+                             PrimitiveType_Name(a_shape.element_type()));
+  }
+#endif
   // TODO(phawkins): Ideally we would relax this constraint. What we actually
   // want is that:
   // a) the batch dimensions are major, in no particular order.
@@ -104,6 +134,9 @@ absl::StatusOr<HloInstruction*> CreateCholesky(GpuSolverContext* context,
   TF_RETURN_IF_ERROR(custom_call->set_backend_config(options));
   HloInstruction* out = computation->AddInstruction(
       HloInstruction::CreateGetTupleElement(a_shape, custom_call, 0));
+#if TENSORFLOW_USE_SYCL
+  return out;
+#else
   HloInstruction* info = computation->AddInstruction(
       HloInstruction::CreateGetTupleElement(info_shape, custom_call, 2));
 
@@ -133,6 +166,7 @@ absl::StatusOr<HloInstruction*> CreateCholesky(GpuSolverContext* context,
       computation->AddInstruction(HloInstruction::CreateTernary(
           a_shape, HloOpcode::kSelect, ok, out, nans));
   return select;
+#endif
 }
 
 // Tries to rewrite a single convolution into a call to cudnn.
diff --git a/xla/service/gpu/elemental_ir_emitter.cc b/xla/service/gpu/elemental_ir_emitter.cc
index 60422d2967..00c8bcee68 100644
--- a/xla/service/gpu/elemental_ir_emitter.cc
+++ b/xla/service/gpu/elemental_ir_emitter.cc
@@ -374,6 +374,7 @@ llvm::Value* GpuElementalIrEmitter::EmitThreadId() {
 
 absl::StatusOr<llvm::Value*> GpuElementalIrEmitter::EmitF32ToBF16(
     llvm::Value* f32_value) {
+#if 0
   // sm_80 and up has an instruction to convert f32 into bf16.
   if (ir_emitter_context_.cuda_compute_capability().IsAtLeast(
           se::CudaComputeCapability::AMPERE)) {
@@ -381,6 +382,7 @@ absl::StatusOr<llvm::Value*> GpuElementalIrEmitter::EmitF32ToBF16(
         FPTrunc(BitCast(f32_value, b()->getFloatTy()), b()->getBFloatTy()),
         b()->getInt16Ty());
   }
+#endif
   return ElementalIrEmitter::EmitF32ToBF16(f32_value);
 }
 
diff --git a/xla/service/gpu/fusions/copy.cc b/xla/service/gpu/fusions/copy.cc
index eeb934c820..ce405ab112 100644
--- a/xla/service/gpu/fusions/copy.cc
+++ b/xla/service/gpu/fusions/copy.cc
@@ -24,6 +24,10 @@ limitations under the License.
 #include "xla/service/gpu/thunk.h"
 #include "xla/statusor.h"
 
+#if TENSORFLOW_USE_SYCL
+#include "xla/service/gpu/ir_emission_utils.h"
+#endif
+
 namespace xla {
 namespace gpu {
 
diff --git a/xla/service/gpu/fusions/fusion_emitter.cc b/xla/service/gpu/fusions/fusion_emitter.cc
index f4ddb497d9..ad95a19215 100644
--- a/xla/service/gpu/fusions/fusion_emitter.cc
+++ b/xla/service/gpu/fusions/fusion_emitter.cc
@@ -61,6 +61,10 @@ limitations under the License.
 #include "tsl/platform/errors.h"
 #include "tsl/platform/statusor.h"
 
+#if TENSORFLOW_USE_SYCL
+#include "xla/service/gpu/ir_emission_utils.h"
+#endif
+
 namespace xla {
 namespace gpu {
 namespace {
@@ -217,9 +221,13 @@ BuildKernelPrototype(IrEmitterContext& ir_emitter_context,
   // Create the kernel and add it to the module.
   auto* llvm_module = ir_emitter_context.llvm_module();
   llvm::LLVMContext& context = llvm_module->getContext();
+
+  // SYCL: Hardcode to global device addrspace.
+  llvm::Type* arg_type =
+      IsSPIR(llvm_module)? builder->getPtrTy(1) : builder->getPtrTy();
   llvm::FunctionType* kernel_type = llvm::FunctionType::get(
       /*Result=*/llvm::Type::getVoidTy(context),
-      std::vector<llvm::Type*>(kNumLlvmArgs, builder->getPtrTy()),
+      std::vector<llvm::Type*>(kNumLlvmArgs, arg_type),
       /*isVarArg=*/false);
   llvm::Function* kernel =
       llvm::Function::Create(kernel_type, llvm::GlobalValue::ExternalLinkage,
@@ -230,6 +238,17 @@ BuildKernelPrototype(IrEmitterContext& ir_emitter_context,
       ir_emitter_context.gpu_device_info(), launch_dimensions, kernel_name,
       llvm_module));
 
+  // SYCL: Set function metadata
+  if (IsSPIR(llvm_module)) {
+    llvm::LLVMContext& context = llvm_module->getContext();
+    llvm::IntegerType* i32 = llvm::Type::getInt32Ty(context);
+    kernel->setMetadata(
+        "intel_reqd_sub_group_size",
+        llvm::MDNode::get(context,
+                          {llvm::ConstantAsMetadata::get(
+                              llvm::ConstantInt::get(i32, WarpSize()))}));
+  }
+
   // TODO(b/65380986): Investigate if adding fast math flags for generated
   // kernels makes sense.
 
diff --git a/xla/service/gpu/fusions/reduction.cc b/xla/service/gpu/fusions/reduction.cc
index 55519f58e3..b4f2b7fab6 100644
--- a/xla/service/gpu/fusions/reduction.cc
+++ b/xla/service/gpu/fusions/reduction.cc
@@ -1329,7 +1329,8 @@ ReductionFusion::ComputeReductionCodegenInfo(
   // parallelizing the z dimension (major reduced dimensions). The general
   // recommendation is to use between 128 and 512 threads, so we just go for
   // 256. See https://forums.developer.nvidia.com/t/55529
-  constexpr int64_t kThreadsPerBlockTarget = 256;
+  // SYCL: Use 32 as WA for intel platform to avoid reduce hang.
+  constexpr int64_t kThreadsPerBlockTarget = 32;
   if (reduction_dimensions.is_row_reduction &&
       num_threads_x * 2 <= kThreadsPerBlockTarget) {
     int64_t kept_size = reduction_dimensions.dimensions[kRowKeptDimension];
diff --git a/xla/service/gpu/gemm_rewriter.cc b/xla/service/gpu/gemm_rewriter.cc
index 8a3919a758..0e03d5a382 100644
--- a/xla/service/gpu/gemm_rewriter.cc
+++ b/xla/service/gpu/gemm_rewriter.cc
@@ -566,6 +566,7 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {
 
   absl::Status HandleMultiply(HloInstruction *instr) override {
     HloInstruction *alpha, *existing_gemm;
+#if !TENSORFLOW_USE_SYCL
     if (Match(instr,
               m::MultiplyAnyOrder(
                   GemmOrCublasLtMatmulMaybeF8(&existing_gemm).WithOneUser(),
@@ -589,11 +590,13 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {
         return ReplaceInstruction(instr, existing_gemm);
       }
     }
+#endif  // !TENSORFLOW_USE_SYCL
 
     // Attempt to match approximate GELU activation
     // (https://arxiv.org/abs/1606.08415), where:
     // approx_gelu(x) = x * cdf(x)
     // cdf(x) = 0.5 * (1 + tanh(sqrt(2 / pi) * (x + 0.044715 * x**3))
+    // SYCL: OptionalBitcast?
     HloInstruction *cdf, *slice_or_bitcast = nullptr;
     if (Match(instr, m::MultiplyAnyOrder(
                          m::AnyOf<HloInstruction>(
@@ -633,6 +636,14 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {
                                   .WithOneUser())
                               .WithOneUser())
                           .WithOneUser())))) {
+      // SYCL
+      // gemm - bitcast - gelu - bitcast
+      if (instr->user_count() == 1) {
+        auto bitcast = instr->users()[0];
+        if (bitcast->opcode() == HloOpcode::kBitcast &&
+            ShapeUtil::Compatible(bitcast->shape(), existing_gemm->shape()))
+          return FuseGeluActivation(bitcast, existing_gemm);
+      }
       return FuseGeluActivation(instr, existing_gemm, slice_or_bitcast);
     }
     return absl::OkStatus();
@@ -1277,8 +1288,8 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {
       return in_out_alias_config.ParameterHasAlias(bias->parameter_number(),
                                                    /*param_index=*/{});
     }();
-    bool want_to_fuse_bias = IsCublasLtMatmulF8(*gemm) ||
-                             IsCublasLtMatmul(*gemm) || can_overwrite_bias;
+    // SYCL: cannot always fuse bias.
+    bool want_to_fuse_bias = can_overwrite_bias;
 
     auto gpu_config = gemm->backend_config<GpuBackendConfig>().value();
     GemmBackendConfig &config = *gpu_config.mutable_gemm_backend_config();
@@ -1530,6 +1541,7 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {
 
     // There are four users of the gemm output within the GELU calculation.
     bool has_aux = gemm->user_count() > 4;
+    if (has_aux) return OkStatus();
 
     TF_ASSIGN_OR_RETURN(auto gpu_config,
                         gemm->backend_config<GpuBackendConfig>());
@@ -1576,13 +1588,14 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {
   absl::StatusOr<absl::string_view> GetNonFp8GemmCustomCallTarget(
       const HloInstruction &instr,
       const GemmBackendConfig &gemm_backend_config) const {
-    if (!instr.GetModule()
-             ->config()
-             .debug_options()
-             .xla_gpu_enable_cublaslt()) {
-      // cublasLt is not enabled.
-      return absl::string_view(kGemmCallTarget);
-    }
+    // SYCL: disable fallback
+    // if (!instr.GetModule()
+    //          ->config()
+    //          .debug_options()
+    //          .xla_gpu_enable_cublaslt()) {
+    //   // cublasLt is not enabled.
+    //   return absl::string_view(kGemmCallTarget);
+    // }
 
     // cublasLt is enabled, check if other internal conditions are met.
     const HloInstruction *lhs = instr.operand(0);
@@ -1879,11 +1892,6 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {
     for (auto batch_dimension : batch_dimensions) {
       batch_count *= lhs->shape().dimensions(batch_dimension);
     }
-    if (batch_count > kMaxBatchCount) {
-      // This is not supported by cublasLt.
-      return false;
-    }
-
     TF_ASSIGN_OR_RETURN(bool output_is_column_major,
                         MatrixIsColumnMajor(instr, gemm_backend_config));
 
diff --git a/xla/service/gpu/gpu_compiler.cc b/xla/service/gpu/gpu_compiler.cc
index 43c390e845..fa5a3fe1ff 100644
--- a/xla/service/gpu/gpu_compiler.cc
+++ b/xla/service/gpu/gpu_compiler.cc
@@ -71,12 +71,12 @@ limitations under the License.
 #include "xla/hlo/ir/hlo_opcode.h"
 #include "xla/hlo/ir/hlo_schedule.h"
 #include "xla/hlo/transforms/hlo_constant_splitter.h"
-#include "xla/mlir/backends/gpu/transforms/passes.h"
-#include "xla/mlir/runtime/transforms/compilation_pipeline_gpu.h"
-#include "xla/mlir/runtime/transforms/compilation_pipeline_options.h"
-#include "xla/runtime/compiler.h"
-#include "xla/runtime/executable.h"
-#include "xla/runtime/jit_executable.h"
+// #include "xla/mlir/backends/gpu/transforms/passes.h"
+// #include "xla/mlir/runtime/transforms/compilation_pipeline_gpu.h"
+// #include "xla/mlir/runtime/transforms/compilation_pipeline_options.h"
+// #include "xla/runtime/compiler.h"
+// #include "xla/runtime/executable.h"
+// #include "xla/runtime/jit_executable.h"
 #include "xla/service/algebraic_simplifier.h"
 #include "xla/service/all_gather_broadcast_reorder.h"
 #include "xla/service/all_gather_combiner.h"
@@ -166,7 +166,7 @@ limitations under the License.
 #include "xla/service/gpu/reduction_splitter.h"
 #include "xla/service/gpu/reduction_utils.h"
 #include "xla/service/gpu/rename_fusions.h"
-#include "xla/service/gpu/runtime/executable.h"
+// #include "xla/service/gpu/runtime/executable.h"
 #include "xla/service/gpu/runtime_intrinsics.h"
 #include "xla/service/gpu/scatter_slice_simplifier.h"
 #include "xla/service/gpu/softmax_rewriter_triton.h"
@@ -265,6 +265,8 @@ limitations under the License.
 #include "xla/hlo/experimental/auto_sharding/auto_sharding.h"
 #endif  // PLATFORM_GOOGLE
 
+#include "xla/service/gpu/dot_expand_dims.h"
+
 namespace xla {
 namespace gpu {
 namespace {
@@ -474,6 +476,7 @@ class GpuThunkAotCompilationResult : public AotCompilationResult {
 absl::StatusOr<std::unique_ptr<Executable>>
 GpuAotCompilationResult::LoadExecutable(
     Compiler* compiler, const se::StreamExecutor* executor) const {
+#if 0
   XlaRuntimeExecutableProto xla_runtime_executable =
       xla_runtime_gpu_executable_.xla_runtime_executable();
   TF_ASSIGN_OR_RETURN(HloModuleConfig hlo_module_config,
@@ -500,6 +503,7 @@ GpuAotCompilationResult::LoadExecutable(
       xla_runtime_gpu_executable_.gpu_asm_text(),
       xla_runtime_gpu_executable_.gpu_binary(), std::move(constants),
       GetGpuVersion(executor));
+#endif
 }
 
 absl::StatusOr<std::unique_ptr<Executable>>
@@ -680,7 +684,9 @@ absl::Status GpuCompiler::OptimizeHloModule(
   layout_insensitive_algsimp_opts
       .set_unconditionally_simplify_reduce_of_transpose_or_reshape(true);
 
-  if (gpu_target_config.platform_name == "ROCM") {
+  // SYCL: Conv swap has accuracy issue in some cases.
+  if (gpu_target_config.platform_name == "ROCM" ||
+      gpu_target_config.platform_name == "SYCL") {
     layout_insensitive_algsimp_opts.set_enable_conv_operand_swap(false);
   }
   layout_insensitive_algsimp_opts
@@ -1064,6 +1070,8 @@ absl::Status GpuCompiler::OptimizeHloModule(
   se::GpuComputeCapability gpu_version =
       gpu_target_config.device_description.gpu_compute_capability();
   se::dnn::VersionInfo dnn_version = gpu_target_config.dnn_version_info;
+  // SYCL: do not check dnn version
+#if 0
   if (stream_exec != nullptr) {
     gpu_version = GetGpuVersion(stream_exec);
     se::dnn::DnnSupport* dnn = stream_exec->AsDnn();
@@ -1074,7 +1082,7 @@ absl::Status GpuCompiler::OptimizeHloModule(
     }
     TF_ASSIGN_OR_RETURN(dnn_version, dnn->GetVersion());
   }
-
+#endif
   TF_RETURN_IF_ERROR(OptimizeHloConvolutionCanonicalization(
       hlo_module, gpu_version, dnn_version, options.device_allocator));
 
@@ -1088,6 +1096,10 @@ absl::Status GpuCompiler::OptimizeHloModule(
     HloPassPipeline pipeline("layout assignment");
     // Layout assignment uses alias analysis, which requires the call graph to
     // be flattened.
+    // SYCL: LLM passes.
+    bool llm_flag = false;
+    tsl::ReadBoolFromEnvVar("LLM", false, &llm_flag);
+    if (llm_flag) pipeline.AddPass<DotExpandDims>();
     pipeline.AddPass<FlattenCallGraph>();
     ChannelLayoutConstraints layout_constraints;
     pipeline.AddPass<GpuLayoutAssignment>(
@@ -1388,7 +1400,8 @@ absl::Status GpuCompiler::OptimizeHloPostLayoutAssignment(
     const auto* cuda_cc = std::get_if<se::CudaComputeCapability>(&gpu_version);
     if (debug_options.xla_gpu_enable_triton_gemm() && cuda_cc != nullptr &&
         cuda_cc->IsAtLeast(se::CudaComputeCapability::VOLTA)) {
-      pipeline.AddPass<GemmRewriterTriton>(gpu_version);
+      // SYCL: disable triton gemm
+      // pipeline.AddPass<GemmRewriterTriton>(gpu_version);
     }
     pipeline.AddPass<GemmRewriter>(gpu_version);
 
@@ -1410,8 +1423,9 @@ absl::Status GpuCompiler::OptimizeHloPostLayoutAssignment(
     if (debug_options.xla_gpu_enable_triton_softmax_fusion() &&
         cuda_cc != nullptr &&
         cuda_cc->IsAtLeast(se::CudaComputeCapability::VOLTA)) {
-      pipeline.AddPass<HloPassFix<AlgebraicSimplifier>>(simplifier_options);
-      pipeline.AddPass<SoftmaxRewriterTriton>(gpu_version);
+      // SYCL: disable triton
+      // pipeline.AddPass<HloPassFix<AlgebraicSimplifier>>(simplifier_options);
+      // pipeline.AddPass<SoftmaxRewriterTriton>(gpu_version);
     }
 
     pipeline.AddPass<ReductionDimensionGrouper>();
@@ -1742,6 +1756,11 @@ GpuCompiler::CompileSingleModule(const HloModuleConfig& module_config,
 
   // Write PTX to IR dump directory, if IR dumping was requested.
   if (should_dump) {
+    // SYCL: dump spv
+    auto spir_vector = result.binary;
+    std::string spir(spir_vector.begin(), spir_vector.end());
+    DumpToFileInDirOrStdout(*debug_module, "", "spv", spir);
+
     absl::string_view ptx = result.asm_text;
     if (debug_module) {
       DumpToFileInDirOrStdout(*debug_module, "",
@@ -2055,6 +2074,7 @@ absl::StatusOr<std::unique_ptr<Executable>> GpuCompiler::RunBackend(
 absl::StatusOr<std::vector<std::unique_ptr<AotCompilationResult>>>
 GpuCompiler::CompileAheadOfTime(std::unique_ptr<HloModuleGroup> module_group,
                                 const AotCompilationOptions& options) {
+#if 0
 #if GOOGLE_CUDA
   CHECK(options.PlatformId() == se::cuda::kCudaPlatformId);
 #elif TENSORFLOW_USE_ROCM
@@ -2146,6 +2166,7 @@ GpuCompiler::CompileAheadOfTime(std::unique_ptr<HloModuleGroup> module_group,
         res.compile_module_results.constants));
   }
   return std::move(results);
+#endif
 }
 
 HloCostAnalysis::ShapeSizeFunction GpuCompiler::ShapeSizeBytesFunction() const {
@@ -2157,6 +2178,7 @@ HloCostAnalysis::ShapeSizeFunction GpuCompiler::ShapeSizeBytesFunction() const {
 
 absl::StatusOr<std::unique_ptr<AotCompilationResult>> GpuCompiler::Export(
     Executable* executable) const {
+#if 0
   auto* gpu_executable = tensorflow::down_cast<GpuExecutable*>(executable);
   if (!gpu_executable) return Internal("GpuExecutable is null");
 
@@ -2168,6 +2190,8 @@ absl::StatusOr<std::unique_ptr<AotCompilationResult>> GpuCompiler::Export(
   return GpuThunkAotCompilationResult::FromModule(
       &gpu_executable->module(), gpu_executable->buffer_assignment(),
       gpu_executable->text(), gpu_executable->binary());
+#endif
+  LOG(FATAL) << "GpuCompiler::Export is not implemented";
 }
 
 absl::Status GpuCompiler::RunPostSchedulingPipelines(
@@ -2233,12 +2257,16 @@ absl::Status GpuCompiler::RunPostSchedulingPipelines(
     auto driver_version = se::gpu::GpuDriver::GetDriverVersion();
 #if GOOGLE_CUDA
     constexpr int toolkit_version = CUDA_VERSION;
-#else
+#elif TENSORFLOW_USE_ROCM
     constexpr int toolkit_version = TF_ROCM_VERSION;
+#else
+    constexpr int toolkit_version = -1;
 #endif
+#if 0
     pipeline.AddPass<CommandBufferScheduling>(
         gpu_device_info.gpu_compute_capability(), toolkit_version,
         driver_version.value_or(toolkit_version));
+#endif
     TF_RETURN_IF_ERROR(pipeline.Run(module).status());
   }
 
diff --git a/xla/service/gpu/gpu_conv_rewriter.cc b/xla/service/gpu/gpu_conv_rewriter.cc
index 697b69306f..972c6117a9 100644
--- a/xla/service/gpu/gpu_conv_rewriter.cc
+++ b/xla/service/gpu/gpu_conv_rewriter.cc
@@ -43,6 +43,30 @@ namespace {
 using ConvolutionMatch = std::optional<
     std::tuple<Window, ConvolutionDimensionNumbers, HloInstruction*>>;
 
+// Determine whether conv2d is equal to conv1d.
+bool MaybeConv1dToConv2d(HloInstruction* conv) {
+  if (conv->window().dimensions().size() != 2) {
+    return false;
+  }
+  if (conv->operand(1)->opcode() != HloOpcode::kReshape) {
+    return false;
+  }
+  auto filter = conv->operand(1);
+  std::optional<ShapeUtil::ShapeEqualityDescriptor> reshape_degenerate =
+      filter->ReshapeMerelyInsertsOrDeletes1SizedDimensions();
+  if (reshape_degenerate.has_value() &&
+      reshape_degenerate->deleted_dimensions.empty() &&
+      reshape_degenerate->inserted_dimensions.size() == 1) {
+    auto dnums = conv->convolution_dimension_numbers();
+    for (auto dim : dnums.kernel_spatial_dimensions()) {
+      if (dim == reshape_degenerate->inserted_dimensions[0]) {
+        return true;
+      }
+    }
+  }
+  return false;
+}
+
 bool CanImplementAsGpuForwardConv(HloInstruction* conv) {
   const ConvolutionDimensionNumbers& dnums =
       conv->convolution_dimension_numbers();
@@ -313,10 +337,18 @@ ConvolutionMatch MatchBackwardInput(HloInstruction* conv) {
       reverse_filter->opcode() == HloOpcode::kReverse &&
       absl::c_is_permutation(dnums.kernel_spatial_dimensions(),
                              reverse_filter->dimensions());
+  // For conv1d which reshape to conv2d, filter reverse pattern is
+  // reshape(reverse(filter)). It seems we can reuse conv2d backward input
+  // pattern matcher, but after algsimp pass, this pattern will change to
+  // reverse(reshape(filter)) and fail to match. So matching conv1d backward
+  // input need different processing logic.
+  bool is_reversed_conv1d_filter =
+      MaybeConv1dToConv2d(conv) &&
+      reverse_filter->operand(0)->opcode() == HloOpcode::kReverse;
   bool is_1x1_filter =
       absl::c_all_of(conv->window().dimensions(),
                      [](const WindowDimension& d) { return d.size() == 1; });
-  if (!is_reversed_filter &&
+  if (!is_reversed_filter && !is_reversed_conv1d_filter &&
       !(window_util::HasBaseDilation(conv->window()) &&
         (reverse_filter->IsConstant() || is_1x1_filter))) {
     VLOG(1) << "Can't match to backwards convolution. Either filter is not "
@@ -488,6 +520,10 @@ ConvolutionMatch MatchBackwardInput(HloInstruction* conv) {
   // One reverse is subsumed by the cuDNN call.
   if (rhs->opcode() == HloOpcode::kReverse) {
     rhs = rhs->mutable_operand(0);
+  } else if (is_reversed_conv1d_filter) {
+    auto src = rhs->mutable_operand(0)->mutable_operand(0);
+    rhs = conv->parent()->AddInstruction(
+        HloInstruction::CreateReshape(rhs->shape(), src));
   }
   if (conv->feature_group_count() == 1) {
     return std::make_tuple(new_window, dnums, rhs);
diff --git a/xla/service/gpu/gpu_executable.cc b/xla/service/gpu/gpu_executable.cc
index 70052ceeea..380364fa1e 100644
--- a/xla/service/gpu/gpu_executable.cc
+++ b/xla/service/gpu/gpu_executable.cc
@@ -41,18 +41,19 @@ limitations under the License.
 #include "xla/executable_run_options.h"
 #include "xla/hlo/ir/hlo_instruction.h"
 #include "xla/map_util.h"
-#include "xla/mlir/runtime/ir/rt_ops.h"
-#include "xla/mlir/runtime/transforms/compilation_pipeline_gpu.h"
-#include "xla/mlir/runtime/transforms/type_converter.h"
-#include "xla/runtime/executable.h"
+// #include "xla/mlir/runtime/ir/rt_ops.h"
+// #include "xla/mlir/runtime/transforms/compilation_pipeline_gpu.h"
+// #include "xla/mlir/runtime/transforms/type_converter.h"
+// #include "xla/runtime/executable.h"
 #include "xla/service/buffer_assignment.h"
+#include "xla/service/gpu/backend_configs.pb.h"
 #include "xla/service/gpu/buffer_allocations.h"
 #include "xla/service/gpu/gpu_constants.h"
 #include "xla/service/gpu/gpu_executable_run_options.h"
 #include "xla/service/gpu/nccl_clique.h"
 #include "xla/service/gpu/nccl_clique_key.h"
 #include "xla/service/gpu/non_atomically_upgradeable_rw_lock.h"
-#include "xla/service/gpu/runtime/executable.h"
+// #include "xla/service/gpu/runtime/executable.h"
 #include "xla/service/gpu/runtime/tracing.h"
 #include "xla/service/gpu/stream_executor_util.h"
 #include "xla/service/gpu/thunk.h"
@@ -137,7 +138,7 @@ absl::StatusOr<std::unique_ptr<GpuExecutable>> GpuExecutable::Create(
     result->thunks_ = std::move(std::get<OwnedThunkSequence>(executable));
     return result;
   }
-
+#if !TENSORFLOW_USE_SYCL
   if (std::holds_alternative<OwnedGpuRuntimeProgram>(executable)) {
     auto& program = std::get<OwnedGpuRuntimeProgram>(executable);
     TF_ASSIGN_OR_RETURN(
@@ -145,6 +146,7 @@ absl::StatusOr<std::unique_ptr<GpuExecutable>> GpuExecutable::Create(
         GpuRuntimeExecutable::Create(result->module_name_, std::move(program)));
     return result;
   }
+#endif
 
   return Internal("No XLA gpu executable was provided");
 }
@@ -207,7 +209,9 @@ absl::Status GpuExecutable::CheckCompatibilityWithServiceExecutableRunOptions(
         << "}, but was {" << std::get<se::CudaComputeCapability>(cc).ToString()
         << "}";
   } else {
+#if !TENSORFLOW_USE_SYCL
     return Internal("Unknown platform");
+#endif
   }
 
   return absl::OkStatus();
@@ -335,6 +339,9 @@ absl::Status ExecuteThunks(
   se::StreamExecutor* executor = main_stream->parent();
   stream_executor::StreamPriority stream_priority =
       stream_executor::StreamPriority::Default;
+#if TENSORFLOW_USE_SYCL
+  use_highest_priority_for_async_stream = false;
+#endif
   if (use_highest_priority_for_async_stream) {
     stream_priority = stream_executor::StreamPriority::Highest;
   }
@@ -589,10 +596,16 @@ GpuExecutable::ResolveConstantGlobals(se::Stream* stream) {
   // The CUDA driver isn't able to load a PTX and a binary which are both empty.
   // It's okay if we skip loading in this case; if the module isn't loaded, all
   // symbol lookups will fail, just as they should for an empty module.
+#if !TENSORFLOW_USE_SYCL
   if (!(executor->platform()->id() == stream_executor::cuda::kCudaPlatformId &&
         binary().empty() && text().empty())) {
     TF_RETURN_IF_ERROR(executor->LoadModule(module_spec, &module_handle));
   }
+#else
+  if (module_spec.has_cuda_cubin_in_memory()) {
+    TF_RETURN_IF_ERROR(executor->LoadModule(module_spec, &module_handle));
+  }
+#endif
 
   // A flag signalling if constant initialization submitted memcpy operations
   // to the `stream`.
@@ -621,6 +634,26 @@ GpuExecutable::ResolveConstantGlobals(se::Stream* stream) {
         submitted_mem_copies = true;
       }
     } else {
+#if TENSORFLOW_USE_SYCL
+      // SYCL: content may be empty
+      if (info.content.span().empty()) {
+        // LLVM module contains the const variable, but it still fails to look
+        // up symbol. So allocate an empty buffer here.
+        void* opaque = nullptr;
+        size_t bytes = 0;
+        global = se::DeviceMemoryBase(opaque, bytes);
+      } else {
+        TF_ASSIGN_OR_RETURN(
+            auto shared, executor->CreateOrShareConstant(stream, info.content.span()));
+        global = *shared;
+        VLOG(3) << "Allocated (or shared) global " << info.symbol_name << " at "
+                << global.opaque();
+        // XLA will continue to own this global at least until this executable
+        // is destroyed (longer if another, longer-lived executable shares the
+        // same constant).
+        shared_constants_.push_back(std::move(shared));
+      }
+#else
       // The constant was not defined in the PTX and therefore must be both
       // allocated and initialized by XLA here.
       CHECK(!info.content.span().empty());
@@ -634,6 +667,7 @@ GpuExecutable::ResolveConstantGlobals(se::Stream* stream) {
       // destroyed (longer if another, longer-lived executable shares the same
       // constant).
       shared_constants_.push_back(std::move(shared));
+#endif
     }
 
     if (info.allocation_index != -1) {
@@ -771,6 +805,7 @@ absl::StatusOr<ScopedShapedBuffer> GpuExecutable::ExecuteAsyncOnStream(
   return out.ConsumeResult();
 }
 
+#if !TENSORFLOW_USE_SYCL
 static absl::Status ExecuteXlaRuntime(
     const std::string& module_name, ModuleIdentifier module_id,
     GpuRuntimeExecutable& gpu_runtime_executable,
@@ -801,6 +836,7 @@ static absl::Status ExecuteXlaRuntime(
       run_options, std::move(execution_timer),
       block_host_until_done ? run_options->stream() : nullptr);
 }
+#endif
 
 absl::StatusOr<ExecutionOutput> GpuExecutable::ExecuteAsyncOnStreamImpl(
     const ServiceExecutableRunOptions* run_options,
@@ -1022,13 +1058,13 @@ absl::Status GpuExecutable::ExecuteThunksOrXlaRuntime(
       if (temp_buffer == nullptr) temp_buffer = &alloc;
     }
   }
-
+#if !TENSORFLOW_USE_SYCL
   if (gpu_runtime_executable_) {
     return ExecuteXlaRuntime(module_name_, unique_id, *gpu_runtime_executable_,
                              run_options, text_, binary_, buffer_allocations,
                              temp_buffer, block_host_until_done, gpu_lock);
   }
-
+#endif
   return FailedPrecondition("Expected XLA gpu executable is not supplied.");
 }
 
@@ -1163,7 +1199,7 @@ GetOutputInfo(const HloModule& hlo_module, const BufferAssignment& assignment) {
       }));
   return output;
 }
-
+#if !TENSORFLOW_USE_SYCL
 GpuExecutable::GpuExecutable(
     std::shared_ptr<HloModule> hlo_module, std::string asm_text,
     std::vector<uint8_t> binary, std::vector<ConstantInfo> constants,
@@ -1368,6 +1404,6 @@ absl::StatusOr<std::string_view> GpuExecutable::GetMlirModule() const {
     return Internal("gpu_runtime_executable is null");
   return gpu_runtime_executable_->GetMlirModule();
 }
-
+#endif
 }  // namespace gpu
 }  // namespace xla
diff --git a/xla/service/gpu/gpu_executable.h b/xla/service/gpu/gpu_executable.h
index 5a4ce3f0e5..9386f11a8f 100644
--- a/xla/service/gpu/gpu_executable.h
+++ b/xla/service/gpu/gpu_executable.h
@@ -40,7 +40,7 @@ limitations under the License.
 #include "xla/service/gpu/ir_emission_utils.h"
 #include "xla/service/gpu/non_atomically_upgradeable_rw_lock.h"
 #include "xla/service/gpu/runtime/annotation.h"
-#include "xla/service/gpu/runtime/executable.h"
+// #include "xla/service/gpu/runtime/executable.h"
 #include "xla/service/gpu/thunk.h"
 #include "xla/service/hlo_execution_profile.h"
 #include "xla/service/rendezvous.h"
@@ -56,6 +56,25 @@ namespace gpu {
 // Returns whether GpuExecutable runs with Xla Runtime.
 bool IsXlaRuntimeExecutableEnabled(const HloModuleConfig& config);
 
+// SYCL: dummy GpuRuntimeProgram for compilation
+#if TENSORFLOW_USE_SYCL
+struct GpuRuntimeProgram {
+  GpuRuntimeProgram(std::string entry_point, std::string module,
+                    std::vector<int64_t> buffer_sizes,
+                    DebugOptions debug_options)
+      : entry_point(std::move(entry_point)),
+        module(std::move(module)),
+        buffer_sizes(std::move(buffer_sizes)),
+        debug_options(std::move(debug_options)) {}
+
+  std::string entry_point;
+  std::string module;
+  std::vector<int64_t> buffer_sizes;
+  DebugOptions debug_options;
+};
+class GpuRuntimeExecutable {};
+#endif
+
 // GPU-targeting implementation of the XLA Executable interface.
 //
 // Launches the given GPU kernel via the StreamExecutor.
@@ -113,7 +132,7 @@ class GpuExecutable : public Executable {
       std::vector<BufferAllocation>* allocations,
       absl::flat_hash_map<ShapeIndex, OutputInfo>* output_info,
       Shape* output_shape);
-
+#if !TENSORFLOW_USE_SYCL
   // Returns an Executable that is loaded from an object file (XLA program
   // compiled to a native function using the XLA Runtime stack).
   static absl::StatusOr<std::unique_ptr<Executable>> LoadFromObjFile(
@@ -122,7 +141,7 @@ class GpuExecutable : public Executable {
       absl::string_view asm_text, absl::string_view binary,
       std::vector<ConstantInfo> constants,
       se::GpuComputeCapability gpu_version);
-
+#endif
   // Constructor to use when loading a GpuExecutable from an object file (native
   // function compiled for XLA Runtime). Omits setting class members that aren't
   // used in XLA Runtime execution mode.
diff --git a/xla/service/gpu/gpu_fused_mha_runner.cc b/xla/service/gpu/gpu_fused_mha_runner.cc
index 889d2e5bcb..a59e092d24 100644
--- a/xla/service/gpu/gpu_fused_mha_runner.cc
+++ b/xla/service/gpu/gpu_fused_mha_runner.cc
@@ -155,6 +155,8 @@ void AssignScale(GpufMHAConfig &config,
   double fmha_scale = 0.0;
 
   switch (config.kind) {
+    // SYCL: supports bias + softmax
+    case CudnnfMHAKind::kSoftmax:
     case CudnnfMHAKind::kScaleBiasMaskSoftmax:
     case CudnnfMHAKind::kScaleBiasMaskSoftmaxDropout:
     case CudnnfMHAKind::kScaleMaskSoftmax:
diff --git a/xla/service/gpu/gpu_fusible.cc b/xla/service/gpu/gpu_fusible.cc
index 2b880f0ab5..fefde65203 100644
--- a/xla/service/gpu/gpu_fusible.cc
+++ b/xla/service/gpu/gpu_fusible.cc
@@ -440,6 +440,13 @@ FusionDecision IsProducerConsumerFusible(const HloInstruction& producer,
   }
 
   if (IsInputFusibleReduction(producer)) {
+#if TENSORFLOW_USE_SYCL
+    // TODO: Check on latest XLA. Reduction epilogue fusion may cause
+    // regression for row reductions cases with large dim.
+    // It changes fusion kind from kInput to kLoop, and
+    // will fail to enter the row vectorization pass.
+    return "Reduction epilogue fusion is not enabled.";
+#endif
     if (!producer.GetModule()
              ->config()
              .debug_options()
diff --git a/xla/service/gpu/gpu_layout_assignment.cc b/xla/service/gpu/gpu_layout_assignment.cc
index c69d218c68..a0c242f94a 100644
--- a/xla/service/gpu/gpu_layout_assignment.cc
+++ b/xla/service/gpu/gpu_layout_assignment.cc
@@ -86,7 +86,7 @@ HeuristicLayoutAssignment(const HloInstruction* instr,
       std::make_tuple(DataLayout::kBatchDepthYX4, FilterLayout::kOutputInputYX4,
                       DataLayout::kBatchDepthYX4);
   constexpr auto kAllNHWC =
-      std::make_tuple(DataLayout::kBatchYXDepth, FilterLayout::kOutputYXInput,
+      std::make_tuple(DataLayout::kBatchYXDepth, FilterLayout::kYXInputOutput,
                       DataLayout::kBatchYXDepth);
 
   // Integer convolution must use NHWC or NCHW_VECT_C.
diff --git a/xla/service/gpu/gpu_transfer_manager.cc b/xla/service/gpu/gpu_transfer_manager.cc
index 4ef4ece8dc..1ca8d7352b 100644
--- a/xla/service/gpu/gpu_transfer_manager.cc
+++ b/xla/service/gpu/gpu_transfer_manager.cc
@@ -36,6 +36,7 @@ limitations under the License.
 #include "xla/stream_executor/host/host_platform_id.h"
 #include "xla/stream_executor/multi_platform_manager.h"
 #include "xla/stream_executor/rocm/rocm_platform_id.h"
+#include "xla/stream_executor/sycl/sycl_platform_id.h"
 #include "xla/stream_executor/stream_executor.h"
 #include "xla/types.h"
 #include "xla/util.h"
@@ -208,11 +209,20 @@ static std::unique_ptr<xla::TransferManager> CreateAMDGPUTransferManager() {
           .getPointerSize(0 /* default address space */));
 }
 
+static std::unique_ptr<xla::TransferManager> CreateSYCLTransferManager() {
+  return std::make_unique<xla::gpu::GpuTransferManager>(
+      /*id=*/stream_executor::sycl::kSyclPlatformId,
+      /*pointer_size=*/llvm::DataLayout(xla::gpu::spir::DataLayout())
+          .getPointerSize(0 /* default address space */));
+}
+
 static bool InitModule() {
   xla::TransferManager::RegisterTransferManager(
       stream_executor::cuda::kCudaPlatformId, &CreateNVPTXTransferManager);
   xla::TransferManager::RegisterTransferManager(
       stream_executor::rocm::kROCmPlatformId, &CreateAMDGPUTransferManager);
+  xla::TransferManager::RegisterTransferManager(
+      stream_executor::sycl::kSyclPlatformId, &CreateSYCLTransferManager);
   return true;
 }
 static bool module_initialized = InitModule();
diff --git a/xla/service/gpu/ir_emission_utils.cc b/xla/service/gpu/ir_emission_utils.cc
index ca299d71d1..079518a9f1 100644
--- a/xla/service/gpu/ir_emission_utils.cc
+++ b/xla/service/gpu/ir_emission_utils.cc
@@ -130,11 +130,11 @@ bool IsMatrixMultiplication(const HloInstruction& dot) {
   const DotDimensionNumbers& dim_numbers = dot.dot_dimension_numbers();
 
   PrimitiveType output_primitive_type = dot.shape().element_type();
+  // Disable F64, C64, C128
   bool type_is_allowed =
       (output_primitive_type == F8E4M3FN || output_primitive_type == F8E5M2 ||
        output_primitive_type == F16 || output_primitive_type == BF16 ||
-       output_primitive_type == F32 || output_primitive_type == F64 ||
-       output_primitive_type == C64 || output_primitive_type == C128) ||
+       output_primitive_type == F32) ||
       (output_primitive_type == S32 && lhs_shape.element_type() == S8 &&
        rhs_shape.element_type() == S8);
   bool shapes_are_valid =
diff --git a/xla/service/gpu/ir_emitter_context.cc b/xla/service/gpu/ir_emitter_context.cc
index f09520eaef..762a4ca0c9 100644
--- a/xla/service/gpu/ir_emitter_context.cc
+++ b/xla/service/gpu/ir_emitter_context.cc
@@ -22,6 +22,7 @@ limitations under the License.
 
 #include "absl/algorithm/container.h"
 #include "llvm/ADT/ArrayRef.h"
+#include "llvm/TargetParser/Triple.h"
 #include "xla/service/gpu/gpu_constants.h"
 #include "xla/service/gpu/ir_emission_utils.h"
 
@@ -57,6 +58,8 @@ void IrEmitterContext::emit_constant(int64_t num_elements,
       return llvm::ConstantAggregateZero::get(global_type);
     }
 
+    // SYCL: always set info.content.
+    info.content = content;
     std::vector<uint8_t> padded(kMinConstAllocationInBytes, 0);
     absl::c_copy(content.span(), padded.begin());
     return llvm::ConstantDataArray::get<uint8_t>(
@@ -74,14 +77,38 @@ void IrEmitterContext::emit_constant(int64_t num_elements,
   //
   // We may have to be more clever here in the future if we notice that we're
   // keeping around too many globals because of their linkage.
+  // SYCL: Hardcode to global addrspace
+  bool is_spir = llvm::Triple(llvm_module_->getTargetTriple()).isSPIR();
+  int addrspace = is_spir ? 1 : 0;
   llvm::GlobalVariable* global_for_const = new llvm::GlobalVariable(
       global_type, /*isConstant=*/should_emit_initializer,
       llvm::GlobalValue::ExternalLinkage,
       /*Initializer=*/initializer, symbol_name,
       /*TLMode=*/llvm::GlobalValue::NotThreadLocal,
-      /*AddressSpace=*/0,
+      /*AddressSpace=*/addrspace,
       /*isExternallyInitialized=*/false);
   global_for_const->setAlignment(llvm::Align(kConstantBufferAlignBytes));
+
+  if (is_spir) {
+    // SYCL: Add spirv.Decorations for global variable. See document about the
+    // annotation:
+    // https://github.com/intel/llvm/blob/sycl/sycl/doc/design/spirv-extensions/SPV_INTEL_global_variable_decorations.asciidoc
+    llvm::LLVMContext& context = llvm_module_->getContext();
+    llvm::SmallVector<llvm::Metadata*, 4> metadatas;
+    std::vector<llvm::Metadata*> ops;
+
+    auto* kind = llvm::ConstantAsMetadata::get(llvm::ConstantInt::get(
+        llvm::Type::getInt32Ty(context), /*IDecHostAccessINTEL*/ 6147));
+    ops.push_back(kind);
+    auto* const acc_mode = llvm::ConstantAsMetadata::get(llvm::ConstantInt::get(
+        llvm::Type::getInt32Ty(context), /*AccessMode*/ 2));
+    ops.push_back(acc_mode);
+    ops.push_back(llvm::MDString::get(context, symbol_name));
+    metadatas.push_back(llvm::MDNode::get(context, ops));
+
+    llvm::MDNode* md_list = llvm::MDNode::get(context, metadatas);
+    global_for_const->setMetadata("spirv.Decorations", md_list);
+  }
   llvm_module_->insertGlobalVariable(global_for_const);
 
   info.symbol_name.assign(symbol_name);
diff --git a/xla/service/gpu/ir_emitter_nested.cc b/xla/service/gpu/ir_emitter_nested.cc
index 3a54197f27..98a8e39e1e 100644
--- a/xla/service/gpu/ir_emitter_nested.cc
+++ b/xla/service/gpu/ir_emitter_nested.cc
@@ -37,6 +37,11 @@ limitations under the License.
 #include "xla/service/llvm_ir/llvm_util.h"
 #include "xla/service/llvm_ir/tuple_ops.h"
 
+#if TENSORFLOW_USE_SYCL
+#include "xla/service/gpu/ir_emission_utils.h"
+#include "llvm/TargetParser/Triple.h"
+#endif
+
 namespace xla {
 namespace gpu {
 namespace {
@@ -299,7 +304,8 @@ void EmitAMDGPUAtomicAdd(llvm::IRBuilder<>* builder,
 
   builder->CreateAtomicRMW(
       llvm::AtomicRMWInst::FAdd, output_ptr, source, llvm::MaybeAlign(),
-      llvm::AtomicOrdering::SequentiallyConsistent,
+      // SYCL: set Monotonic.
+      llvm::AtomicOrdering::Monotonic,
       builder->getContext().getOrInsertSyncScopeID("agent"));
 }
 
@@ -367,7 +373,7 @@ bool MaybeEmitDirectAtomicOperation(llvm::IRBuilder<>* builder,
       if (atomic_add_supported) {
         builder->CreateAtomicRMW(llvm::AtomicRMWInst::FAdd, output_address,
                                  source, llvm::MaybeAlign(),
-                                 llvm::AtomicOrdering::SequentiallyConsistent);
+                                 llvm::AtomicOrdering::Monotonic);
         return true;
       }
     }
@@ -381,11 +387,19 @@ bool MaybeEmitDirectAtomicOperation(llvm::IRBuilder<>* builder,
       return true;
     }
 
+    if (target_triple.isSPIR() &&
+        element_type == F32) {
+      builder->CreateAtomicRMW(llvm::AtomicRMWInst::FAdd, output_address,
+                               source, llvm::MaybeAlign(),
+                               llvm::AtomicOrdering::Monotonic);
+      return true;
+    }
+
     if (is_atomic_integral) {
       // integral + integral
       builder->CreateAtomicRMW(
           llvm::AtomicRMWInst::Add, output_address, source, llvm::MaybeAlign(),
-          llvm::AtomicOrdering::SequentiallyConsistent, sync_scope);
+          llvm::AtomicOrdering::Monotonic, sync_scope);
       return true;
     }
   }
@@ -402,7 +416,7 @@ bool MaybeEmitDirectAtomicOperation(llvm::IRBuilder<>* builder,
                         : llvm::AtomicRMWInst::UMax;
       builder->CreateAtomicRMW(
           opcode, output_address, source, llvm::MaybeAlign(),
-          llvm::AtomicOrdering::SequentiallyConsistent, sync_scope);
+          llvm::AtomicOrdering::Monotonic, sync_scope);
       return true;
     } else if (element_type == F32) {
       // max(float, float) via AtomicMax and AtomicMin on int
@@ -455,7 +469,7 @@ bool MaybeEmitDirectAtomicOperation(llvm::IRBuilder<>* builder,
                     builder->CreateAtomicRMW(
                         llvm::AtomicRMWInst::Max, output_address,
                         source_float_as_int, llvm::MaybeAlign(),
-                        llvm::AtomicOrdering::SequentiallyConsistent,
+                        llvm::AtomicOrdering::Monotonic,
                         sync_scope);
                   },
                   [&]() {
@@ -463,7 +477,7 @@ bool MaybeEmitDirectAtomicOperation(llvm::IRBuilder<>* builder,
                     builder->CreateAtomicRMW(
                         llvm::AtomicRMWInst::UMin, output_address,
                         source_float_as_int, llvm::MaybeAlign(),
-                        llvm::AtomicOrdering::SequentiallyConsistent,
+                        llvm::AtomicOrdering::Monotonic,
                         sync_scope);
                   });
             });
@@ -479,7 +493,7 @@ bool MaybeEmitDirectAtomicOperation(llvm::IRBuilder<>* builder,
                       ? llvm::AtomicRMWInst::Min
                       : llvm::AtomicRMWInst::UMin;
     builder->CreateAtomicRMW(opcode, output_address, source, llvm::MaybeAlign(),
-                             llvm::AtomicOrdering::SequentiallyConsistent,
+                             llvm::AtomicOrdering::Monotonic,
                              sync_scope);
     return true;
   }
@@ -648,8 +662,8 @@ absl::Status EmitAtomicOperationUsingCAS(llvm::IRBuilder<>* builder,
   //                                       cas_new_output);
   llvm::Value* ret_value = builder->CreateAtomicCmpXchg(
       atomic_memory_address, cas_old_output, cas_new_output, llvm::MaybeAlign(),
-      llvm::AtomicOrdering::SequentiallyConsistent,
-      llvm::AtomicOrdering::SequentiallyConsistent, DetermineSyncScope(module));
+      llvm::AtomicOrdering::Monotonic,
+      llvm::AtomicOrdering::Monotonic, DetermineSyncScope(module));
 
   // Extract the memory value returned from atomicCAS and store it as
   // cas_old_output.
diff --git a/xla/service/gpu/ir_emitter_unnested.cc b/xla/service/gpu/ir_emitter_unnested.cc
index f3015e2865..d40cb84546 100644
--- a/xla/service/gpu/ir_emitter_unnested.cc
+++ b/xla/service/gpu/ir_emitter_unnested.cc
@@ -1,4 +1,6 @@
-/*Copyright 2022 The OpenXLA Authors.
+/* Copyright (c) 2024 Intel Corporation
+
+Copyright 2019 The TensorFlow Authors. All Rights Reserved.
 
 Licensed under the Apache License, Version 2.0 (the "License");
 you may not use this file except in compliance with the License.
@@ -116,16 +118,26 @@ limitations under the License.
 #include "xla/service/gpu/kernels/topk_custom_kernel.h"
 #include "xla/service/gpu/launch_dimensions.h"
 #include "xla/service/gpu/matmul_utils.h"
-#include "xla/service/gpu/nccl_all_to_all_thunk.h"
-#include "xla/service/gpu/nccl_api.h"
-#include "xla/service/gpu/nccl_collective_permute_thunk.h"
-#include "xla/service/gpu/nccl_collective_thunk.h"
-#include "xla/service/gpu/nccl_recv_thunk.h"
-#include "xla/service/gpu/nccl_send_thunk.h"
+// #include "xla/service/gpu/nccl_all_to_all_thunk.h"
+// #include "xla/service/gpu/nccl_api.h"
+// #include "xla/service/gpu/nccl_collective_permute_thunk.h"
+// #include "xla/service/gpu/nccl_collective_thunk.h"
+// #include "xla/service/gpu/runtime3/nccl_all_gather_thunk.h"
+// #include "xla/service/gpu/runtime3/nccl_all_reduce_thunk.h"
+// #include "xla/service/gpu/nccl_recv_thunk.h"
+// #include "xla/service/gpu/nccl_send_thunk.h"
+#include "xla/service/gpu/ccl_all_to_all_thunk.h"
+// #include "xla/service/gpu/nccl_api.h"
+#include "xla/service/gpu/ccl_collective_permute_thunk.h"
+#include "xla/service/gpu/ccl_collective_thunk.h"
+#include "xla/service/gpu/ccl_all_gather_thunk.h"
+#include "xla/service/gpu/ccl_all_reduce_thunk.h"
+// #include "xla/service/gpu/ccl_recv_thunk.h"
+// #include "xla/service/gpu/ccl_send_thunk.h"
 #include "xla/service/gpu/parallel_loop_emitter.h"
-#include "xla/service/gpu/runtime3/command_buffer_cmd.h"
-#include "xla/service/gpu/runtime3/command_buffer_cmd_emitter.h"
-#include "xla/service/gpu/runtime3/command_buffer_thunk.h"
+// #include "xla/service/gpu/runtime3/command_buffer_cmd.h"
+// #include "xla/service/gpu/runtime3/command_buffer_cmd_emitter.h"
+// #include "xla/service/gpu/runtime3/command_buffer_thunk.h"
 #include "xla/service/gpu/runtime3/conditional_thunk.h"
 #include "xla/service/gpu/runtime3/convolution_thunk.h"
 #include "xla/service/gpu/runtime3/copy_thunk.h"
@@ -135,8 +147,6 @@ limitations under the License.
 #include "xla/service/gpu/runtime3/gemm_thunk.h"
 #include "xla/service/gpu/runtime3/infeed_thunk.h"
 #include "xla/service/gpu/runtime3/kernel_thunk.h"
-#include "xla/service/gpu/runtime3/nccl_all_gather_thunk.h"
-#include "xla/service/gpu/runtime3/nccl_all_reduce_thunk.h"
 #include "xla/service/gpu/runtime3/norm_thunk.h"
 #include "xla/service/gpu/runtime3/outfeed_thunk.h"
 #include "xla/service/gpu/runtime3/replica_id_thunk.h"
@@ -172,16 +182,16 @@ limitations under the License.
 #include "tsl/platform/statusor.h"
 #include "tsl/protobuf/dnn.pb.h"
 
-#if GOOGLE_CUDA || TF_HIPBLASLT
+#if GOOGLE_CUDA || TF_HIPBLASLT || TENSORFLOW_USE_SYCL
 #include "xla/service/gpu/runtime3/gpublas_lt_matmul_thunk.h"
-#endif  // GOOGLE_CUDA || TF_HIPBLASLT
+#endif  // GOOGLE_CUDA || TF_HIPBLASLT || TENSORFLOW_USE_SYCL
 
-#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM
-#include "xla/service/gpu/ir_emitter_triton.h"
+#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM || TENSORFLOW_USE_SYCL
+// #include "xla/service/gpu/ir_emitter_triton.h"
 #include "xla/service/gpu/runtime3/cholesky_thunk.h"
-#include "xla/service/gpu/runtime3/cub_sort_thunk.h"
+// #include "xla/service/gpu/runtime3/cub_sort_thunk.h"
 #include "xla/service/gpu/runtime3/triangular_solve_thunk.h"
-#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM
+#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM || TENSORFLOW_USE_SYCL
 
 namespace xla {
 namespace gpu {
@@ -753,16 +763,19 @@ absl::Status IrEmitterUnnested::EmitCommandBufferThunk(
   // computation. Then convert emitted thunks to a sequence of CommandBufferCmd.
   // The resulting thunk added to the thunk sequence is a CommandBufferThunk.
   // Thunks emitted from the command buffer computation are discarded.
+  /*
   DCHECK_EQ(instr->called_computations().size(), 1);
   const HloComputation* command_buffer = instr->called_computations().front();
   auto ir_emitter = IrEmitterUnnested::Create(ir_emitter_context_);
   TF_RETURN_IF_ERROR(ir_emitter->EmitHloComputation(command_buffer));
   std::unique_ptr<ThunkSequence> thunk_sequence =
       ir_emitter->ConsumeThunkSequence();
+  */
 
   // Linearize all commands in a sequence by forcing barriers between all
   // recorded commands. This guarantees that we execute all device operations
   // in the exact same order as a thunk sequence.
+  /*
   bool force_barriers = !ir_emitter_context_->debug_options()
                              .xla_gpu_graph_enable_concurrent_region();
 
@@ -771,7 +784,7 @@ absl::Status IrEmitterUnnested::EmitCommandBufferThunk(
   AddThunkToThunkSequence(std::make_unique<CommandBufferThunk>(
       std::move(cmd_sequence), Thunk::ThunkInfo::WithProfileAnnotation(instr),
       std::move(*thunk_sequence)));
-
+  */
   return absl::OkStatus();
 }
 
@@ -813,10 +826,32 @@ absl::Status IrEmitterUnnested::EmitConvolutionThunk(
                                   instr->convolution_dimension_numbers(),
                                   instr->feature_group_count()};
 
-  TF_ASSIGN_OR_RETURN(GpuConvConfig config, GetGpuConvConfig(descriptor, ""));
-  AddThunkToThunkSequence(std::make_unique<ConvolutionThunk>(
-      Thunk::ThunkInfo::WithProfileAnnotation(instr), std::move(config),
+  ThunkSequence thunks;
+  if (kind == CudnnConvKind::kForwardActivation) {
+    // SYCL: OneDNN requires inplace sum
+    if (operand_slices.size() > 3 && operand_slices[3] != result_slices[0]) {
+      thunks.push_back(std::make_unique<DeviceToDeviceCopyThunk>(
+          Thunk::ThunkInfo::WithProfileAnnotation(instr),
+          /*side_input_buffer=*/operand_slices[3],
+          /*destination_buffer=*/result_slices[0],
+          /*mem_size=*/ShapeUtil::ByteSizeOf(instr->operand(3)->shape()),
+          /*source_value=*/nullptr,
+          /*destination_value=*/nullptr));
+    }
+  }
+
+  // SYCL: use descriptor for sycl conv
+  // TF_ASSIGN_OR_RETURN(GpuConvConfig config, GetGpuConvConfig(descriptor, ""));
+  thunks.push_back(std::make_unique<ConvolutionThunk>(
+      Thunk::ThunkInfo::WithProfileAnnotation(instr), std::move(descriptor),
       std::move(operand_slices), std::move(result_slices), scratch_slice));
+  if (thunks.size() == 1) {
+    AddThunkToThunkSequence(std::move(thunks[0]));
+  } else {
+    AddThunkToThunkSequence(std::make_unique<SequentialThunk>(
+        Thunk::ThunkInfo::WithProfileAnnotation(instr), std::move(thunks)));
+  }
+
   return OkStatus();
 }
 
@@ -930,6 +965,7 @@ absl::Status IrEmitterUnnested::EmitConvolutionThunk(mlir::Operation* op) {
     return absl::OkStatus();
   };
 
+  ThunkSequence thunks;
   if (auto conv = dyn_cast<ConvForwardOp>(op)) {
     descriptor.kind = CudnnConvKind::kForward;
     fill_conv_descriptor(conv);
@@ -956,13 +992,31 @@ absl::Status IrEmitterUnnested::EmitConvolutionThunk(mlir::Operation* op) {
     TF_RETURN_IF_ERROR(set_activation_mode(conv));
     descriptor.backend_config.set_side_input_scale(
         conv.getSideInputScale().convertToDouble());
+
+    // SYCL: OneDNN requires inplace sum
+    if (operand_slices[3] != result_slices[0]) {
+      thunks.push_back(std::make_unique<DeviceToDeviceCopyThunk>(
+          Thunk::ThunkInfo::WithProfileAnnotation(op),
+          /*side_input_buffer=*/operand_slices[3],
+          /*destination_buffer=*/result_slices[0],
+          /*mem_size=*/ShapeUtil::ByteSizeOf(GetShape(op->getOperand(3))),
+          /*source_value=*/op->getOperand(3),
+          /*destination_value=*/op->getOperand(4)));
+    }
   } else {
     return Internal("EmitConvolutionThunk: Unexpected operation");
   }
-  TF_ASSIGN_OR_RETURN(GpuConvConfig config, GetGpuConvConfig(descriptor, ""));
-  AddThunkToThunkSequence(std::make_unique<ConvolutionThunk>(
-      Thunk::ThunkInfo::WithProfileAnnotation(op), std::move(config),
+  // SYCL: use descriptor for sycl conv
+  // TF_ASSIGN_OR_RETURN(GpuConvConfig config, GetGpuConvConfig(descriptor, ""));
+  thunks.push_back(std::make_unique<ConvolutionThunk>(
+      Thunk::ThunkInfo::WithProfileAnnotation(op), std::move(descriptor),
       std::move(operand_slices), std::move(result_slices), scratch_slice));
+  if (thunks.size() == 1) {
+    AddThunkToThunkSequence(std::move(thunks[0]));
+  } else {
+    AddThunkToThunkSequence(std::make_unique<SequentialThunk>(
+        Thunk::ThunkInfo::WithProfileAnnotation(op), std::move(thunks)));
+  }
   return absl::OkStatus();
 }
 
@@ -977,6 +1031,7 @@ absl::Status IrEmitterUnnested::EmitGemmThunk(mlir::Operation* op) {
       ir_emitter_context_->debug_options().xla_gpu_deterministic_ops();
 
   TF_ASSIGN_OR_RETURN(GemmConfig config, GemmConfig::For(gemm));
+  config.epilogue = se::cuda::BlasLt::Epilogue::kDefault;
   auto thunk = std::make_unique<GemmThunk>(
       Thunk::ThunkInfo::WithProfileAnnotation(op), std::move(config), a, b, c,
       std::nullopt, deterministic_ops);
@@ -1018,7 +1073,7 @@ absl::Status IrEmitterUnnested::EmitGemmThunk(
   return absl::OkStatus();
 }
 
-#if GOOGLE_CUDA || TF_HIPBLASLT
+#if GOOGLE_CUDA || TF_HIPBLASLT || TENSORFLOW_USE_SYCL
 
 absl::Status IrEmitterUnnested::EmitCublasLtMatmulThunk(
     const HloCustomCallInstruction* instr) {
@@ -1115,8 +1170,9 @@ absl::Status IrEmitterUnnested::EmitCublasLtMatmulThunk(mlir::Operation* op) {
   AddThunkToThunkSequence(std::move(thunk));
   return absl::OkStatus();
 }
-#endif  // GOOGLE_CUDA || TF_HIPBLASLT
+#endif  // GOOGLE_CUDA || TF_HIPBLASLT || TENSORFLOW_USE_SYCL
 
+#if GOOGLE_CUDA || TENSORFLOW_USE_SYCL
 #if GOOGLE_CUDA
 
 absl::Status IrEmitterUnnested::EmitCublasLtMatmulThunkF8(
@@ -1437,6 +1493,7 @@ absl::Status IrEmitterUnnested::EmitNormThunk(mlir::Operation* op) {
   return absl::OkStatus();
 }
 
+#endif  // GOOGLE_CUDA
 absl::Status IrEmitterUnnested::EmitFusedMHAThunk(mlir::Operation* op) {
   using mlir::dyn_cast;
   using mlir::lmhlo_gpu::fusedMHAOp;
@@ -1565,6 +1622,7 @@ absl::Status IrEmitterUnnested::EmitFusedMHAThunk(mlir::Operation* op) {
   return absl::OkStatus();
 }
 
+#if GOOGLE_CUDA
 absl::Status IrEmitterUnnested::EmitFusedMHABackwardThunk(mlir::Operation* op) {
   using mlir::dyn_cast;
   using mlir::lmhlo_gpu::fusedMHABackwardOp;
@@ -1778,6 +1836,7 @@ absl::Status IrEmitterUnnested::EmitFusedMHABackwardThunk(mlir::Operation* op) {
   return absl::OkStatus();
 }
 #endif  // GOOGLE_CUDA
+#endif  // GOOGLE_CUDA || TENSORFLOW_USE_SYCL
 
 absl::StatusOr<BufferAllocation::Slice>
 IrEmitterUnnested::GetAllocationSliceForHlo(const HloInstruction* instr,
@@ -1786,8 +1845,8 @@ IrEmitterUnnested::GetAllocationSliceForHlo(const HloInstruction* instr,
                                       instr, index);
 }
 
-#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM
-
+#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM || TENSORFLOW_USE_SYCL
+#if 0
 absl::Status IrEmitterUnnested::EmitCubDeviceRadixSort(
     const HloCustomCallInstruction* instr) {
   if (instr->operand_count() != 1 && instr->operand_count() != 2) {
@@ -1858,7 +1917,7 @@ absl::Status IrEmitterUnnested::EmitCubDeviceRadixSort(mlir::Operation* op) {
   AddThunkToThunkSequence(std::move(thunk));
   return absl::OkStatus();
 }
-
+#endif
 absl::Status IrEmitterUnnested::EmitCholeskyThunk(mlir::Operation* op) {
   auto cholesky_op = mlir::cast<mlir::lmhlo_gpu::CholeskyOp>(op);
 
@@ -1960,7 +2019,7 @@ absl::Status IrEmitterUnnested::EmitCholeskyThunk(const HloInstruction* instr) {
 
   return absl::OkStatus();
 }
-#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM
+#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM || TENSORFLOW_USE_SYCL
 
 // Converts MLIR dictionary attribute attached to a custom call operation to a
 // custom call thunk attributes that are forwarded to the FFI handler.
@@ -2398,7 +2457,7 @@ absl::Status IrEmitterUnnested::EmitFftThunk(const HloFftInstruction* instr) {
   return absl::OkStatus();
 }
 
-#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM
+#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM || TENSORFLOW_USE_SYCL
 absl::Status IrEmitterUnnested::EmitTriangularSolveCustomCall(
     mlir::Operation* op) {
   auto custom_call = mlir::cast<mlir::lmhlo::CustomCallOp>(op);
@@ -2568,7 +2627,7 @@ absl::Status IrEmitterUnnested::EmitTriangularSolveCustomCall(
   }
   return absl::OkStatus();
 }
-#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM
+#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM || TENSORFLOW_USE_SYCL
 
 absl::Status IrEmitterUnnested::EmitTopKCustomCall(
     const HloCustomCallInstruction* instr) {
@@ -4108,33 +4167,33 @@ static std::optional<GlobalDeviceId> DeviceConstraint(
 }
 
 absl::Status IrEmitterUnnested::EmitSendThunk(const HloSendInstruction* instr) {
-  if (!instr->channel_id().has_value())
-    return absl::InternalError("Unknown send instruction channel id");
-
-  const HloInstruction* src = instr->operand(0);
-  TF_ASSIGN_OR_RETURN(BufferAllocation::Slice buffer,
-                      GetAllocationSliceForHlo(src, {}));
-  if (!instr->is_host_transfer()) {
-    const auto& hlo_config = ir_emitter_context_->hlo_module().config();
-    const int64_t replica_count = hlo_config.replica_count();
-    const int64_t partition_count = hlo_config.num_partitions();
-    const NcclCollectiveThunk::Buffer nccl_buffer = {
-        /*element_count=*/ShapeUtil::ElementsIn(src->shape()),
-        /*source_buffer=*/buffer,
-        /*destination_buffer=*/buffer};
-    auto thunk = std::make_unique<NcclSendThunk>(
-        Thunk::ThunkInfo::WithProfileAnnotation(instr), NcclApi::Default(),
-        instr, replica_count, partition_count, nccl_buffer);
-    collectives_async_events_.try_emplace(instr, thunk->async_events());
-    AddThunkToThunkSequence(std::move(thunk));
-    return absl::OkStatus();
-  }
+  // if (!instr->channel_id().has_value())
+  //   return absl::InternalError("Unknown send instruction channel id");
+
+  // const HloInstruction* src = instr->operand(0);
+  // TF_ASSIGN_OR_RETURN(BufferAllocation::Slice buffer,
+  //                     GetAllocationSliceForHlo(src, {}));
+  // if (!instr->is_host_transfer()) {
+  //   const auto& hlo_config = ir_emitter_context_->hlo_module().config();
+  //   const int64_t replica_count = hlo_config.replica_count();
+  //   const int64_t partition_count = hlo_config.num_partitions();
+  //   const NcclCollectiveThunk::Buffer nccl_buffer = {
+  //       /*element_count=*/ShapeUtil::ElementsIn(src->shape()),
+  //       /*source_buffer=*/buffer,
+  //       /*destination_buffer=*/buffer};
+  //   auto thunk = std::make_unique<NcclSendThunk>(
+  //       Thunk::ThunkInfo::WithProfileAnnotation(instr), NcclApi::Default(),
+  //       instr, replica_count, partition_count, nccl_buffer);
+  //   collectives_async_events_.try_emplace(instr, thunk->async_events());
+  //   AddThunkToThunkSequence(std::move(thunk));
+  //   return absl::OkStatus();
+  // }
 
-  AddThunkToThunkSequence(std::make_unique<SendThunk>(
-      Thunk::ThunkInfo::WithProfileAnnotation(instr), src->shape(), buffer,
-      *instr->channel_id(), send_recv_events_,
-      ConvertFrontendAttributes(instr->frontend_attributes()),
-      DeviceConstraint(instr)));
+  // AddThunkToThunkSequence(std::make_unique<SendThunk>(
+  //     Thunk::ThunkInfo::WithProfileAnnotation(instr), src->shape(), buffer,
+  //     *instr->channel_id(), send_recv_events_,
+  //     ConvertFrontendAttributes(instr->frontend_attributes()),
+  //     DeviceConstraint(instr)));
 
   return absl::OkStatus();
 }
@@ -4156,33 +4215,33 @@ absl::Status IrEmitterUnnested::EmitSendDoneThunk(
 }
 
 absl::Status IrEmitterUnnested::EmitRecvThunk(const HloRecvInstruction* instr) {
-  if (!instr->channel_id().has_value())
-    return absl::InternalError("Unknown recv instruction channel id");
-  TF_RET_CHECK(instr->shape().IsTuple());
-  TF_ASSIGN_OR_RETURN(BufferAllocation::Slice buffer,
-                      GetAllocationSliceForHlo(instr, {0}));
-  if (!instr->is_host_transfer()) {
-    const auto& hlo_config = ir_emitter_context_->hlo_module().config();
-    const int64_t replica_count = hlo_config.replica_count();
-    const int64_t partition_count = hlo_config.num_partitions();
-    const NcclCollectiveThunk::Buffer nccl_buffer = {
-        /*element_count=*/ShapeUtil::ElementsIn(instr->shape().tuple_shapes(0)),
-        /*source_buffer=*/buffer,
-        /*destination_buffer=*/buffer};
-    auto thunk = std::make_unique<NcclRecvThunk>(
-        Thunk::ThunkInfo::WithProfileAnnotation(instr), NcclApi::Default(),
-        instr, replica_count, partition_count, nccl_buffer);
-    collectives_async_events_.try_emplace(instr, thunk->async_events());
-    AddThunkToThunkSequence(std::move(thunk));
-    return absl::OkStatus();
-  }
+  // if (!instr->channel_id().has_value())
+  //   return absl::InternalError("Unknown recv instruction channel id");
+  // TF_RET_CHECK(instr->shape().IsTuple());
+  // TF_ASSIGN_OR_RETURN(BufferAllocation::Slice buffer,
+  //                     GetAllocationSliceForHlo(instr, {0}));
+  // if (!instr->is_host_transfer()) {
+  //   const auto& hlo_config = ir_emitter_context_->hlo_module().config();
+  //   const int64_t replica_count = hlo_config.replica_count();
+  //   const int64_t partition_count = hlo_config.num_partitions();
+  //   const NcclCollectiveThunk::Buffer nccl_buffer = {
+  //       /*element_count=*/ShapeUtil::ElementsIn(instr->shape().tuple_shapes(0)),
+  //       /*source_buffer=*/buffer,
+  //       /*destination_buffer=*/buffer};
+  //   auto thunk = std::make_unique<NcclRecvThunk>(
+  //       Thunk::ThunkInfo::WithProfileAnnotation(instr), NcclApi::Default(),
+  //       instr, replica_count, partition_count, nccl_buffer);
+  //   collectives_async_events_.try_emplace(instr, thunk->async_events());
+  //   AddThunkToThunkSequence(std::move(thunk));
+  //   return absl::OkStatus();
+  // }
 
-  AddThunkToThunkSequence(std::make_unique<RecvThunk>(
-      Thunk::ThunkInfo::WithProfileAnnotation(instr),
-      instr->shape().tuple_shapes()[0], buffer, *instr->channel_id(),
-      send_recv_events_,
-      ConvertFrontendAttributes(instr->frontend_attributes()),
-      DeviceConstraint(instr)));
+  // AddThunkToThunkSequence(std::make_unique<RecvThunk>(
+  //     Thunk::ThunkInfo::WithProfileAnnotation(instr),
+  //     instr->shape().tuple_shapes()[0], buffer, *instr->channel_id(),
+  //     send_recv_events_,
+  //     ConvertFrontendAttributes(instr->frontend_attributes()),
+  //     DeviceConstraint(instr)));
 
   return absl::OkStatus();
 }
@@ -4233,12 +4292,12 @@ absl::Status IrEmitterUnnested::EmitOp(
       return EmitSliceToDynamic(op);
     }
     const llvm::StringRef call_target = call.getCallTargetName();
-#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM
+#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM || TENSORFLOW_USE_SYCL
     if (absl::string_view(call_target.data(), call_target.size()) ==
         kTriangularSolveCallTarget) {
       return EmitTriangularSolveCustomCall(op);
     }
-#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM
+#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM || TENSORFLOW_USE_SYCL
 
     if (!is_gpu_runtime && call.getCallTargetName() == "__gpu$TopK") {
       return EmitTopKCustomCall(
@@ -4258,7 +4317,7 @@ absl::Status IrEmitterUnnested::EmitOp(
     return EmitGemmThunk(op);
   }
 
-#if GOOGLE_CUDA || TF_HIPBLASLT
+#if GOOGLE_CUDA || TF_HIPBLASLT || TENSORFLOW_USE_SYCL
   if (mlir::isa<mlir::lmhlo_gpu::CublasLtMatmulOp>(op)) {
     if (ir_emitter_context_->emit_ir_from_hlo()) {
       const auto* instr = Cast<HloCustomCallInstruction>(hlo_for_lmhlo.at(op));
@@ -4266,7 +4325,7 @@ absl::Status IrEmitterUnnested::EmitOp(
     }
     return EmitCublasLtMatmulThunk(op);
   }
-#endif  // GOOGLE_CUDA || TF_HIPBLASLT
+#endif  // GOOGLE_CUDA || TF_HIPBLASLT || TENSORFLOW_USE_SYCL
 #if GOOGLE_CUDA
   if (mlir::isa<mlir::lmhlo_gpu::CublasLtMatmulF8Op>(op)) {
     if (ir_emitter_context_->emit_ir_from_hlo()) {
@@ -4290,13 +4349,13 @@ absl::Status IrEmitterUnnested::EmitOp(
     }
     return EmitNormThunk(op);
   }
-  if (mlir::isa<mlir::lmhlo_gpu::fusedMHAOp>(op)) {
-    return EmitFusedMHAThunk(op);
-  }
   if (mlir::isa<mlir::lmhlo_gpu::fusedMHABackwardOp>(op)) {
     return EmitFusedMHABackwardThunk(op);
   }
 #endif  // GOOGLE_CUDA
+  if (mlir::isa<mlir::lmhlo_gpu::fusedMHAOp>(op)) {
+    return EmitFusedMHAThunk(op);
+  }
 
   if (mlir::isa<mlir::lmhlo_gpu::ConvForwardOp,
                 mlir::lmhlo_gpu::ConvForwardGraphOp,
@@ -4311,7 +4370,8 @@ absl::Status IrEmitterUnnested::EmitOp(
     return EmitConvolutionThunk(op);
   }
 
-#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM
+#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM || TENSORFLOW_USE_SYCL
+#if 0
   if (mlir::isa<mlir::lmhlo_gpu::RadixSortOp>(op)) {
     if (ir_emitter_context_->emit_ir_from_hlo()) {
       auto* instr = Cast<HloCustomCallInstruction>(hlo_for_lmhlo.at(op));
@@ -4319,6 +4379,7 @@ absl::Status IrEmitterUnnested::EmitOp(
     }
     return EmitCubDeviceRadixSort(op);
   }
+#endif
   if (mlir::isa<mlir::lmhlo_gpu::CholeskyOp>(op)) {
     if (ir_emitter_context_->emit_ir_from_hlo()) {
       return EmitCholeskyThunk(hlo_for_lmhlo.at(op));
@@ -4326,7 +4387,7 @@ absl::Status IrEmitterUnnested::EmitOp(
       return EmitCholeskyThunk(op);
     }
   }
-#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM
+#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM || TENSORFLOW_USE_SYCL
 
   if (mlir::isa<mlir::lmhlo::FftOp>(op)) {
     if (ir_emitter_context_->emit_ir_from_hlo()) {
@@ -4668,11 +4729,11 @@ absl::Status IrEmitterUnnested::EmitHloInstruction(
       if (IsLegacyCublasMatmul(*instr)) {
         return EmitGemmThunk(custom_call);
       }
-#if GOOGLE_CUDA || TF_HIPBLASLT
+#if GOOGLE_CUDA || TF_HIPBLASLT || TENSORFLOW_USE_SYCL
       if (IsCublasLtMatmul(*instr)) {
         return EmitCublasLtMatmulThunk(custom_call);
       }
-#endif  // GOOGLE_CUDA || TF_HIPBLASLT
+#endif  // GOOGLE_CUDA || TF_HIPBLASLT || TENSORFLOW_USE_SYCL
 #if GOOGLE_CUDA
       if (IsCublasLtMatmulF8(*instr)) {
         return EmitCublasLtMatmulThunkF8(custom_call);
@@ -4690,17 +4751,19 @@ absl::Status IrEmitterUnnested::EmitHloInstruction(
       if (IsCustomCallToDnnConvolution(*instr)) {
         return EmitConvolutionThunk(custom_call);
       }
-#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM
+#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM || TENSORFLOW_USE_SYCL
       if (IsCustomCallToCusolver(*instr)) {
         return EmitCholeskyThunk(instr);
       }
       if (IsTriangularSolve(*instr)) {
         return EmitTriangularSolveCustomCall(instr);
       }
+#if 0
       if (IsCubDeviceRadixSort(*instr)) {
         return EmitCubDeviceRadixSort(custom_call);
       }
-#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM
+#endif
+#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM || TENSORFLOW_USE_SYCL
       if (custom_call->custom_call_target() == "PadToStatic") {
         return EmitPadToStatic(custom_call);
       }
diff --git a/xla/service/gpu/ir_emitter_unnested.h b/xla/service/gpu/ir_emitter_unnested.h
index 5fa23a5142..31da6809ae 100644
--- a/xla/service/gpu/ir_emitter_unnested.h
+++ b/xla/service/gpu/ir_emitter_unnested.h
@@ -1,4 +1,6 @@
-/* Copyright 2018 The OpenXLA Authors.
+/* Copyright (c) 2024 Intel Corporation
+
+Copyright 2019 The TensorFlow Authors. All Rights Reserved.
 
 Licensed under the Apache License, Version 2.0 (the "License");
 you may not use this file except in compliance with the License.
@@ -39,7 +41,8 @@ limitations under the License.
 #include "xla/service/gpu/fusions/fusion_emitter.h"
 #include "xla/service/gpu/hlo_fusion_analysis.h"
 #include "xla/service/gpu/ir_emitter.h"
-#include "xla/service/gpu/nccl_collective_thunk.h"
+// #include "xla/service/gpu/nccl_collective_thunk.h"
+#include "xla/service/gpu/ccl_collective_thunk.h"
 #include "xla/service/gpu/runtime3/send_recv_thunk.h"
 #include "xla/service/gpu/thunk.h"
 #include "xla/service/llvm_ir/ir_array.h"
@@ -148,10 +151,10 @@ class IrEmitterUnnested : public IrEmitter {
   absl::Status EmitConvolutionThunk(const HloCustomCallInstruction* instr);
   absl::Status EmitGemmThunk(mlir::Operation* op);
   absl::Status EmitGemmThunk(const HloCustomCallInstruction* instr);
-#if GOOGLE_CUDA || TF_HIPBLASLT
+#if GOOGLE_CUDA || TF_HIPBLASLT || TENSORFLOW_USE_SYCL
   absl::Status EmitCublasLtMatmulThunk(mlir::Operation* op);
   absl::Status EmitCublasLtMatmulThunk(const HloCustomCallInstruction* instr);
-#endif  // GOOGLE_CUDA || TF_HIPBLASLT
+#endif  // GOOGLE_CUDA || TF_HIPBLASLT || TENSORFLOW_USE_SYCL
 #if GOOGLE_CUDA
   absl::Status EmitCublasLtMatmulThunkF8(mlir::Operation* op);
   absl::Status EmitCublasLtMatmulThunkF8(const HloCustomCallInstruction* instr);
@@ -163,12 +166,13 @@ class IrEmitterUnnested : public IrEmitter {
   absl::Status EmitFusedMHAThunk(mlir::Operation* op);
   absl::Status EmitFusedMHABackwardThunk(mlir::Operation* op);
 #endif  // GOOGLE_CUDA
+  absl::Status EmitFusedMHAThunk(mlir::Operation* op);
 #if GOOGLE_CUDA || TENSORFLOW_USE_ROCM
   absl::Status EmitCubDeviceRadixSort(mlir::Operation* op);
   absl::Status EmitCubDeviceRadixSort(const HloCustomCallInstruction* instr);
+#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM
   absl::Status EmitCholeskyThunk(mlir::Operation* op);
   absl::Status EmitCholeskyThunk(const HloInstruction* instr);
-#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM
   absl::Status EmitCustomCallThunk(mlir::Operation* op,
                                    const HloCustomCallInstruction* instr);
   absl::Status EmitCustomCallThunk(const HloCustomCallInstruction* instr);
@@ -201,10 +205,10 @@ class IrEmitterUnnested : public IrEmitter {
 
   absl::Status EmitSort(mlir::Operation* op, const HloSortInstruction* sort);
   absl::Status EmitSort(const HloSortInstruction* sort);
-#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM
+#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM || TENSORFLOW_USE_SYCL
   absl::Status EmitTriangularSolveCustomCall(mlir::Operation* op);
   absl::Status EmitTriangularSolveCustomCall(const HloInstruction* instr);
-#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM
+#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM || TENSORFLOW_USE_SYCL
   absl::Status EmitTopKCustomCall(const HloCustomCallInstruction* instr);
 
   absl::Status EmitSendThunk(const HloSendInstruction* instr);
diff --git a/xla/service/gpu/launch_dimensions.cc b/xla/service/gpu/launch_dimensions.cc
index 741e2dd343..1e032c0257 100644
--- a/xla/service/gpu/launch_dimensions.cc
+++ b/xla/service/gpu/launch_dimensions.cc
@@ -94,6 +94,9 @@ struct BlockSizes {
 BlockSizes GetBlockSizes(LaunchDimensionsConfig dim_config,
                          const se::DeviceDescription& gpu_device_info,
                          const Shape& shape, int64_t num_elements) {
+#if !TENSORFLOW_USE_SYCL
+  // TODO: It will set 128 threads per block by default. We prefer to use
+  // the max value (1024 on PVC). It can benefit instructions like scatter.
   if (!dim_config.row_vectorized && !dim_config.few_waves) {
     BlockSizes result;
     const int kWarpSchedulers = 4;
@@ -104,6 +107,7 @@ BlockSizes GetBlockSizes(LaunchDimensionsConfig dim_config,
         num_elements, result.threads_per_block_x * result.threads_per_block_y);
     return result;
   }
+#endif
 
   int64_t threads_per_block_row_vectorized =
       ThreadsPerBlockRowVectorized(shape, gpu_device_info, dim_config);
diff --git a/xla/service/gpu/llvm_gpu_backend/BUILD b/xla/service/gpu/llvm_gpu_backend/BUILD
index 9f437f78c3..426f039e9c 100644
--- a/xla/service/gpu/llvm_gpu_backend/BUILD
+++ b/xla/service/gpu/llvm_gpu_backend/BUILD
@@ -3,6 +3,10 @@ load(
     "@local_config_rocm//rocm:build_defs.bzl",
     "if_rocm_is_configured",
 )
+load(
+    "@local_config_sycl//sycl:build_defs.bzl",
+    "if_sycl_is_configured",
+)
 
 package(
     # copybara:uncomment default_applicable_licenses = ["//tensorflow:license"],
@@ -36,6 +40,7 @@ cc_library(
         "//xla/service/gpu:metrics",
         "//xla/service/llvm_ir:llvm_command_line_options",
         "//xla/service/llvm_ir:llvm_type_conversion_util",
+        "//xla/service/llvm_ir:llvm_util",
         "//xla/stream_executor:device_description",
         "@com_google_absl//absl/base",
         "@com_google_absl//absl/memory",
@@ -68,6 +73,8 @@ cc_library(
     ] + if_rocm_is_configured([
         "@local_config_rocm//rocm:rocm_headers",
         "@llvm-project//llvm:AMDGPUCodeGen",
+    ]) + if_sycl_is_configured([
+        "@llvm_spir//:llvm_spir_translator",
     ]),
 )
 
diff --git a/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc b/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc
index bbd1352fd0..3858f52ff4 100644
--- a/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc
+++ b/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc
@@ -59,6 +59,7 @@ limitations under the License.
 #include "xla/service/gpu/metrics.h"
 #include "xla/service/llvm_ir/llvm_command_line_options.h"
 #include "xla/service/llvm_ir/llvm_type_conversion_util.h"
+#include "xla/service/llvm_ir/llvm_util.h"
 #include "xla/status_macros.h"
 #include "xla/stream_executor/device_description.h"
 #include "xla/types.h"
@@ -76,6 +77,9 @@ limitations under the License.
 #include "rocm/rocm_config.h"
 #endif
 
+#include "LLVMSPIRVLib.h"
+#include "LLVMSPIRVOpts.h"
+
 namespace xla {
 namespace gpu {
 namespace {
@@ -156,7 +160,6 @@ std::unique_ptr<llvm::TargetMachine> GetTargetMachine(
                << " -- " << error;
     return nullptr;
   }
-
   llvm::TargetOptions target_options =
       llvm::codegen::InitTargetOptionsFromCodeGenFlags(llvm::Triple());
 
@@ -368,10 +371,13 @@ absl::Status LinkAndOptimizeModule(
   llvm::CGSCCAnalysisManager cgam;
   llvm::ModuleAnalysisManager mam;
 
-  fam.registerPass([&] { return target_machine->getTargetIRAnalysis(); });
+  if (target_machine)
+    fam.registerPass([&] { return target_machine->getTargetIRAnalysis(); });
 
+  // SYCL: customized config
   llvm::PipelineTuningOptions pto;
-  pto.SLPVectorization = true;
+  pto.SLPVectorization = false;
+  pto.LoopVectorization = false;
   pto.InlinerThreshold = inline_threshold;
 
   llvm::PassInstrumentationCallbacks pic;
@@ -1034,5 +1040,107 @@ absl::StatusOr<std::vector<uint8_t>> CompileToHsaco(
 
 }  // namespace amdgpu
 
+namespace {
+std::unique_ptr<llvm::TargetMachine> SPIRGetTargetMachine(
+    llvm::Triple target_triple, se::GpuComputeCapability gpu_version,
+    const DebugOptions& debug_options) {
+  return nullptr;
+}
+
+Status SPIRTargetModuleLinker(llvm::Module* module,
+                              se::GpuComputeCapability gpu_version,
+                              const DebugOptions& debug_options,
+                              const std::string& device_bitcode_dir_path) {
+  return OkStatus();
+}
+
+StatusOr<std::string> EmitModuleToSpir(llvm::Module* module,
+                                       se::GpuComputeCapability gpu_version,
+                                       const DebugOptions& debug_options) {
+  SPIRV::TranslatorOpts::ExtensionsStatusMap ExtensionsStatus;
+  SPIRV::TranslatorOpts opts(SPIRV::VersionNumber::MaximumVersion,
+                             ExtensionsStatus);
+  opts.enableAllExtensions();  // enable all SPIR-V extension first
+
+  std::ostringstream oss;
+  std::string err;
+  bool success = llvm::writeSpirv(module, opts, oss, err);
+  if (!success) {
+    return xla::Internal("Fails to convert LLVM as SPIR-V: %s", err);
+  }
+  return oss.str();
+}
+
+void SPIRBackendInit(const DebugOptions& debug_options) {
+
+  FeedLLVMWithFlags({"-slp-vectorize-hor=false"});
+
+  bool vec = true;
+  tsl::ReadBoolFromEnvVar("VECTORIZE", true, &vec);
+  if (vec) {
+    FeedLLVMWithFlags({
+        "-slp-min-reg-size=64",
+        "-slp-max-reg-size=64",
+    });
+  } else {
+    // TODO: sycl-opt disables all LLVM vectorization passes. Evaluate if it is
+    // needed.
+    FeedLLVMWithFlags({"-sycl-opt=1"});
+  }
+
+  llvm_ir::InitializeLLVMCommandLineOptions(
+      debug_options.xla_backend_extra_options());
+
+  llvm::PassRegistry* registry = llvm::PassRegistry::getPassRegistry();
+  InitializePasses(registry);
+}
+}  // namespace
+
+namespace spir {
+absl::StatusOr<std::vector<uint8_t>> CompileToSpir(
+    llvm::Module* module, se::GpuComputeCapability gpu_version,
+    const DebugOptions& debug_options) {
+  std::string libdevice_dir_path;
+  static absl::once_flag backend_init_flag;
+  absl::call_once(backend_init_flag, SPIRBackendInit, debug_options);
+
+  std::string spir;
+  {
+    XLA_SCOPED_LOGGING_TIMER("Compile module " + module->getName().str());
+
+    // If the module has no functions or globals, there's nothing to compile.
+    if (module->empty() && module->global_empty()) {
+      VLOG(2) << "Module '" << module->getName().str()
+              << "' is empty. Skipping compilation.";
+      return std::vector<uint8_t>();
+    }
+
+    // No SPIR target machine?
+    llvm::Triple default_target_triple("spir64-unknown-unknown");
+    std::unique_ptr<llvm::TargetMachine> target_machine =
+        SPIRGetTargetMachine(default_target_triple, gpu_version, debug_options);
+
+    bool opt = true;
+    tsl::ReadBoolFromEnvVar("SYCL_LLVM_OPT", true, &opt);
+    if (opt) {
+      // Link with libdevice, and optimize the LLVM module.
+      TF_RETURN_IF_ERROR(LinkAndOptimizeModule(
+          module, gpu_version, debug_options, libdevice_dir_path,
+          SPIRTargetModuleLinker, default_target_triple, target_machine.get(),
+          kDefaultInlineThreshold));
+    }
+
+#if 0
+    LOG(ERROR) << "Optimized IR before converting to spir\n" << llvm_ir::DumpToString(module);
+#endif
+
+    // Lower optimized LLVM module to SPIR.
+    TF_ASSIGN_OR_RETURN(spir,
+                        EmitModuleToSpir(module, gpu_version, debug_options));
+  }
+  return std::vector<uint8_t>(spir.begin(), spir.end());
+}
+}  // namespace spir
+
 }  // namespace gpu
 }  // namespace xla
diff --git a/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.h b/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.h
index b774daac3e..0e2bce0950 100644
--- a/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.h
+++ b/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.h
@@ -66,6 +66,12 @@ absl::StatusOr<std::vector<uint8_t>> CompileToHsaco(
     const std::string& module_config_cache_key);
 }  // namespace amdgpu
 
+namespace spir {
+absl::StatusOr<std::vector<uint8_t>> CompileToSpir(
+    llvm::Module* module, se::GpuComputeCapability gpu_version,
+    const DebugOptions& debug_options);
+}  // namespace spir
+
 }  // namespace gpu
 }  // namespace xla
 
diff --git a/xla/service/gpu/make_batch_pointers.cc b/xla/service/gpu/make_batch_pointers.cc
index 9cc84bf6be..741b406045 100644
--- a/xla/service/gpu/make_batch_pointers.cc
+++ b/xla/service/gpu/make_batch_pointers.cc
@@ -44,6 +44,7 @@ namespace make_batch_pointers {
 void* kernel();  // returns a pointer to a CUDA C++ device function
 }  // namespace make_batch_pointers
 
+#if !TENSORFLOW_USE_SYCL
 absl::Status MakeBatchPointers(se::Stream* stream,
                                se::DeviceMemoryBase base_ptr,
                                size_t stride_bytes, size_t n,
@@ -71,5 +72,14 @@ absl::Status MakeBatchPointers(se::Stream* stream,
 #endif
   return absl::OkStatus();
 }
+#else
+absl::Status MakeBatchPointers(se::Stream* stream,
+                               const se::DeviceMemoryBase& base_ptr,
+                               size_t stride_bytes, size_t n,
+                               se::DeviceMemoryBase& ptrs_out) {
+  ptrs_out = base_ptr;
+  return absl::OkStatus();
+}
+#endif // !TENSORFLOW_USE_SYCL
 
 }  // namespace xla::gpu
diff --git a/xla/service/gpu/make_batch_pointers.h b/xla/service/gpu/make_batch_pointers.h
index 1be4994ec5..b0eeb058e7 100644
--- a/xla/service/gpu/make_batch_pointers.h
+++ b/xla/service/gpu/make_batch_pointers.h
@@ -50,11 +50,16 @@ namespace xla::gpu {
 //    driver and slow down *all* work on the GPU.  So to do this right, we'd
 //    need to allocate the host memory as pinned, one alloc per stream.  Then
 //    we'd need to manage this memory without leaks.  This becomes complex!
+#if !TENSORFLOW_USE_SYCL
 absl::Status MakeBatchPointers(se::Stream* stream,
                                se::DeviceMemoryBase base_ptr,
                                size_t stride_bytes, size_t n,
                                se::DeviceMemoryBase ptrs_out);
-
+#else
+absl::Status MakeBatchPointers(se::Stream* stream,
+                               const se::DeviceMemoryBase& base_ptr,
+                               size_t stride_bytes, size_t n,
+                               se::DeviceMemoryBase& ptrs_out);
+#endif
 }  // namespace xla::gpu
-
-#endif  // XLA_SERVICE_GPU_MAKE_BATCH_POINTERS_H_
+#endif  // XLA_SERVICE_GPU_PRECOMPILED_KERNELS_H_
diff --git a/xla/service/gpu/matmul_utils.h b/xla/service/gpu/matmul_utils.h
index 2efbdd386a..f5d69e51fc 100644
--- a/xla/service/gpu/matmul_utils.h
+++ b/xla/service/gpu/matmul_utils.h
@@ -40,6 +40,23 @@ limitations under the License.
 #include "rocm/rocm_config.h"
 #endif
 
+namespace stream_executor {
+namespace cuda {
+namespace BlasLt {
+enum class Epilogue {
+  kDefault = 1,                   // No special postprocessing
+  kReLU = 2,                      // Apply point-wise ReLU function
+  kBias = 4,                      // Add broadcasted bias vector
+  kBiasThenReLU = kBias | kReLU,  // Apply bias and then ReLU transform
+  kGELU = 32,                // Apply GELU point-wise transform to the results
+  kGELUWithAux = 32 | 1024,  // Apply GELU with auxiliary output.
+  kBiasThenGELU = kBias | kGELU,  // Apply bias and then approximate GELU.
+  kBiasThenGELUWithAux = kBiasThenGELU | 1024,
+};
+}
+}  // namespace cuda
+}  // namespace stream_executor
+
 namespace xla {
 namespace gpu {
 
@@ -165,6 +182,8 @@ struct GemmConfig : public se::gpu::GemmConfig {
   absl::StatusOr<DescriptorsTuple> GetMatrixDescriptors(
       se::DeviceMemoryBase lhs_buf, se::DeviceMemoryBase rhs_buf,
       se::DeviceMemoryBase out_buf) const;
+
+  se::cuda::BlasLt::Epilogue epilogue;
 };
 
 // Run the given GEMM instruction `gemm` subject to the configuration
diff --git a/xla/service/gpu/model/gpu_performance_model.cc b/xla/service/gpu/model/gpu_performance_model.cc
index d03686306c..a6940de0ad 100644
--- a/xla/service/gpu/model/gpu_performance_model.cc
+++ b/xla/service/gpu/model/gpu_performance_model.cc
@@ -263,7 +263,12 @@ LaunchDimensions EstimateFusionLaunchDimensions(
       return kernel_emitter->launch_dimensions();
     }
   }
-  int64_t block_size = 128;  // Result for default LaunchDimensionsConfig.
+  // Result for default LaunchDimensionsConfig.
+#if TENSORFLOW_USE_SYCL
+  int64_t block_size = RoundUpTo(device_info.threads_per_block_limit(),int64_t{32});
+#else
+  int64_t block_size = 128;
+#endif
   int64_t num_blocks = CeilOfRatio(estimated_num_threads, block_size);
   return LaunchDimensions(num_blocks, block_size);
 }
diff --git a/xla/service/gpu/nccl_api.h b/xla/service/gpu/nccl_api.h
index 86f19a0d6c..80db1a4b99 100644
--- a/xla/service/gpu/nccl_api.h
+++ b/xla/service/gpu/nccl_api.h
@@ -127,6 +127,11 @@ class NcclApi {
   // https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/api/comms.html#ncclgetuniqueid
   virtual absl::StatusOr<NcclCliqueId> GetUniqueId() = 0;
 
+
+  // Temporary function to get unique id for non-NCCL backend.
+  virtual absl::StatusOr<NcclCliqueId> GetId(const NcclCliqueKey& key,
+                                             const RunId& id) = 0;
+
   // Creates a new communicator.
   //
   // https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/api/comms.html#ncclcomminitrank
diff --git a/xla/service/gpu/nccl_clique.cc b/xla/service/gpu/nccl_clique.cc
index 5ea4d2d556..23539a3fc9 100644
--- a/xla/service/gpu/nccl_clique.cc
+++ b/xla/service/gpu/nccl_clique.cc
@@ -70,8 +70,11 @@ absl::StatusOr<const NcclCliqueIdCallback*> GetNcclCliqueIdCallback(
       << "If non-local devices are taking part of a collective API on "
          "GPU, the nccl_clique_id_callback must be provided by the client.";
 
-  static auto* local_callback = new NcclCliqueIdCallback(
-      [](const NcclCliqueKey&) { return NcclApi::Default()->GetUniqueId(); });
+  static auto* local_callback =
+      new NcclCliqueIdCallback([](const NcclCliqueKey& key, const RunId& id) {
+        // return NcclApi::Default()->GetUniqueId();
+        return NcclApi::Default()->GetId(key, id);
+      });
   return local_callback;
 }
 
@@ -278,7 +281,7 @@ static absl::StatusOr<std::shared_ptr<NcclClique::Lock>> InitializeNcclClique(
   // Creates initialization state for participating ranks.
   auto create_initialization_state = [&](absl::Span<const int32_t* const> ranks)
       -> absl::StatusOr<InitializationState> {
-    TF_ASSIGN_OR_RETURN(auto clique_id, clique_id_callback(clique_key));
+    TF_ASSIGN_OR_RETURN(auto clique_id, clique_id_callback(clique_key, run_id));
     VLOG(3) << "Created unique clique id (hash): " << absl::HashOf(clique_id);
     return InitializationState(clique_id, ranks);
   };
diff --git a/xla/service/gpu/nccl_clique.h b/xla/service/gpu/nccl_clique.h
index 274c56ba7b..8c4e310734 100644
--- a/xla/service/gpu/nccl_clique.h
+++ b/xla/service/gpu/nccl_clique.h
@@ -30,6 +30,7 @@ limitations under the License.
 #include "absl/strings/str_format.h"
 #include "xla/executable_run_options.h"
 #include "xla/service/global_device_id.h"
+#include "xla/service/gpu/ccl_api.h"
 #include "xla/service/gpu/nccl_api.h"
 #include "xla/service/gpu/nccl_clique_key.h"
 #include "xla/service/lockable.h"
diff --git a/xla/service/gpu/nccl_clique_key.h b/xla/service/gpu/nccl_clique_key.h
index ef4df88330..cc0d1d2f5c 100644
--- a/xla/service/gpu/nccl_clique_key.h
+++ b/xla/service/gpu/nccl_clique_key.h
@@ -26,6 +26,7 @@ limitations under the License.
 
 #include "absl/status/statusor.h"
 #include "absl/types/span.h"
+#include "xla/executable_run_options.h"
 #include "xla/service/global_device_id.h"
 
 namespace xla::gpu {
@@ -140,7 +141,8 @@ H AbslHashValue(H h, const NcclCliqueId& id) {
 
 // A callback to get a unique clique id (see `ncclUniqueId` documentation).
 using NcclCliqueIdCallback =  // NOLINT
-    std::function<absl::StatusOr<NcclCliqueId>(const NcclCliqueKey&)>;
+    std::function<absl::StatusOr<NcclCliqueId>(const NcclCliqueKey&,
+                                               const RunId&)>;
 
 }  // namespace xla::gpu
 
diff --git a/xla/service/gpu/parallel_loop_emitter.cc b/xla/service/gpu/parallel_loop_emitter.cc
index 326cdce5d0..4d8365ebc1 100644
--- a/xla/service/gpu/parallel_loop_emitter.cc
+++ b/xla/service/gpu/parallel_loop_emitter.cc
@@ -192,6 +192,7 @@ ParallelLoopEmitter::EmitIndexAndSetExitBasicBlock(absl::string_view loop_name,
     // kernel is unrolled, the following GEP instruction shares the same pointer
     // and sequential indices with others, allowing the default SLP pass to
     // optimize them into vectorized load/store operations.
+
     llvm::Value* linear_index =
         b_->CreateAdd(linear_index_base, llvm::ConstantInt::get(index_type, i),
                       absl::StrCat("linear_index", i),
diff --git a/xla/service/gpu/runtime/BUILD b/xla/service/gpu/runtime/BUILD
index cbadbe8dce..33a2cc0b74 100644
--- a/xla/service/gpu/runtime/BUILD
+++ b/xla/service/gpu/runtime/BUILD
@@ -1,7 +1,7 @@
 load("@local_config_cuda//cuda:build_defs.bzl", "cuda_library")
 load("//xla:xla.bzl", "xla_cc_test")
 load("//xla/service/gpu:build_defs.bzl", "gpu_kernel_library")
-load("//xla/stream_executor:build_defs.bzl", "if_gpu_is_configured")
+load("//xla/stream_executor:build_defs.bzl", "if_gpu_is_configured", "if_cuda_or_rocm")
 load(
     "@local_config_rocm//rocm:build_defs.bzl",
     "if_rocm_is_configured",
@@ -206,7 +206,7 @@ cc_library(
         "//xla/service:executable",
         "//xla/stream_executor:device_memory",
         "@com_google_absl//absl/status",
-    ] + if_gpu_is_configured([
+    ] + if_cuda_or_rocm([
         "//xla/service/gpu/runtime3:cub_sort_thunk",
     ]),
 )
diff --git a/xla/service/gpu/runtime3/BUILD b/xla/service/gpu/runtime3/BUILD
index 35a8ed2be4..df6fd84772 100644
--- a/xla/service/gpu/runtime3/BUILD
+++ b/xla/service/gpu/runtime3/BUILD
@@ -3,6 +3,7 @@ load("//xla/service/gpu:build_defs.bzl", "get_cub_sort_kernel_types")
 load("//xla/stream_executor:build_defs.bzl", "if_gpu_is_configured")
 load("@local_config_rocm//rocm:build_defs.bzl", "if_rocm_is_configured")
 load("@tsl//tsl/platform/default:cuda_build_defs.bzl", "if_cuda_is_configured")
+load("@local_config_sycl//sycl:build_defs.bzl", "if_sycl_is_configured")
 
 package(
     # copybara:uncomment default_applicable_licenses = ["//tensorflow:license"],
@@ -273,6 +274,7 @@ cc_library(
         "@com_google_absl//absl/container:flat_hash_map",
         "@com_google_absl//absl/container:inlined_vector",
         "@com_google_absl//absl/status",
+        "@intel_extension_for_openxla//xla/service/gpu:onednn_gpu_conv_runner",
     ],
 )
 
@@ -354,8 +356,10 @@ cc_library(
         "//xla/hlo/ir:hlo",
         "//xla/service:buffer_assignment",
         "//xla/service/gpu:buffer_allocations",
+        "//xla/service/gpu:cusolver_context",
         "//xla/service/gpu:thunk",
         "//xla/stream_executor",
+        "//xla/stream_executor/gpu:gpu_helpers_header",
         "@com_google_absl//absl/container:flat_hash_map",
         "@com_google_absl//absl/status",
         "@com_google_absl//absl/status:statusor",
@@ -378,6 +382,7 @@ cc_library(
         "//xla/service/gpu:thunk",
         "//xla/stream_executor",
         "@com_google_absl//absl/container:flat_hash_map",
+        "@intel_extension_for_openxla//xla/service/gpu:xetla_gpu_fused_mha_runner",
     ],
 )
 
@@ -392,6 +397,7 @@ cc_library(
         "//xla/service/gpu:thunk",
         "//xla/stream_executor:device_memory",
         "@com_google_absl//absl/status",
+        "@intel_extension_for_openxla//xla/service/gpu:onednn_matmul_utils",
         "@tsl//tsl/platform:logging",
     ],
 )
@@ -410,6 +416,7 @@ cc_library(
         "//xla:status",
         "//xla/stream_executor:device_memory",
         "//xla/stream_executor",
+        "@intel_extension_for_openxla//xla/service/gpu:onednn_matmul_utils",
         "@tsl//tsl/platform:logging",
     ]),
 )
@@ -614,6 +621,7 @@ cc_library(
         "@com_google_absl//absl/strings:str_format",
         "//xla/service/gpu:buffer_allocations",
         "//xla/service/gpu:make_batch_pointers",
+        "//xla/service/gpu:cusolver_context",
         "//xla/service/gpu:thunk",
         "//xla:types",
         "//xla:util",
diff --git a/xla/service/gpu/runtime3/cholesky_thunk.cc b/xla/service/gpu/runtime3/cholesky_thunk.cc
index d6db646821..9d90cb0c7e 100644
--- a/xla/service/gpu/runtime3/cholesky_thunk.cc
+++ b/xla/service/gpu/runtime3/cholesky_thunk.cc
@@ -33,13 +33,12 @@ namespace xla {
 namespace gpu {
 
 namespace {
-
 template <typename T>
 absl::Status DoPotrfBatched(const se::GpuAsmOpts& asm_opts,
                             CholeskyParams* params, se::Stream* stream,
                             GpuSolverContext& context) {
   T* a_base = static_cast<T*>(params->a_buffer.opaque());
-  se::DeviceMemory<int> infos(params->info_buffer);
+
 #if TENSORFLOW_USE_ROCSOLVER
   // hipsolver is not supported so allocate a GPU buffer
   se::ScopedDeviceMemory<T*> ptrs =
@@ -49,6 +48,9 @@ absl::Status DoPotrfBatched(const se::GpuAsmOpts& asm_opts,
   se::DeviceMemory<T*> as(params->workspace_buffer);
 #endif
 
+  se::DeviceMemory<int> infos(params->info_buffer);
+
+#if !TENSORFLOW_USE_SYCL
   CHECK_GE(as.size(), params->batch_size);
   CHECK_GE(infos.size(), params->batch_size);
 
@@ -61,8 +63,11 @@ absl::Status DoPotrfBatched(const se::GpuAsmOpts& asm_opts,
   // Now that we've set up the `as` array, we can call cusolver.
   return context.PotrfBatched(params->uplo, params->n, as, params->n, infos,
                               params->batch_size);
+#else // !TENSORFLOW_USE_SYCL
+  return context.PotrfBatched(params->uplo, params->n, as, params->n, infos,
+                              params->batch_size, a_base);
+#endif // !TENSORFLOW_USE_SYCL
 }
-
 }  // namespace
 
 CholeskyThunk::CholeskyThunk(ThunkInfo thunk_info,
diff --git a/xla/service/gpu/runtime3/cholesky_thunk.h b/xla/service/gpu/runtime3/cholesky_thunk.h
index 26054a194e..91130ad2bd 100644
--- a/xla/service/gpu/runtime3/cholesky_thunk.h
+++ b/xla/service/gpu/runtime3/cholesky_thunk.h
@@ -18,6 +18,7 @@ limitations under the License.
 
 #include <optional>
 
+#include "tsl/platform/status.h"
 #include "xla/hlo/ir/hlo_instruction.h"
 #include "xla/service/buffer_assignment.h"
 #include "xla/service/gpu/buffer_allocations.h"
@@ -28,7 +29,6 @@ limitations under the License.
 #include "xla/stream_executor/stream_executor.h"
 #include "xla/types.h"
 #include "xla/xla_data.pb.h"
-#include "tsl/platform/status.h"
 
 namespace xla {
 namespace gpu {
@@ -74,8 +74,8 @@ struct CholeskyParams {
   se::DeviceMemoryBase workspace_buffer;
   se::DeviceMemoryBase info_buffer;
 };
-absl::Status RunCholesky(const se::GpuAsmOpts& asm_opts, PrimitiveType type,
-                         CholeskyParams* params, se::Stream* stream);
+Status RunCholesky(const se::GpuAsmOpts& asm_opts, PrimitiveType type,
+                   CholeskyParams* params, se::Stream* stream);
 
 }  // namespace gpu
 }  // namespace xla
diff --git a/xla/service/gpu/runtime3/convolution_thunk.cc b/xla/service/gpu/runtime3/convolution_thunk.cc
index 56bd9617ed..a25230da6c 100644
--- a/xla/service/gpu/runtime3/convolution_thunk.cc
+++ b/xla/service/gpu/runtime3/convolution_thunk.cc
@@ -25,11 +25,20 @@ limitations under the License.
 #include "xla/stream_executor/stream_executor.h"
 #include "xla/util.h"
 
+#if TENSORFLOW_USE_SYCL
+#include "xla/service/gpu/onednn_gpu_conv_runner.h"
+#include "xla/stream_executor/scratch_allocator.h"
+#endif
+
 namespace xla {
 namespace gpu {
 
 ConvolutionThunk::ConvolutionThunk(
+#if TENSORFLOW_USE_SYCL
+    ThunkInfo thunk_info, GpuConvDescriptor descriptor,
+#else
     ThunkInfo thunk_info, GpuConvConfig config,
+#endif
     std::vector<BufferAllocation::Slice> operand_slices,
     std::vector<BufferAllocation::Slice> result_slices,
     BufferAllocation::Slice scratch_slice)
@@ -37,8 +46,13 @@ ConvolutionThunk::ConvolutionThunk(
       operand_buffers_(std::move(operand_slices)),
       result_buffers_(std::move(result_slices)),
       scratch_buffer_(scratch_slice),
+#if TENSORFLOW_USE_SYCL
+      descriptor_(std::move(descriptor)) {}
+#else
       config_(std::move(config)) {}
+#endif
 
+#if !TENSORFLOW_USE_SYCL
 GenericConvRunner& ConvolutionThunk::GetOrCreateRunner(
     const stream_executor::Stream* stream) {
   absl::MutexLock lock(&mu_);
@@ -50,6 +64,7 @@ GenericConvRunner& ConvolutionThunk::GetOrCreateRunner(
   }
   return *it->second;
 }
+#endif
 
 absl::Status ConvolutionThunk::ExecuteOnStream(const ExecuteParams& params) {
   const auto& buffer_allocations = *params.buffer_allocations;
@@ -68,13 +83,26 @@ absl::Status ConvolutionThunk::ExecuteOnStream(const ExecuteParams& params) {
   se::DeviceMemoryBase scratch =
       buffer_allocations.GetDeviceAddress(scratch_buffer_);
 
+#if TENSORFLOW_USE_SYCL
+  auto stream = params.stream;
+  se::OwningScratchAllocator<2> scratch_allocator(
+      buffer_allocations.device_ordinal(),
+      buffer_allocations.memory_allocator());
+  TF_ASSIGN_OR_RETURN(auto conv_primitive,
+                      GetOrCreateOneDnnConvPrimitive(stream, descriptor_, operand_se_buffers,
+                                                     result_se_buffers[0], params, &scratch_allocator));
+
+  TF_RETURN_IF_ERROR(RunGpuConv(conv_primitive, descriptor_,
+                                absl::MakeSpan(operand_se_buffers),
+                                result_se_buffers[0], params));
+#else
   RunConvOptions opts;
   opts.runner_cache = &GetOrCreateRunner(params.stream);
 
   TF_RETURN_IF_ERROR(RunGpuConv(config_, absl::MakeSpan(operand_se_buffers),
                                 absl::MakeSpan(result_se_buffers), scratch,
                                 params.stream, opts));
-
+#endif
   // Note: Convolution has a tuple buffer as an output, but we don't need to
   // populate it as no one should be reading from the tuple directly.
   if (!params.stream->ok()) {
diff --git a/xla/service/gpu/runtime3/convolution_thunk.h b/xla/service/gpu/runtime3/convolution_thunk.h
index 5c202a6b71..df9898fc2c 100644
--- a/xla/service/gpu/runtime3/convolution_thunk.h
+++ b/xla/service/gpu/runtime3/convolution_thunk.h
@@ -37,7 +37,12 @@ class ConvolutionThunk : public Thunk {
   // Constructs a thunk for launching a DNN convolution.
   //
   // operand_slices should be in the same order as cudnn_call->operands().
-  ConvolutionThunk(ThunkInfo thunk_info, GpuConvConfig config,
+  ConvolutionThunk(ThunkInfo thunk_info,
+#if TENSORFLOW_USE_SYCL
+                   GpuConvDescriptor descriptor,
+#else
+                   GpuConvConfig config,
+#endif
                    std::vector<BufferAllocation::Slice> operand_slices,
                    std::vector<BufferAllocation::Slice> result_slices,
                    BufferAllocation::Slice scratch_slice);
@@ -54,7 +59,11 @@ class ConvolutionThunk : public Thunk {
   GenericConvRunner& GetOrCreateRunner(const stream_executor::Stream* stream);
 
   // Convolution config
+#if TENSORFLOW_USE_SYCL
+  const GpuConvDescriptor descriptor_;
+#else
   const GpuConvConfig config_;
+#endif
   absl::Mutex mu_;
   absl::flat_hash_map<const stream_executor::Stream*,
                       std::unique_ptr<GenericConvRunner>>
diff --git a/xla/service/gpu/runtime3/custom_call_thunk.cc b/xla/service/gpu/runtime3/custom_call_thunk.cc
index 6329f6764e..51721e1e96 100644
--- a/xla/service/gpu/runtime3/custom_call_thunk.cc
+++ b/xla/service/gpu/runtime3/custom_call_thunk.cc
@@ -36,7 +36,7 @@ limitations under the License.
 #include "xla/stream_executor/device_memory.h"
 #include "xla/util.h"
 
-#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM
+#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM || TENSORFLOW_USE_SYCL
 #include "xla/stream_executor/gpu/gpu_stream.h"
 #endif
 
@@ -89,7 +89,7 @@ absl::Status CustomCallThunk::ExecuteCustomCall(const ExecuteParams& params) {
     }
   }
 
-#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM
+#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM || TENSORFLOW_USE_SYCL
   auto gpu_stream = se::gpu::AsGpuStreamValue(params.stream);
   XlaCustomCallStatus custom_call_status;
   call_target_(gpu_stream, buffers.data(), opaque_.data(), opaque_.size(),
@@ -100,11 +100,11 @@ absl::Status CustomCallThunk::ExecuteCustomCall(const ExecuteParams& params) {
   } else {
     return absl::OkStatus();
   }
-#else   //  GOOGLE_CUDA || TENSORFLOW_USE_ROCM
+#else   //  GOOGLE_CUDA || TENSORFLOW_USE_ROCM || TENSORFLOW_USE_SYCL
   return Unavailable(
       "Custom calls on GPU are not supported in this configuration. Please "
       "build with --config=cuda or --config=rocm");
-#endif  //   GOOGLE_CUDA || TENSORFLOW_USE_ROCM
+#endif  //   GOOGLE_CUDA || TENSORFLOW_USE_ROCM || TENSORFLOW_USE_SYCL
 }
 
 absl::Status CustomCallThunk::ExecuteFfiHandler(const ExecuteParams& params) {
diff --git a/xla/service/gpu/runtime3/custom_call_thunk.h b/xla/service/gpu/runtime3/custom_call_thunk.h
index 55134f5923..fec30bd810 100644
--- a/xla/service/gpu/runtime3/custom_call_thunk.h
+++ b/xla/service/gpu/runtime3/custom_call_thunk.h
@@ -35,7 +35,7 @@ limitations under the License.
 #include "xla/shape.h"
 #include "xla/status.h"
 
-#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM
+#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM || TENSORFLOW_USE_SYCL
 #include "xla/stream_executor/gpu/gpu_types.h"
 #endif
 
@@ -55,11 +55,11 @@ namespace gpu {
 // compiler is allowed to create.
 class CustomCallThunk : public Thunk {
  public:
-#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM
+#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM || TENSORFLOW_USE_SYCL
   using Stream = stream_executor::gpu::GpuStreamHandle;
-#else   //  GOOGLE_CUDA || TENSORFLOW_USE_ROCM
+#else   //  GOOGLE_CUDA || TENSORFLOW_USE_ROCM || TENSORFLOW_USE_SYCL
   using Stream = void*;
-#endif  //  GOOGLE_CUDA || TENSORFLOW_USE_ROCM
+#endif  //  GOOGLE_CUDA || TENSORFLOW_USE_ROCM || TENSORFLOW_USE_SYCL
 
   using CustomCallTarget = std::function<void(Stream, void**, const char*,
                                               size_t, XlaCustomCallStatus*)>;
diff --git a/xla/service/gpu/runtime3/fft_thunk.cc b/xla/service/gpu/runtime3/fft_thunk.cc
index 618eed1a20..1e7ad80ab4 100644
--- a/xla/service/gpu/runtime3/fft_thunk.cc
+++ b/xla/service/gpu/runtime3/fft_thunk.cc
@@ -67,6 +67,7 @@ std::string FftTypeToString(se::fft::Type type) {
   }
 }
 
+
 absl::StatusOr<stream_executor::blas::BlasSupport*> GetBlas(
     se::Stream* stream) {
   auto blas = stream->parent()->AsBlas();
@@ -83,6 +84,7 @@ absl::StatusOr<stream_executor::fft::FftSupport*> GetFft(se::Stream* stream) {
   }
   return fft;
 }
+
 }  // namespace
 
 FftThunk::FftThunk(ThunkInfo thunk_info, FftType fft_type,
diff --git a/xla/service/gpu/runtime3/fused_mha_thunk.cc b/xla/service/gpu/runtime3/fused_mha_thunk.cc
index 731f4a087a..e3590adb5c 100644
--- a/xla/service/gpu/runtime3/fused_mha_thunk.cc
+++ b/xla/service/gpu/runtime3/fused_mha_thunk.cc
@@ -21,6 +21,10 @@ limitations under the License.
 #include "xla/stream_executor/stream_executor.h"
 #include "xla/util.h"
 
+#if TENSORFLOW_USE_SYCL
+#include "xla/service/gpu/xetla_gpu_fused_mha_runner.h"
+#endif
+
 namespace xla {
 namespace gpu {
 
@@ -83,12 +87,19 @@ absl::Status FusedMHAThunk::ExecuteOnStream(const ExecuteParams& params) {
   std::optional<se::DeviceMemoryBase> activation_buffer =
       AssignBufferIfNotNull(buffer_allocations, activation_buffer_);
 
+#if TENSORFLOW_USE_SYCL
+  TF_RETURN_IF_ERROR(RunXetlaGpuFMHA(config_, lhs_bmm1_buffer, rhs_bmm1_buffer,
+                                     rhs_bmm2_buffer, output_buffer, scratch_buffer,
+                                     mask_buffer, bias_buffer,
+                                     activation_buffer, params.stream));
+#else
   RunFusedMHAOptions opts;
   opts.runner_cache = &GetOrCreateRunner(params.stream);
   TF_RETURN_IF_ERROR(RunGpuFMHA(config_, lhs_bmm1_buffer, rhs_bmm1_buffer,
                                 rhs_bmm2_buffer, output_buffer, scratch_buffer,
                                 mask_buffer, bias_buffer, activation_buffer,
                                 params.stream, opts));
+#endif
 
   if (!params.stream->ok()) {
     return Internal("FusedMHAThunk::ExecuteOnStream failed.");
diff --git a/xla/service/gpu/runtime3/gemm_thunk.cc b/xla/service/gpu/runtime3/gemm_thunk.cc
index 88fcb1e72a..8599b2f3ed 100644
--- a/xla/service/gpu/runtime3/gemm_thunk.cc
+++ b/xla/service/gpu/runtime3/gemm_thunk.cc
@@ -24,6 +24,10 @@ limitations under the License.
 #include "xla/stream_executor/device_memory.h"
 #include "tsl/platform/logging.h"
 
+#if TENSORFLOW_USE_SYCL
+#include "xla/service/gpu/onednn_matmul_utils.h"
+#endif
+
 namespace xla {
 namespace gpu {
 
@@ -41,6 +45,7 @@ GemmThunk::GemmThunk(ThunkInfo thunk_info, GemmConfig config,
       workspace_(workspace),
       deterministic_(deterministic) {}
 
+#if !TENSORFLOW_USE_SYCL
 absl::Status GemmThunk::ExecuteOnStream(const ExecuteParams& params) {
   VLOG(3) << "Running GEMM thunk";
   const BufferAllocations& allocs = *params.buffer_allocations;
@@ -57,6 +62,26 @@ absl::Status GemmThunk::ExecuteOnStream(const ExecuteParams& params) {
                  allocs.GetDeviceAddress(output_buffer_), workspace,
                  deterministic_, stream);
 }
+#else // !TENSORFLOW_USE_SYCL
+Status GemmThunk::ExecuteOnStream(const ExecuteParams& params) {
+  VLOG(3) << "Running GEMM thunk";
+  const BufferAllocations& allocs = *params.buffer_allocations;
+  se::DeviceMemoryBase lhs_data = allocs.GetDeviceAddress(lhs_buffer_);
+  se::DeviceMemoryBase rhs_data = allocs.GetDeviceAddress(rhs_buffer_);
+  se::DeviceMemoryBase output_data = allocs.GetDeviceAddress(output_buffer_);
+  se::DeviceMemoryBase add_data;
+  se::DeviceMemoryBase bias_data;
+
+  auto& buffer_allocations = *params.buffer_allocations;
+  se::OwningScratchAllocator<> scratch_allocator(
+      buffer_allocations.device_ordinal(),
+      buffer_allocations.memory_allocator());
+
+  se::gpu::BlasLt::Epilogue epilogue = se::gpu::BlasLt::Epilogue::kDefault;
+  return RunGemm(config_, lhs_data, rhs_data, add_data, output_data, bias_data,
+                 params.stream, epilogue, &scratch_allocator);
+}
+#endif // !TENSORFLOW_USE_SYCL
 
 absl::Status GemmThunk::Initialize(const InitializeParams& params) {
   if (!params.executor->AsBlas()) {
diff --git a/xla/service/gpu/runtime3/gpublas_lt_matmul_thunk.cc b/xla/service/gpu/runtime3/gpublas_lt_matmul_thunk.cc
index ae0c7eb908..4e76b14b38 100644
--- a/xla/service/gpu/runtime3/gpublas_lt_matmul_thunk.cc
+++ b/xla/service/gpu/runtime3/gpublas_lt_matmul_thunk.cc
@@ -24,6 +24,10 @@ limitations under the License.
 #include "xla/stream_executor/scratch_allocator.h"
 #include "tsl/platform/logging.h"
 
+#if TENSORFLOW_USE_SYCL
+#include "xla/service/gpu/onednn_matmul_utils.h"
+#endif
+
 namespace xla {
 namespace gpu {
 
@@ -52,6 +56,36 @@ CublasLtMatmulThunk::CublasLtMatmulThunk(
       d_scale_buffer_(d_scale),
       d_amax_buffer_(d_amax) {}
 
+#if TENSORFLOW_USE_SYCL
+absl::Status CublasLtMatmulThunk::ExecuteOnStream(const ExecuteParams& params) {
+  VLOG(3) << "Running cublas_lt matmul thunk";
+  const BufferAllocations& allocs = *params.buffer_allocations;
+
+  se::DeviceMemoryBase a, b, c, d;
+  if (a_buffer_.allocation() != nullptr) {
+    a = allocs.GetDeviceAddress(a_buffer_);
+  }
+  if (b_buffer_.allocation() != nullptr) {
+    b = allocs.GetDeviceAddress(b_buffer_);
+  }
+  if (c_buffer_.allocation() != nullptr) {
+    c = allocs.GetDeviceAddress(c_buffer_);
+  }
+  if (d_buffer_.allocation() != nullptr) {
+    d = allocs.GetDeviceAddress(d_buffer_);
+  }
+
+  se::DeviceMemoryBase bias, a_scale, b_scale, c_scale, d_scale, d_amax;
+  if (bias_buffer_.allocation() != nullptr) {
+    bias = allocs.GetDeviceAddress(bias_buffer_);
+  }
+
+  se::OwningScratchAllocator<> scratch_allocator(allocs.device_ordinal(),
+                                                 allocs.memory_allocator());
+  return RunGemm(gemm_config_, a, b, c, d, bias, params.stream, epilogue_,
+                 &scratch_allocator);
+}
+#else  // TENSORFLOW_USE_SYCL
 absl::Status CublasLtMatmulThunk::ExecuteOnStream(const ExecuteParams& params) {
   TF_ASSIGN_OR_RETURN(auto plan, GetMatmulPlan(params.stream));
   TF_ASSIGN_OR_RETURN(auto algorithm, GetMatmulAlgorithm(plan));
@@ -118,6 +152,6 @@ CublasLtMatmulThunk::GetMatmulAlgorithm(
   }
   return it->second;
 }
-
+#endif  // TENSORFLOW_USE_SYCL
 }  // namespace gpu
 }  // namespace xla
diff --git a/xla/service/gpu/runtime3/gpublas_lt_matmul_thunk.h b/xla/service/gpu/runtime3/gpublas_lt_matmul_thunk.h
index e7eaf3a359..dd3d729357 100644
--- a/xla/service/gpu/runtime3/gpublas_lt_matmul_thunk.h
+++ b/xla/service/gpu/runtime3/gpublas_lt_matmul_thunk.h
@@ -45,6 +45,7 @@ class CublasLtMatmulThunk : public Thunk {
   absl::Status ExecuteOnStream(const ExecuteParams& params) override;
 
  private:
+#if !TENSORFLOW_USE_SYCL
   absl::StatusOr<se::gpu::BlasLt::MatmulPlan*> GetMatmulPlan(
       const stream_executor::Stream* stream);
   absl::StatusOr<std::optional<se::gpu::BlasLt::MatmulAlgorithm> >
@@ -59,7 +60,7 @@ class CublasLtMatmulThunk : public Thunk {
   absl::flat_hash_map<const se::gpu::BlasLt::MatmulPlan*,
                       se::gpu::BlasLt::MatmulAlgorithm>
       matmul_algorithm_cache_ ABSL_GUARDED_BY(matmul_algorithm_cache_mutex_);
-
+#endif
   GemmConfig gemm_config_;
   se::gpu::BlasLt::Epilogue epilogue_;
   int64_t algorithm_idx_;
diff --git a/xla/service/gpu/spir_compiler.cc b/xla/service/gpu/spir_compiler.cc
new file mode 100644
index 0000000000..efc3aa744a
--- /dev/null
+++ b/xla/service/gpu/spir_compiler.cc
@@ -0,0 +1,273 @@
+/* Copyright (c) 2023 Intel Corporation
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "xla/service/gpu/spir_compiler.h"
+
+#include <stdlib.h>
+
+#include <fstream>
+#include <memory>
+#include <string>
+#include <utility>
+#include <vector>
+
+#include "tsl/platform/path.h"
+#include "tsl/platform/status.h"
+#include "tsl/util/env_var.h"
+#include "xla/hlo/ir/hlo_opcode.h"
+#include "xla/service/algebraic_simplifier.h"
+#include "xla/service/call_inliner.h"
+#include "xla/service/convert_mover.h"
+#include "xla/service/dot_dimension_merger.h"
+#include "xla/service/dump.h"
+#include "xla/service/float_normalization.h"
+#include "xla/service/float_support.h"
+#include "xla/service/gpu/backend_configs.pb.h"
+#include "xla/service/gpu/buffer_sharing.h"
+#include "xla/service/gpu/cublas_cudnn.h"
+#include "xla/service/gpu/cudnn_fused_conv_rewriter.h"
+#include "xla/service/gpu/cudnn_fused_mha_rewriter.h"
+#include "xla/service/gpu/cusolver_rewriter.h"
+#include "xla/service/gpu/gpu_conv_padding_legalization.h"
+#include "xla/service/gpu/gpu_conv_rewriter.h"
+#include "xla/service/gpu/gpu_layout_assignment.h"
+#include "xla/service/gpu/ir_emission_utils.h"
+#include "xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.h"
+#include "xla/service/gpu/move_copy_to_users.h"
+#include "xla/service/gpu/redundant_convert_mover.h"
+#include "xla/service/gpu/target_constants.h"
+#include "xla/service/gpu/triangular_solve_rewriter.h"
+#include "xla/service/hlo_constant_folding.h"
+#include "xla/service/hlo_cse.h"
+#include "xla/service/hlo_dce.h"
+#include "xla/service/hlo_pass_fix.h"
+#include "xla/service/hlo_pass_pipeline.h"
+#include "xla/service/hlo_verifier.h"
+#include "xla/service/layout_normalization.h"
+#include "xla/service/llvm_ir/llvm_util.h"
+#include "xla/service/reshape_decomposer.h"
+#include "xla/service/reshape_mover.h"
+#include "xla/service/tuple_simplifier.h"
+#include "xla/status_macros.h"
+#include "xla/stream_executor/sycl/hw_info.h"
+#include "xla/stream_executor/sycl/sycl_platform_id.h"
+#include "xla/types.h"
+#include "xla/util.h"
+
+namespace xla {
+namespace gpu {
+namespace {
+
+class ConvBfloat16Support : public FloatSupport {
+ public:
+  explicit ConvBfloat16Support()
+      : FloatSupport(BF16), is_conv_bf16_supported_(true) {}
+
+  bool SupportsLowPrecisionOperand(const HloInstruction& hlo,
+                                   int64_t operand_index) const override {
+    return (hlo.opcode() != HloOpcode::kConvolution) || is_conv_bf16_supported_;
+  }
+
+  bool SupportsLowPrecisionOutput(const HloInstruction& hlo) const override {
+    return (hlo.opcode() != HloOpcode::kConvolution) || is_conv_bf16_supported_;
+  }
+
+  bool SupportsMixedPrecisions(const HloInstruction& hlo) const override {
+    // Skip all HLOs other than convolutions.
+    return (hlo.opcode() != HloOpcode::kConvolution);
+  }
+
+ private:
+  bool is_conv_bf16_supported_;
+};
+
+}  // namespace
+
+absl::Status SPIRCompiler::OptimizeHloConvolutionCanonicalization(
+    HloModule* hlo_module, se::GpuComputeCapability gpu_version,
+    se::dnn::VersionInfo dnn_version,
+    se::DeviceMemoryAllocator* device_allocator) {
+  auto cuda_compute_capability =
+      std::get<se::CudaComputeCapability>(gpu_version);
+  // Convert convolutions into CustomCalls to onednn, then canonicalize them
+  // (GpuConvPaddingLegalization). Also expand cuSolver calls.
+  HloPassPipeline pipeline("conv_canonicalization");
+  pipeline.AddInvariantCheckerDebug<HloVerifier>(
+      /*layout_sensitive=*/false,
+      /*allow_mixed_precision=*/false);
+
+  // Convert upsupported bf16 convolutions to f32.
+  ConvBfloat16Support conv_bf16_support;
+  pipeline.AddPass<FloatNormalization>(&conv_bf16_support);
+
+  pipeline.AddPass<GpusolverRewriter>();
+  pipeline.AddPass<GpuConvRewriter>();
+  pipeline.AddPass<CudnnFusedConvRewriter>(cuda_compute_capability);
+  pipeline.AddPass<GpuConvPaddingLegalization>();
+
+  // The conv padding/vectorization passes which we need to get rid of.  They
+  // also leave behind unnecessary tuple/get-tuple-element pairs that
+  // TupleSimplifier fixes.
+  pipeline.AddPass<CallInliner>();
+  pipeline.AddPass<TupleSimplifier>();
+
+  AlgebraicSimplifierOptions algsimp_options =
+      GetAlgebraicSimplifierOptions(hlo_module->config());
+  algsimp_options.set_enable_conv_operand_swap(false);
+  algsimp_options.set_enable_unconditional_reduce_of_concat_replacement(false);
+  pipeline.AddPass<HloPassFix<AlgebraicSimplifier>>(algsimp_options);
+
+  // tf2xla bridge, DepthwiseConvolutionConverter, GpuConvRewriter, and
+  // CudnnSimplifyPadding introduce reshapes and transposes.  Run ReshapeMover
+  // to a fixed point.  Include algsimp because ReshapeMover relies on it.
+  [&, &pipeline = pipeline.AddPass<HloPassFix<HloPassPipeline>>(
+          "reshape_mover_after_conv_canonicalization")] {
+    ReshapeMoverOptions reshape_mover_options;
+    reshape_mover_options.reshape_of_1d_broadcast_is_cheap = true;
+    pipeline.AddPass<HloPassFix<ReshapeMover>>(reshape_mover_options);
+    pipeline.AddPass<AlgebraicSimplifier>(algsimp_options);
+  }();
+
+  // The reshapes and transposes can possibly be eliminated using
+  // AlgebraicSimplifier. ConvertMover and ReshapeMover fight with each other.
+  // ConvertMover wants to move some converts down the graph, but ReshapeMover
+  // wants to move them up the graph. We run ConvertMover and algsimp to a fixed
+  // point.
+  [&, &pipeline = pipeline.AddPass<HloPassFix<HloPassPipeline>>(
+          "simplify_after_conv_canonicalization")] {
+    pipeline.AddPass<ConvertMover>();
+    pipeline.AddPass<AlgebraicSimplifier>(algsimp_options);
+  }();
+
+  // GpuConvRewriter, GpuConvPaddingLegalization and
+  // CudnnConvPadForTensorCores may add instructions which can be simplified
+  // by constant folding.
+  pipeline.AddPass<HloConstantFolding>();
+  TF_RETURN_IF_ERROR(pipeline.Run(hlo_module).status());
+
+  return absl::OkStatus();
+}
+
+absl::Status SPIRCompiler::OptimizeHloPostLayoutAssignment(
+    HloModule* hlo_module, se::StreamExecutor* stream_exec,
+    const CompileOptions& options, const TargetConfig& gpu_target_config,
+    tsl::thread::ThreadPool* thread_pool) {
+  HloPassPipeline pre_pipeline("spir post-layout_assignment part 1");
+
+  // This needs to run before GemmRewriter, which is part of
+  // OptimizeHloPostLayoutAssignment().
+  auto cuda_compute_capability = std::get<se::CudaComputeCapability>(
+      gpu_target_config.device_description.gpu_compute_capability());
+
+  bool use_mha = true;
+  TF_CHECK_OK(tsl::ReadBoolFromEnvVar("MHA", true, &use_mha));
+  if (use_mha && IsXetlaHardwareSupport()) {
+    HloPassPipeline mha_fusion_pipeline("multi-headed attention fusion");
+    const DebugOptions& debug_options = hlo_module->config().debug_options();
+    // The LayoutAssignment pass may leave behind kCopy instructions which are
+    // duplicate or NOPs, so remove them with algebraic simplification and CSE.
+    AlgebraicSimplifierOptions alg_sim_options;
+    alg_sim_options.set_supports_non_canonical_dots(false);
+    alg_sim_options.set_is_layout_sensitive(true);
+    alg_sim_options.set_enable_conv_operand_swap(false);
+    // "slow" minmax means we propagate nan.
+    alg_sim_options.set_minmax_propagate_nan(
+        !hlo_module->config().debug_options().xla_gpu_enable_fast_min_max());
+    alg_sim_options.set_enable_unconditional_reduce_of_concat_replacement(
+        false);
+    if (debug_options.xla_gpu_normalize_layouts()) {
+      mha_fusion_pipeline.AddPass<ReshapeDecomposer>();
+      mha_fusion_pipeline.AddPass<HloPassFix<MoveCopyToUsers>>();
+      mha_fusion_pipeline.AddPass<LayoutNormalization>();
+    }
+    mha_fusion_pipeline.AddPass<HloCSE>(/*is_layout_sensitive=*/true);
+    mha_fusion_pipeline.AddPass<HloPassFix<AlgebraicSimplifier>>(
+        alg_sim_options);
+    mha_fusion_pipeline.AddPass<HloCSE>(/*is_layout_sensitive=*/true);
+
+    // Rewrite Multi-Headed Attention modules to Fused MHA custom-calls.
+    mha_fusion_pipeline.AddPass<RedundantConvertMover>();
+    mha_fusion_pipeline.AddPass<HloDCE>();
+    mha_fusion_pipeline.AddPass<CudnnFusedMHARewriter>(
+        cuda_compute_capability, stream_exec);
+    mha_fusion_pipeline.AddPass<AlgebraicSimplifier>(alg_sim_options);
+    mha_fusion_pipeline.AddPass<HloDCE>();
+    mha_fusion_pipeline.AddPass<HloCSE>(/*is_layout_sensitive=*/true,
+                                        /*only_fusion_computations*/ false);
+    TF_RETURN_IF_ERROR(mha_fusion_pipeline.Run(hlo_module).status());
+  }
+
+  pre_pipeline.AddPass<DotDimensionMerger>();
+
+  // Padding a gemm operand that's a constant results in pad(constant).  Run
+  // constant-folding to simplify this into a new constant.
+  pre_pipeline.AddPass<HloConstantFolding>();
+  TF_RETURN_IF_ERROR(pre_pipeline.Run(hlo_module).status());
+
+  TF_RETURN_IF_ERROR(GpuCompiler::OptimizeHloPostLayoutAssignment(
+      hlo_module, stream_exec, options, gpu_target_config, thread_pool));
+
+  HloPassPipeline post_pipeline("spir post-layout_assignment part 2");
+
+  // Transform TriangularSolve ops into custom-calls, so we can add temp
+  // memory.
+  post_pipeline.AddPass<TriangularSolveRewriter>();
+
+  TF_RETURN_IF_ERROR(post_pipeline.Run(hlo_module).status());
+
+  return absl::OkStatus();
+}
+
+
+SPIRCompiler::SPIRCompiler()
+    : GpuCompiler(stream_executor::sycl::kSyclPlatformId, spir::TargetTriple(),
+                  spir::DataLayout()) {}
+
+HloDataflowAnalysis::CanShareBuffer SPIRCompiler::GetCanShareBuffer() const {
+  return &CanShareBufferHint;
+}
+
+absl::StatusOr<GpuCompiler::BackendCompileResult>
+SPIRCompiler::CompileTargetBinary(const HloModuleConfig& module_config,
+                                  llvm::Module* llvm_module,
+                                  se::GpuComputeCapability gpu_version,
+                                  bool relocatable,
+                                  const HloModule* debug_module,
+                                  const CompileOptions& options) {
+  if (relocatable) {
+    return Unimplemented("relocatable target binary is not implemented");
+  }
+
+  std::vector<uint8_t> spir;
+  {
+    // This may print multiple lines per HLO compilation because of the
+    // parallelized compilation of LLVM modules.
+    XLA_SCOPED_LOGGING_TIMER_IF(
+        "SPIRCompiler::CompileTargetBinary - CompileToSpir",
+        !options.is_autotuning_compilation);
+    TF_ASSIGN_OR_RETURN(spir, spir::CompileToSpir(llvm_module, gpu_version,
+                                                  module_config.debug_options()));
+  }
+
+  return BackendCompileResult{"", std::move(spir)};
+}
+
+/*static*/ SPIRCompiler* SPIRCompiler::CreateSPIRCompiler() {
+  static auto compiler = absl::make_unique<SPIRCompiler>();
+  return compiler.get();
+}
+
+}  // namespace gpu
+}  // namespace xla
diff --git a/xla/service/gpu/spir_compiler.h b/xla/service/gpu/spir_compiler.h
new file mode 100644
index 0000000000..c4ceec8e3f
--- /dev/null
+++ b/xla/service/gpu/spir_compiler.h
@@ -0,0 +1,65 @@
+/* Copyright (c) 2023 Intel Corporation
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef XLA_SERVICE_GPU_SPIR_COMPILER_H_
+#define XLA_SERVICE_GPU_SPIR_COMPILER_H_
+
+#include <string>
+#include <utility>
+#include <vector>
+
+#include "absl/base/call_once.h"
+#include "llvm/IRReader/IRReader.h"
+#include "llvm/Support/SourceMgr.h"
+#include "xla/service/gpu/gpu_compiler.h"
+#include "xla/statusor.h"
+
+namespace xla {
+namespace gpu {
+
+// SPIRCompiler generates efficient GPU executables for NVPTX target.
+class SPIRCompiler : public GpuCompiler {
+ public:
+  SPIRCompiler();
+  ~SPIRCompiler() override {}
+
+  absl::Status OptimizeHloConvolutionCanonicalization(
+      HloModule* hlo_module, se::GpuComputeCapability gpu_version,
+      se::dnn::VersionInfo dnn_version,
+      se::DeviceMemoryAllocator* device_allocator) override;
+
+  absl::Status OptimizeHloPostLayoutAssignment(
+      HloModule* hlo_module, se::StreamExecutor* stream_exec,
+      const CompileOptions& options, const TargetConfig& gpu_target_config,
+      tsl::thread::ThreadPool* thread_pool) override;
+
+  HloDataflowAnalysis::CanShareBuffer GetCanShareBuffer() const override;
+
+  absl::StatusOr<BackendCompileResult> CompileTargetBinary(
+      const HloModuleConfig& module_config, llvm::Module* llvm_module,
+      se::GpuComputeCapability gpu_version, bool relocatable,
+      const HloModule* debug_module, const CompileOptions& options) override;
+
+  static SPIRCompiler* CreateSPIRCompiler();
+
+ private:
+  SPIRCompiler(const SPIRCompiler&) = delete;
+  SPIRCompiler& operator=(const SPIRCompiler&) = delete;
+};
+
+}  // namespace gpu
+}  // namespace xla
+
+#endif  // XLA_SERVICE_GPU_SPIR_COMPILER_H_
diff --git a/xla/service/gpu/spir_compiler_registration.cc b/xla/service/gpu/spir_compiler_registration.cc
new file mode 100644
index 0000000000..a397ae7a17
--- /dev/null
+++ b/xla/service/gpu/spir_compiler_registration.cc
@@ -0,0 +1,27 @@
+/* Copyright (c) 2023 Intel Corporation
+
+Copyright 2019 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "xla/service/gpu/spir_compiler.h"
+#include "xla/stream_executor/sycl/sycl_platform_id.h"
+
+static bool InitCompilerModule() {
+  xla::Compiler::RegisterCompilerFactory(
+      stream_executor::sycl::kSyclPlatformId,
+      []() { return std::make_unique<xla::gpu::SPIRCompiler>(); });
+  return true;
+}
+static bool compiler_module_initialized = InitCompilerModule();
diff --git a/xla/service/gpu/stream_executor_util.cc b/xla/service/gpu/stream_executor_util.cc
index 60c6c9fec0..5e56adbd8b 100644
--- a/xla/service/gpu/stream_executor_util.cc
+++ b/xla/service/gpu/stream_executor_util.cc
@@ -142,6 +142,13 @@ StreamExecutorConvLayoutsToXlaLayouts(const ConvolutionDimensionNumbers& dnums,
                            dnums.kernel_spatial_dimensions().end());
       filter_layout.push_back(dnums.kernel_input_feature_dimension());
       break;
+    case FilterLayout::kYXInputOutput:  // HWIO
+      filter_layout.insert(filter_layout.end(),
+                           dnums.kernel_spatial_dimensions().begin(),
+                           dnums.kernel_spatial_dimensions().end());
+      filter_layout.push_back(dnums.kernel_input_feature_dimension());
+      filter_layout.push_back(dnums.kernel_output_feature_dimension());
+      break;
     default:
       return Internal("Invalid filter layout %s for conv with dnums %s,",
                       FilterLayoutString(filter),
@@ -179,7 +186,7 @@ XlaConvShapesToStreamExecutorLayouts(const ConvolutionDimensionNumbers& dnums,
   Layout nhwc_input, nhwc_filter, nhwc_output;
   std::tie(nhwc_input, nhwc_filter, nhwc_output) =
       StreamExecutorConvLayoutsToXlaLayouts(dnums, DataLayout::kBatchYXDepth,
-                                            FilterLayout::kOutputYXInput,
+                                            FilterLayout::kYXInputOutput,
                                             DataLayout::kBatchYXDepth)
           .value();
 
@@ -228,7 +235,7 @@ XlaConvShapesToStreamExecutorLayouts(const ConvolutionDimensionNumbers& dnums,
           ConvolutionDimensionNumbersToString(dnums), vect_size);
     }
   } else if (LayoutUtil::Equal(filter.layout(), nhwc_filter)) {
-    filter_layout = FilterLayout::kOutputYXInput;
+    filter_layout = FilterLayout::kYXInputOutput;
   } else {
     return Internal(
         "Invalid filter layout %s for conv with dnums %s, expected one of (%s, "
@@ -331,7 +338,8 @@ absl::StatusOr<std::unique_ptr<se::Kernel>> CreateKernel(
 
   if (!cubin_data.empty()) {
     loader_spec.AddCudaCubinInMemory(
-        reinterpret_cast<const char*>(cubin_data.data()), kernel_name);
+        reinterpret_cast<const char*>(cubin_data.data()), cubin_data.size(),
+        kernel_name);
   }
 
   auto kernel_base = std::make_unique<se::Kernel>(stream_exec);
diff --git a/xla/service/gpu/stream_executor_util.h b/xla/service/gpu/stream_executor_util.h
index ef418f5274..8cc86e92d3 100644
--- a/xla/service/gpu/stream_executor_util.h
+++ b/xla/service/gpu/stream_executor_util.h
@@ -109,6 +109,9 @@ absl::StatusOr<se::dnn::FusedMHAKind> GetDNNFusedMHAKindFromCudnnfMHAKind(
 absl::StatusOr<se::dnn::DataType> GetDNNDataTypeFromPrimitiveType(
     PrimitiveType type);
 
+StatusOr<se::dnn::FusedMHAKind> GetDNNFusedMHAKindFromCudnnfMHAKind(
+    CudnnfMHAKind kind);
+
 // Returns result with the smallest time which has not failed.
 // If deterministic output is requested, returns first (not failing) result.
 absl::StatusOr<AutotuneResult> PickBestResult(
diff --git a/xla/service/gpu/target_constants.h b/xla/service/gpu/target_constants.h
index 13190ae690..574a4120d7 100644
--- a/xla/service/gpu/target_constants.h
+++ b/xla/service/gpu/target_constants.h
@@ -67,7 +67,7 @@ inline const char* DataLayout() {
       "e-p:64:64:64-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-f32:"
       "32:32-f64:64:64-v16:16:16-v24:32:32-v32:32:32-v48:64:64-v64:64:64-v96:"
       "128:128-v128:128:128-v192:256:256-v256:256:256-v512:512:512-v1024:1024:"
-      "1024";
+      "1024-n8:16:32:64";
   return kDataLayout;
 }
 }  // namespace spir
diff --git a/xla/service/gpu/target_util.cc b/xla/service/gpu/target_util.cc
index 15d0799b00..8ad26a2dd0 100644
--- a/xla/service/gpu/target_util.cc
+++ b/xla/service/gpu/target_util.cc
@@ -23,12 +23,12 @@ limitations under the License.
 #include "llvm/IR/IntrinsicsAMDGPU.h"
 #include "llvm/IR/IntrinsicsNVPTX.h"
 #include "llvm/IR/MDBuilder.h"
+#include "tsl/platform/logging.h"
 #include "xla/hlo/ir/hlo_opcode.h"
 #include "xla/primitive_util.h"
 #include "xla/service/llvm_ir/llvm_type_conversion_util.h"
 #include "xla/service/llvm_ir/llvm_util.h"
 #include "xla/status.h"
-#include "tsl/platform/logging.h"
 
 namespace xla {
 namespace gpu {
diff --git a/xla/service/gpu/thunk.cc b/xla/service/gpu/thunk.cc
index 14c0d43772..7d1fee9e37 100644
--- a/xla/service/gpu/thunk.cc
+++ b/xla/service/gpu/thunk.cc
@@ -243,6 +243,7 @@ Thunk::ExecuteParams::ExecuteParams(
     CASE(kTriangularSolve);
     CASE(kWhile);
     CASE(kFusedMHA);
+    CASE(kFusedQKV);
     CASE(kWaitForStreams);
   }
 }
diff --git a/xla/service/gpu/thunk.h b/xla/service/gpu/thunk.h
index 33626190b6..a2f5db1ff1 100644
--- a/xla/service/gpu/thunk.h
+++ b/xla/service/gpu/thunk.h
@@ -129,6 +129,7 @@ class Thunk {
     kTriangularSolve,
     kWhile,
     kFusedMHA,
+    kFusedQKV,
     kWaitForStreams
   };
 
diff --git a/xla/service/layout_normalization_test.cc b/xla/service/layout_normalization_test.cc
index 6cebe0f2a8..e743e1a22b 100644
--- a/xla/service/layout_normalization_test.cc
+++ b/xla/service/layout_normalization_test.cc
@@ -597,6 +597,31 @@ ENTRY main {
   )");
 }
 
+TEST_F(LayoutNormalizationTest, ConstantAvoidRevisitOfUser) {
+  const char* hlo = R"(
+HloModule module
+
+ENTRY main {
+  c = f32[5,4]{0,1} constant({...})
+  s = f32[5,4]{0,1} sine(c)
+  t = f32[5,4]{0,1} tanh(s)
+  ROOT o = f32[5,4]{0,1} add(s, t)
+}
+)";
+  // If we allowed visiting the normalized user 's' of the constant, we would
+  // run into a CHECK failure, because the constant was normalized in-place and
+  // therefore would not be revisited.
+  CheckLayoutNormalization(hlo, R"(
+// CHECK: [[constant_2:%[^ ]+]] = f32[4,5]{1,0} constant({...})
+// CHECK-NEXT: [[sine:%[^ ]+]] = f32[4,5]{1,0} sine([[constant_2]])
+// CHECK-NEXT: [[bitcast_1:%[^ ]+]] = f32[5,4]{0,1} bitcast([[sine]])
+// CHECK-NEXT: [[bitcast_2:%[^ ]+]] = f32[4,5]{1,0} bitcast([[bitcast_1]])
+// CHECK-NEXT: [[tanh:%[^ ]+]] = f32[4,5]{1,0} tanh([[bitcast_2]])
+// CHECK-NEXT: [[add_3:%[^ ]+]] = f32[4,5]{1,0} add([[bitcast_2]], [[tanh]])
+// CHECK-NEXT: ROOT [[bitcast_3_4:%[^ ]+]] = f32[5,4]{0,1} bitcast([[add_3]])
+  )");
+}
+
 TEST_F(LayoutNormalizationTest, Slice) {
   const char* hlo = R"(
 HloModule module
diff --git a/xla/service/llvm_ir/fused_ir_emitter.cc b/xla/service/llvm_ir/fused_ir_emitter.cc
index d15499c60a..57d876d1bd 100644
--- a/xla/service/llvm_ir/fused_ir_emitter.cc
+++ b/xla/service/llvm_ir/fused_ir_emitter.cc
@@ -23,6 +23,7 @@ limitations under the License.
 #include "llvm/IR/IRBuilder.h"
 #include "llvm/IR/Module.h"
 #include "llvm/IR/Value.h"
+#include "llvm/TargetParser/Triple.h"
 #include "xla/hlo/ir/hlo_computation.h"
 #include "xla/hlo/ir/hlo_instruction.h"
 #include "xla/hlo/ir/hlo_opcode.h"
@@ -97,6 +98,8 @@ FusedIrEmitter::IndexedGenerator FusedIrEmitter::HandleConstant(
 
   llvm::Constant* initializer =
       llvm_ir::ConvertLiteralToIrConstant(constant.literal(), module);
+  // SYCL: Hardcode to global addrspace
+  int addrspace = llvm::Triple(module->getTargetTriple()).isSPIR() ? 1 : 0;
   llvm::GlobalVariable* global = new llvm::GlobalVariable(
       *b->GetInsertBlock()->getModule(), initializer->getType(),
       /*isConstant=*/true,
@@ -104,7 +107,7 @@ FusedIrEmitter::IndexedGenerator FusedIrEmitter::HandleConstant(
       /*Initializer=*/initializer,
       /*Name=*/"", /*InsertBefore=*/nullptr,
       /*TLMode=*/llvm::GlobalValue::NotThreadLocal,
-      /*AddressSpace=*/0,
+      /*AddressSpace=*/addrspace,
       /*isExternallyInitialized=*/false);
   global->setUnnamedAddr(llvm::GlobalVariable::UnnamedAddr::Global);
 
diff --git a/xla/service/llvm_ir/ir_array.cc b/xla/service/llvm_ir/ir_array.cc
index 25785d175b..30ffbb7c95 100644
--- a/xla/service/llvm_ir/ir_array.cc
+++ b/xla/service/llvm_ir/ir_array.cc
@@ -26,6 +26,7 @@ limitations under the License.
 #include "llvm/IR/Instructions.h"
 #include "llvm/IR/Type.h"
 #include "llvm/IR/Value.h"
+#include "tsl/platform/logging.h"
 #include "xla/layout_util.h"
 #include "xla/permutation_util.h"
 #include "xla/primitive_util.h"
@@ -35,7 +36,6 @@ limitations under the License.
 #include "xla/statusor.h"
 #include "xla/util.h"
 #include "xla/xla_data.pb.h"
-#include "tsl/platform/logging.h"
 
 namespace xla {
 namespace llvm_ir {
@@ -564,7 +564,6 @@ llvm::Value* IrArray::EmitLinearArrayElementAddress(
       llvm::Value* index_operand_1 = linear_index->getOperand(1);
       llvm::Value* ptr_address =
           b->CreateGEP(type, base_ptr_, index_operand_0, "");
-
       return b->CreateInBoundsGEP(type, ptr_address, index_operand_1,
                                   llvm_ir::AsStringRef(name));
     } else {
diff --git a/xla/service/llvm_ir/llvm_util.cc b/xla/service/llvm_ir/llvm_util.cc
index ece29e6383..e58250677c 100644
--- a/xla/service/llvm_ir/llvm_util.cc
+++ b/xla/service/llvm_ir/llvm_util.cc
@@ -50,6 +50,7 @@ limitations under the License.
 #include "llvm/Support/Casting.h"
 #include "llvm/Support/CodeGen.h"
 #include "llvm/Support/raw_ostream.h"
+#include "llvm/TargetParser/Triple.h"
 #include "llvm/Transforms/Utils/Cloning.h"
 #include "mlir/IR/BuiltinOps.h"  // from @llvm-project
 #include "mlir/IR/Location.h"  // from @llvm-project
@@ -420,8 +421,13 @@ llvm::AllocaInst* EmitAllocaAtFunctionEntryWithCount(llvm::Type* type,
   llvm::Function* function = b->GetInsertBlock()->getParent();
   b->SetInsertPoint(&function->getEntryBlock(),
                     function->getEntryBlock().getFirstInsertionPt());
+  // SYCL: Fix atomic issue by allocating on private addrspace
+  int addrspace =
+      llvm::Triple(b->GetInsertBlock()->getModule()->getTargetTriple()).isSPIR()
+          ? 5
+          : 0;
   llvm::AllocaInst* alloca =
-      b->CreateAlloca(type, element_count, AsStringRef(name));
+      b->CreateAlloca(type, addrspace, element_count, AsStringRef(name));
   if (alignment != 0) {
     alloca->setAlignment(llvm::Align(alignment));
   }
@@ -540,6 +546,7 @@ void SetDereferenceableMetadataForLoad(llvm::LoadInst* load,
 
 llvm::Instruction* AddRangeMetadata(int32_t lower, int32_t upper,
                                     llvm::Instruction* inst) {
+  /* SYCL: This range is for NVPTX target only.
   llvm::LLVMContext& context = inst->getParent()->getContext();
   llvm::IntegerType* i32 = llvm::Type::getInt32Ty(context);
   inst->setMetadata(
@@ -548,6 +555,7 @@ llvm::Instruction* AddRangeMetadata(int32_t lower, int32_t upper,
           context,
           {llvm::ConstantAsMetadata::get(llvm::ConstantInt::get(i32, lower)),
            llvm::ConstantAsMetadata::get(llvm::ConstantInt::get(i32, upper))}));
+  */
   return inst;
 }
 
diff --git a/xla/stream_executor/build_defs.bzl b/xla/stream_executor/build_defs.bzl
index a29f7d28d6..e77a9b6f3b 100644
--- a/xla/stream_executor/build_defs.bzl
+++ b/xla/stream_executor/build_defs.bzl
@@ -2,6 +2,7 @@
 
 load("@local_config_cuda//cuda:build_defs.bzl", "if_cuda_is_configured")
 load("@local_config_rocm//rocm:build_defs.bzl", "if_rocm_is_configured")
+load("@local_config_sycl//sycl:build_defs.bzl", "if_sycl_is_configured")
 
 def stream_executor_friends():
     return ["//..."]
@@ -18,10 +19,10 @@ def tf_additional_cudnn_plugin_copts():
 
 # Returns whether any GPU backend is configuered.
 def if_gpu_is_configured(x):
-    return if_cuda_is_configured(x) + if_rocm_is_configured(x)
+    return if_cuda_is_configured(x) + if_rocm_is_configured(x) + if_sycl_is_configured(x)
 
 def if_cuda_or_rocm(x):
-    return if_gpu_is_configured(x)
+    return if_cuda_is_configured(x) + if_rocm_is_configured(x)
 
 # nvlink is not available via the pip wheels, disable it since it will create
 # unnecessary dependency
diff --git a/xla/stream_executor/cuda/cuda_driver.cc b/xla/stream_executor/cuda/cuda_driver.cc
index 36662e6cc7..77335a4e68 100644
--- a/xla/stream_executor/cuda/cuda_driver.cc
+++ b/xla/stream_executor/cuda/cuda_driver.cc
@@ -1400,6 +1400,14 @@ struct BitPatternToValue {
       "Feature not supported on CUDA platform (LoadHsaco)");
 }
 
+/* static */ absl::Status GpuDriver::LoadLevelzero(GpuContext* context,
+                                                  const char* spir_contents,
+                                                  const size_t size,
+                                                  CUmodule* module) {
+  return absl::InternalError(
+      "Feature not supported on CUDA platform (LoadLevelzero)");
+}
+
 /* static */ absl::Status GpuDriver::SynchronousMemsetUint8(
     GpuContext* context, CUdeviceptr location, uint8_t value, size_t size) {
   ScopedActivateContext activation(context);
diff --git a/xla/stream_executor/cuda/cuda_executor.cc b/xla/stream_executor/cuda/cuda_executor.cc
index 2b3f6d414b..9ef02730ac 100644
--- a/xla/stream_executor/cuda/cuda_executor.cc
+++ b/xla/stream_executor/cuda/cuda_executor.cc
@@ -190,6 +190,12 @@ absl::Status GpuExecutor::LoadModuleFromHsaco(const char* hsaco,
       "Feature not supported on CUDA platform (LoadModuleFromHsaco)");
 }
 
+absl::Status GpuExecutor::LoadModuleFromSpir(const char* spir, const size_t size,
+                                             CUmodule* module) {
+  return absl::InternalError(
+      "Feature not supported on CUDA platform (LoadModuleFromSpir)");
+}
+
 absl::Status GpuExecutor::GetKernel(const MultiKernelLoaderSpec& spec,
                                     Kernel* kernel) {
   GpuKernel* cuda_kernel = AsGpuKernel(kernel);
diff --git a/xla/stream_executor/device_memory.h b/xla/stream_executor/device_memory.h
index c201afc765..29a99d4c8b 100644
--- a/xla/stream_executor/device_memory.h
+++ b/xla/stream_executor/device_memory.h
@@ -103,9 +103,6 @@ class DeviceMemoryBase {
         reinterpret_cast<std::byte *>(opaque_) + offset_bytes, size_bytes);
   }
 
- protected:
-  friend class StreamExecutor;
-
   // Resets the internal values of the opaque pointer and number of bytes in the
   // memory region, just as in the constructor.
   void Reset(void *opaque, uint64_t bytes) {
@@ -113,6 +110,10 @@ class DeviceMemoryBase {
     size_ = bytes;
   }
 
+ protected:
+  friend class StreamExecutor;
+
+
  private:
   // Platform-dependent value representing allocated memory.
   //
diff --git a/xla/stream_executor/gpu/BUILD b/xla/stream_executor/gpu/BUILD
index e4a4e3ac0f..1b3f39ba65 100644
--- a/xla/stream_executor/gpu/BUILD
+++ b/xla/stream_executor/gpu/BUILD
@@ -26,6 +26,10 @@ load(
     "if_rocm",
     "if_rocm_is_configured",
 )
+load(
+    "@local_config_sycl//sycl:build_defs.bzl",
+    "if_sycl_is_configured",
+)
 load(
     "@tsl//tsl:tsl.bzl",
     "if_libtpu",
@@ -179,6 +183,8 @@ cc_library(
     ]) + if_rocm_is_configured([
         "//xla/stream_executor/rocm:hip_conditional_kernels",
         "//xla/stream_executor/rocm:hip_noop_kernel",
+    ]) + if_sycl_is_configured([
+        "@intel_extension_for_openxla//xla/stream_executor/sycl:sycl_conditional_kernels",
     ]),
 )
 
@@ -362,6 +368,8 @@ cc_library(
         "@local_config_cuda//cuda:cuda_headers",
     ]) + if_rocm_is_configured([
         "@local_config_rocm//rocm:rocm_headers",
+    ]) + if_sycl_is_configured([
+        "@local_config_sycl//sycl:sycl_headers",
     ]),
 )
 
@@ -422,7 +430,7 @@ cc_library(
     name = "asm_compiler",
     srcs = if_gpu_is_configured(["asm_compiler.cc"]),
     hdrs = if_gpu_is_configured(["asm_compiler.h"]),
-    copts = tsl_copts(),
+    copts = tsl_copts(allow_exceptions=True),
     local_defines = if_cuda_is_configured(["GOOGLE_CUDA=1"]),
     visibility = set_external_visibility([
         "//third_party/py/jax:__subpackages__",
diff --git a/xla/stream_executor/gpu/gpu_driver.h b/xla/stream_executor/gpu/gpu_driver.h
index 8124a3c20f..f9a51732c9 100644
--- a/xla/stream_executor/gpu/gpu_driver.h
+++ b/xla/stream_executor/gpu/gpu_driver.h
@@ -628,6 +628,9 @@ class GpuDriver {
   // (supported on ROCm only)
   static absl::Status LoadHsaco(GpuContext* context, const char* hsaco_contents,
                                 GpuModuleHandle* module);
+  static absl::Status LoadLevelzero(GpuContext* context,
+                                    const char* spir_contents, const size_t size,
+                                    GpuModuleHandle* module);
 
   // Retrieves a named kernel from a loaded module, and places the resulting
   // handle into function (outparam) on success. Neither kernel_name nor
diff --git a/xla/stream_executor/gpu/gpu_executor.h b/xla/stream_executor/gpu/gpu_executor.h
index 88b7228696..70efb602b5 100644
--- a/xla/stream_executor/gpu/gpu_executor.h
+++ b/xla/stream_executor/gpu/gpu_executor.h
@@ -341,6 +341,11 @@ class GpuExecutor : public internal::StreamExecutorInterface {
   absl::Status LoadModuleFromHsaco(const char* hsaco, GpuModuleHandle* module)
       TF_EXCLUSIVE_LOCKS_REQUIRED(in_memory_modules_mu_);
 
+  // (supported on SYCL only)
+  absl::Status LoadModuleFromSpir(const char* spir, const size_t size,
+                                  GpuModuleHandle* module)
+      TF_EXCLUSIVE_LOCKS_REQUIRED(in_memory_modules_mu_);
+
   absl::Status Launch(Stream* stream, const ThreadDim& thread_dims,
                       const BlockDim& block_dims,
                       const std::optional<ClusterDim>& cluster_dims,
diff --git a/xla/stream_executor/gpu/gpu_helpers.h b/xla/stream_executor/gpu/gpu_helpers.h
index 16f18454ff..cd2de4ea1d 100644
--- a/xla/stream_executor/gpu/gpu_helpers.h
+++ b/xla/stream_executor/gpu/gpu_helpers.h
@@ -52,14 +52,18 @@ T* GpuMemoryMutable(DeviceMemory<T>* mem) {
 static_assert(
     sizeof(std::complex<float>) == sizeof(GpuComplexType),
     "std::complex<float> and GpuComplexType should have the same size");
+#if !TENSORFLOW_USE_SYCL
 static_assert(offsetof(GpuComplexType, x) == 0,
               "The real part of GpuComplexType should appear first.");
+#endif // !TENSORFLOW_USE_SYCL
 static_assert(
     sizeof(std::complex<double>) == sizeof(GpuDoubleComplexType),
     "std::complex<double> and GpuDoubleComplexType should have the same "
     "size");
+#if !TENSORFLOW_USE_SYCL
 static_assert(offsetof(GpuDoubleComplexType, x) == 0,
               "The real part of GpuDoubleComplexType should appear first.");
+#endif // !TENSORFLOW_USE_SYCL
 
 // Type traits to get CUDA complex types from std::complex<>.
 
diff --git a/xla/stream_executor/gpu/gpu_types.h b/xla/stream_executor/gpu/gpu_types.h
index c8d6266b35..c552f1acbd 100644
--- a/xla/stream_executor/gpu/gpu_types.h
+++ b/xla/stream_executor/gpu/gpu_types.h
@@ -18,7 +18,18 @@ limitations under the License.
 #ifndef XLA_STREAM_EXECUTOR_GPU_GPU_TYPES_H_
 #define XLA_STREAM_EXECUTOR_GPU_GPU_TYPES_H_
 
-#if TENSORFLOW_USE_ROCM
+#if TENSORFLOW_USE_SYCL
+
+#if __has_include(<sycl/sycl.hpp>)
+#include <sycl/sycl.hpp>
+#elif __has_include(<CL/sycl.hpp>)
+#include <CL/sycl.hpp>
+#else
+#error "Unsupported compiler"
+#endif
+#include <level_zero/ze_api.h>
+
+#elif TENSORFLOW_USE_ROCM
 
 #define __HIP_DISABLE_CPP_FUNCTIONS__
 
@@ -40,7 +51,34 @@ namespace gpu {
 // current CUDA/HIP version.
 struct UnsupportedGpuFeature {};
 
-#if TENSORFLOW_USE_ROCM
+#if TENSORFLOW_USE_SYCL
+typedef struct SYCLEventWrapper {
+  ::sycl::event* event;
+  ::sycl::queue* queue;
+} EventWrapper;
+
+using GpuContextHandle = const void*;
+using GpuStreamHandle = ::sycl::queue*;
+using GpuEventHandle = EventWrapper*;
+using GpuFunctionHandle = ::sycl::kernel*;
+using GpuFunctionAttribute = const void*;
+using GpuDeviceHandle = ::sycl::device*;
+using GpuDevicePtr = void*;
+using GpuDeviceAttribute = const void*;
+using GpuDeviceProperty = const void*;
+using GpuModuleHandle = ze_module_handle_t;
+using GpuStatus = const void*;
+using GpuFuncCachePreference = const void*;
+using GpuSharedMemConfig = const void*;
+using GpuComplexType = std::complex<float>;
+using GpuDoubleComplexType = std::complex<double>;
+using GpuRngHandle = const void*;
+using GpuGraphHandle = const void*;
+using GpuGraphExecHandle = const void*;
+using GpuGraphNodeHandle = const void*;
+using GpuGraphConditionalHandle = UnsupportedGpuFeature;
+
+#elif TENSORFLOW_USE_ROCM
 
 using GpuContextHandle = hipCtx_t;
 using GpuStreamHandle = hipStream_t;
diff --git a/xla/stream_executor/gpu/redzone_allocator.cc b/xla/stream_executor/gpu/redzone_allocator.cc
index aa1643f5d9..f7daaac8fd 100644
--- a/xla/stream_executor/gpu/redzone_allocator.cc
+++ b/xla/stream_executor/gpu/redzone_allocator.cc
@@ -305,6 +305,7 @@ static absl::StatusOr<RedzoneCheckStatus> CheckRedzonesForBuffer(
   return RedzoneCheckStatus::OK();
 }
 
+#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM
 absl::StatusOr<RedzoneCheckStatus> RedzoneAllocator::CheckRedzones() const {
   StreamExecutor* executor = stream_->parent();
 
@@ -354,6 +355,7 @@ absl::StatusOr<RedzoneCheckStatus> RedzoneAllocator::CheckRedzones() const {
 
   return RedzoneCheckStatus::OK();
 }
+#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM
 
 std::string RedzoneCheckStatus::RedzoneFailureMsg() const {
   return absl::StrFormat(
diff --git a/xla/stream_executor/integrations/tf_allocator_adapter.cc b/xla/stream_executor/integrations/tf_allocator_adapter.cc
index 97a6c7c7b0..4902167f73 100644
--- a/xla/stream_executor/integrations/tf_allocator_adapter.cc
+++ b/xla/stream_executor/integrations/tf_allocator_adapter.cc
@@ -59,6 +59,24 @@ absl::Status TfAllocatorAdapter::Deallocate(int device_ordinal,
   return absl::OkStatus();
 }
 
+void* TfAllocatorAdapter::AllocateRaw(
+    int device_ordinal, uint64_t size, bool retry_on_failure,
+    int64_t memory_space) {
+  CHECK_EQ(memory_space, 0);
+  tsl::AllocationAttributes attrs;
+  attrs.retry_on_failure = retry_on_failure;
+  void *data = nullptr;
+  if (size != 0) {
+    data =
+        wrapped_->AllocateRaw(tsl::Allocator::kAllocatorAlignment, size, attrs);
+   // if (data == nullptr) {
+   //   return tsl::errors::ResourceExhausted(
+   //       "Out of memory while trying to allocate ", size, " bytes.");
+   // }
+  }
+  return data;
+}
+
 absl::StatusOr<Stream *> TfAllocatorAdapter::GetStream(int device_ordinal) {
   CHECK_EQ(stream_->parent()->device_ordinal(), device_ordinal);
   return stream_;
diff --git a/xla/stream_executor/integrations/tf_allocator_adapter.h b/xla/stream_executor/integrations/tf_allocator_adapter.h
index 0a1b2bbf37..6ab9affbbd 100644
--- a/xla/stream_executor/integrations/tf_allocator_adapter.h
+++ b/xla/stream_executor/integrations/tf_allocator_adapter.h
@@ -54,6 +54,10 @@ class TfAllocatorAdapter : public DeviceMemoryAllocator {
                                               bool retry_on_failure,
                                               int64_t memory_space) override;
 
+  void* AllocateRaw(int device_ordinal, uint64_t size,
+                                             bool retry_on_failure,
+                                             int64_t memory_space);
+
   absl::Status Deallocate(int device_ordinal, DeviceMemoryBase mem) override;
 
   // The Tensorflow BFC allocator used on GPU allows host-side deallocation
@@ -127,8 +131,9 @@ class MultiDeviceAdapter : public DeviceMemoryAllocator {
         auto result, it->second[device_ordinal]->Allocate(
                          device_ordinal, size, retry_on_failure, memory_space));
 
-    absl::MutexLock lock(&mu_);
-    buffer_memory_spaces_[{device_ordinal, result->opaque()}] = memory_space;
+    // For better performance, comment below code.
+    // absl::MutexLock lock(&mu_);
+    // buffer_memory_spaces_[{device_ordinal, result->opaque()}] = memory_space;
     return result;
   }
 
@@ -136,18 +141,19 @@ class MultiDeviceAdapter : public DeviceMemoryAllocator {
     if (mem.opaque() == nullptr) return absl::OkStatus();
     // Memory space is not passed to deallocate, look up in
     // buffer_memory_spaces_.
-    int64_t memory_space;
-    {
-      absl::MutexLock lock(&mu_);
-      auto it = buffer_memory_spaces_.find({device_ordinal, mem.opaque()});
-      if (it == buffer_memory_spaces_.end()) {
-        return absl::InternalError(
-            absl::StrFormat("Memory %p was not allocated on device %d.",
-                            mem.opaque(), device_ordinal));
-      }
-      memory_space = it->second;
-      buffer_memory_spaces_.erase(it);
-    }
+    const int64_t memory_space = 0;
+    // For better performance, comment below code.
+    // {
+    //   absl::MutexLock lock(&mu_);
+    //   auto it = buffer_memory_spaces_.find({device_ordinal, mem.opaque()});
+    //   if (it == buffer_memory_spaces_.end()) {
+    //     return absl::InternalError(
+    //         absl::StrFormat("Memory %p was not allocated on device %d.",
+    //                         mem.opaque(), device_ordinal));
+    //   }
+    //   memory_space = it->second;
+    //   buffer_memory_spaces_.erase(it);
+    // }
 
     auto it = memory_space_to_per_device_allocators_.find(memory_space);
     CHECK(it != memory_space_to_per_device_allocators_.end());
@@ -155,6 +161,23 @@ class MultiDeviceAdapter : public DeviceMemoryAllocator {
     return it->second[device_ordinal]->Deallocate(device_ordinal, mem);
   }
 
+  void* AllocateRaw(int device_ordinal, uint64_t size,
+                    bool retry_on_failure, 
+                    int64_t memory_space) {
+// memory_space is used here to select allocator. This isn't a need to pass
+    // it any lower to TfAllocatorAdapter.
+    auto it = memory_space_to_per_device_allocators_.find(memory_space);
+    CHECK(it != memory_space_to_per_device_allocators_.end());
+    CHECK_LT(device_ordinal, it->second.size());
+    return it->second[device_ordinal]->AllocateRaw(
+        device_ordinal, size, retry_on_failure, memory_space);
+  }
+
+  absl::Status DeallocateRaw(int device_ordinal, void* mem_opaque) {
+    DeviceMemoryBase device_mem(mem_opaque);
+    return this->Deallocate(device_ordinal, device_mem);
+  }
+
   // The Tensorflow BFC allocator used on GPU allows host-side deallocation
   // before GPU execution takes place. Tensorflow uses the ordering of the main
   // compute stream to enforce a happens-before relationship between a memory
diff --git a/xla/stream_executor/kernel_spec.cc b/xla/stream_executor/kernel_spec.cc
index 0cafa82805..9b544e767e 100644
--- a/xla/stream_executor/kernel_spec.cc
+++ b/xla/stream_executor/kernel_spec.cc
@@ -35,9 +35,9 @@ KernelLoaderSpec::KernelLoaderSpec(absl::string_view kernel_name)
 InProcessSymbol::InProcessSymbol(void *symbol, std::string kernel_name)
     : KernelLoaderSpec(std::move(kernel_name)), symbol_(symbol) {}
 
-CudaCubinInMemory::CudaCubinInMemory(const char *bytes,
+CudaCubinInMemory::CudaCubinInMemory(const char *bytes, int size,
                                      absl::string_view kernel_name)
-    : KernelLoaderSpec(kernel_name), bytes_(bytes) {}
+    : KernelLoaderSpec(kernel_name), size_(size), bytes_(bytes) {}
 
 bool CompareComputeCapability(const std::tuple<int, int> &lhs,
                               const std::tuple<int, int> &rhs) {
@@ -167,9 +167,9 @@ MultiKernelLoaderSpec *MultiKernelLoaderSpec::AddInProcessSymbol(
 }
 
 MultiKernelLoaderSpec *MultiKernelLoaderSpec::AddCudaCubinInMemory(
-    const char *bytes, absl::string_view kernel_name) {
+    const char *bytes, int size, absl::string_view kernel_name) {
   CHECK(cuda_cubin_in_memory_ == nullptr);
-  cuda_cubin_in_memory_.reset(new CudaCubinInMemory{bytes, kernel_name});
+  cuda_cubin_in_memory_.reset(new CudaCubinInMemory{bytes, size, kernel_name});
   return this;
 }
 
diff --git a/xla/stream_executor/kernel_spec.h b/xla/stream_executor/kernel_spec.h
index fa02d7206b..13a1b0a601 100644
--- a/xla/stream_executor/kernel_spec.h
+++ b/xla/stream_executor/kernel_spec.h
@@ -186,13 +186,16 @@ class CudaPtxInMemory : public KernelLoaderSpec {
 // Kernel loader specification for a CUBIN blob that resides in memory.
 class CudaCubinInMemory : public KernelLoaderSpec {
  public:
-  CudaCubinInMemory(const char *bytes, absl::string_view kernel_name);
+  CudaCubinInMemory(const char *bytes, int size, absl::string_view kernel_name);
   ~CudaCubinInMemory() override {}
 
   const char *bytes() const { return bytes_; }
+  const int size() const { return size_; }
 
  private:
   const char *bytes_;
+  // SYCL: this is needed only for SPIRV
+  int size_;
 
   CudaCubinInMemory(const CudaCubinInMemory &) = delete;
   void operator=(const CudaCubinInMemory &) = delete;
@@ -246,7 +249,7 @@ class MultiKernelLoaderSpec {
   // mangled by the compiler if it is not declared in an extern "C" scope.
   MultiKernelLoaderSpec *AddInProcessSymbol(void *symbol,
                                             absl::string_view kernel_name);
-  MultiKernelLoaderSpec *AddCudaCubinInMemory(const char *cubin_bytes,
+  MultiKernelLoaderSpec *AddCudaCubinInMemory(const char *cubin_bytes, int size,
                                               absl::string_view kernel_name);
   MultiKernelLoaderSpec *AddCudaPtxInMemory(absl::string_view ptx,
                                             absl::string_view kernel_name);
diff --git a/xla/stream_executor/rocm/rocm_driver.cc b/xla/stream_executor/rocm/rocm_driver.cc
index 02dcac01c0..1e240b2aca 100644
--- a/xla/stream_executor/rocm/rocm_driver.cc
+++ b/xla/stream_executor/rocm/rocm_driver.cc
@@ -1165,6 +1165,14 @@ struct BitPatternToValue {
   return ret;
 }
 
+/* static */ absl::Status GpuDriver::LoadLevelzero(GpuContext* context,
+                                                  const char* spir_contents,
+                                                  const size_t size,
+                                                  hipModule_t* module) {
+  return absl::InternalError(
+      "Feature not supported on ROCm platform (LoadLevelzero)");
+}
+
 /* static */ absl::Status GpuDriver::SynchronousMemsetUint8(
     GpuContext* context, hipDeviceptr_t location, uint8 value, size_t size) {
   ScopedActivateContext activation{context};
diff --git a/xla/stream_executor/rocm/rocm_executor.cc b/xla/stream_executor/rocm/rocm_executor.cc
index 6ee1824f9c..230e45e6e8 100644
--- a/xla/stream_executor/rocm/rocm_executor.cc
+++ b/xla/stream_executor/rocm/rocm_executor.cc
@@ -461,6 +461,16 @@ absl::Status GpuExecutor::LoadModuleFromHsaco(const char* hsaco,
   return absl::OkStatus();
 }
 
+tsl::Status GpuExecutor::LoadModuleFromSpir(const char* spir, const size_t size,
+                                            hipModule_t* module) {
+  LOG(FATAL) << "Feature not supported on ROCM platform (LoadModuleFromSpir)";
+}
+
+tsl::Status GpuExecutor::LoadModuleFromSpir(const char* spir, const size_t size,
+                                            hipModule_t* module) {
+  LOG(FATAL) << "Feature not supported on ROCM platform (LoadModuleFromSpir)";
+}
+
 // This is a non-essential operation; if there's a failure, proceed without
 // logging an error. It's nearly certain that in case of failures, we'd never
 // get here in the first place; these are very low-impact routines.
diff --git a/xla/stream_executor/stream_executor_pimpl.h b/xla/stream_executor/stream_executor_pimpl.h
index 2a624ffc70..9ae7931f7a 100644
--- a/xla/stream_executor/stream_executor_pimpl.h
+++ b/xla/stream_executor/stream_executor_pimpl.h
@@ -584,7 +584,8 @@ StreamExecutor::CreateTypedKernel(absl::string_view kernel_name,
 
   if (!cubin_data.empty()) {
     loader_spec.AddCudaCubinInMemory(
-        reinterpret_cast<const char*>(cubin_data.data()), kernel_name);
+        reinterpret_cast<const char*>(cubin_data.data()), cubin_data.size(),
+        kernel_name);
   }
 
   TF_RETURN_IF_ERROR(GetKernel(loader_spec, kernel_base.get()));
diff --git a/xla/stream_executor/tf_allocator_adapter.h b/xla/stream_executor/tf_allocator_adapter.h
new file mode 100644
index 0000000000..6810585d8a
--- /dev/null
+++ b/xla/stream_executor/tf_allocator_adapter.h
@@ -0,0 +1,173 @@
+/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef XLA_STREAM_EXECUTOR_TF_ALLOCATOR_ADAPTER_H_
+#define XLA_STREAM_EXECUTOR_TF_ALLOCATOR_ADAPTER_H_
+
+#include <memory>
+#include <tuple>
+#include <utility>
+#include <vector>
+
+#include "xla/stream_executor/device_memory.h"
+#include "xla/stream_executor/device_memory_allocator.h"
+#include "xla/stream_executor/platform.h"
+#include "xla/stream_executor/stream.h"
+#include "xla/stream_executor/stream_executor.h"
+#include "tsl/framework/allocator.h"
+#include "tsl/platform/statusor.h"
+
+namespace stream_executor {
+
+// Adapter class that wraps a Tensorflow allocator.
+//
+// Assumes that the Tensorflow allocator permits asynchronous deallocation:
+// see comment on `AllowsAsynchronousDeallocation()`.
+class TfAllocatorAdapter : public DeviceMemoryAllocator {
+ public:
+  // stream: a Stream on which the allocator can only be used. If non-null, the
+  // allocator can not be used on any other stream.
+  TfAllocatorAdapter(tsl::Allocator *wrapped, Stream *stream);
+
+  // Constructor for the cases where `stream` can not be provided.
+  TfAllocatorAdapter(tsl::Allocator *wrapped, Platform *platform);
+
+  ~TfAllocatorAdapter() override;
+
+  tsl::StatusOr<OwningDeviceMemory> Allocate(int device_ordinal, uint64_t size,
+                                             bool retry_on_failure,
+                                             int64_t memory_space) override;
+
+  void* AllocateRaw(int device_ordinal, uint64_t size,
+                                             bool retry_on_failure,
+                                             int64_t memory_space);
+
+  tsl::Status Deallocate(int device_ordinal, DeviceMemoryBase mem) override;
+
+  // The Tensorflow BFC allocator used on GPU allows host-side deallocation
+  // before GPU execution takes place. Tensorflow uses the ordering of the main
+  // compute stream to enforce a happens-before relationship between a memory
+  // allocation and code that reuses the same memory. If Tensorflow adds
+  // support for multiple GPU streams or allocators with different ordering
+  // requirements, this code may need to change.
+  // (This attribute has no effect on CPU.)
+  bool AllowsAsynchronousDeallocation() const override { return true; }
+
+  tsl::StatusOr<Stream *> GetStream(int device_ordinal) override;
+
+  tsl::StatusOr<tsl::Allocator *> GetAllocator(int device_ordinal);
+
+ private:
+  tsl::Allocator *wrapped_;
+  Stream *stream_;
+};
+
+// Adapter class that wraps per-device TF allocators with corresponding streams
+// as a TfAllocatorAdapter. Assumes that the Tensorflow allocator permits
+// asynchronous deallocation; see comment on `AllowsAsynchronousDeallocation()`.
+class MultiDeviceAdapter : public DeviceMemoryAllocator {
+ public:
+  using AllocatorWithStream =
+      std::pair<std::unique_ptr<tsl::Allocator>, Stream *>;
+  using AllocatorWithLogicalIdAndStream =
+      std::tuple<std::unique_ptr<tsl::Allocator>, int, Stream *>;
+
+  MultiDeviceAdapter(const Platform *platform,
+                     std::vector<AllocatorWithStream> tf_allocators)
+      : DeviceMemoryAllocator(platform) {
+    tf_allocators_.reserve(tf_allocators.size());
+    for (AllocatorWithStream &p : tf_allocators) {
+      int device_ordinal = p.second->parent()->device_ordinal();
+      if (per_device_allocators_.size() <= device_ordinal) {
+        per_device_allocators_.resize(device_ordinal + 1);
+      }
+      CHECK(!per_device_allocators_[device_ordinal]);
+      per_device_allocators_[device_ordinal] =
+          std::make_unique<TfAllocatorAdapter>(p.first.get(), p.second);
+      tf_allocators_.push_back(std::move(p.first));
+    }
+  }
+
+  MultiDeviceAdapter(const Platform *platform,
+                     std::vector<AllocatorWithLogicalIdAndStream> tf_allocators)
+      : DeviceMemoryAllocator(platform) {
+    tf_allocators_.reserve(tf_allocators.size());
+    for (AllocatorWithLogicalIdAndStream &t : tf_allocators) {
+      const int device_ordinal = std::get<1>(t);
+      Stream *stream = std::get<2>(t);
+      if (per_device_allocators_.size() <= device_ordinal) {
+        per_device_allocators_.resize(device_ordinal + 1);
+      }
+      CHECK(!per_device_allocators_[device_ordinal]);
+      per_device_allocators_[device_ordinal] =
+          std::make_unique<TfAllocatorAdapter>(std::get<0>(t).get(), stream);
+      tf_allocators_.push_back(std::move(std::get<0>(t)));
+    }
+  }
+
+  tsl::StatusOr<OwningDeviceMemory> Allocate(int device_ordinal, uint64_t size,
+                                             bool retry_on_failure,
+                                             int64_t memory_space) override {
+    CHECK_LT(device_ordinal, per_device_allocators_.size());
+    return per_device_allocators_[device_ordinal]->Allocate(
+        device_ordinal, size, retry_on_failure, memory_space);
+  }
+
+  tsl::Status Deallocate(int device_ordinal, DeviceMemoryBase mem) override {
+    CHECK_LT(device_ordinal, per_device_allocators_.size());
+    return per_device_allocators_[device_ordinal]->Deallocate(device_ordinal,
+                                                              mem);
+  }
+
+  void* AllocateRaw(int device_ordinal, uint64_t size,
+                    bool retry_on_failure,
+                    int64_t memory_space) {
+    CHECK_LT(device_ordinal, per_device_allocators_.size());
+    return per_device_allocators_[device_ordinal]->AllocateRaw(
+        device_ordinal, size, retry_on_failure, memory_space);
+  }
+  
+  tsl::Status DeallocateRaw(int device_ordinal, void* mem_opaque) {
+    DeviceMemoryBase device_mem(mem_opaque);
+    return this->Deallocate(device_ordinal, device_mem);
+  }
+
+  // The Tensorflow BFC allocator used on GPU allows host-side deallocation
+  // before GPU execution takes place. Tensorflow uses the ordering of the main
+  // compute stream to enforce a happens-before relationship between a memory
+  // allocation and code that reuses the same memory. If Tensorflow adds
+  // support for multiple GPU streams or allocators with different ordering
+  // requirements, this code may need to change.
+  // (This attribute has no effect on CPU.)
+  bool AllowsAsynchronousDeallocation() const override { return true; }
+
+  tsl::StatusOr<Stream *> GetStream(int device_ordinal) override {
+    return per_device_allocators_[device_ordinal]->GetStream(device_ordinal);
+  }
+
+  tsl::StatusOr<tsl::Allocator *> GetAllocator(int device_ordinal) {
+    return per_device_allocators_[device_ordinal]->GetAllocator(device_ordinal);
+  }
+
+ private:
+  std::vector<std::unique_ptr<TfAllocatorAdapter>> per_device_allocators_;
+  // The wrapped TF allocators backing per_device_allocators_
+  // (TfAllocatorAdapter does not take ownership of its underlying Allocator).
+  std::vector<std::unique_ptr<tsl::Allocator>> tf_allocators_;
+};
+
+}  // namespace stream_executor
+
+#endif  // XLA_STREAM_EXECUTOR_TF_ALLOCATOR_ADAPTER_H_
