diff --git a/third_party/tsl/third_party/eigen3/eigen.patch b/third_party/tsl/third_party/eigen3/eigen.patch
new file mode 100644
index 0000000000..8954b1d2e3
--- /dev/null
+++ b/third_party/tsl/third_party/eigen3/eigen.patch
@@ -0,0 +1,13 @@
+diff --git a/Eigen/src/Core/util/Macros.h b/Eigen/src/Core/util/Macros.h
+index 69bbf2e73..251053b55 100644
+--- a/Eigen/src/Core/util/Macros.h
++++ b/Eigen/src/Core/util/Macros.h
+@@ -686,7 +686,7 @@
+ // For instance, if compiling with gcc and -std=c++17, then EIGEN_COMP_CXXVER
+ // is defined to 17.
+ #if EIGEN_CPLUSPLUS >= 202002L
+-  #define EIGEN_COMP_CXXVER 20
++  #define EIGEN_COMP_CXXVER 17
+ #elif EIGEN_CPLUSPLUS >= 201703L
+   #define EIGEN_COMP_CXXVER 17
+ #elif EIGEN_CPLUSPLUS >= 201402L
diff --git a/third_party/tsl/third_party/eigen3/workspace.bzl b/third_party/tsl/third_party/eigen3/workspace.bzl
index 027454e46d..ca5489940e 100644
--- a/third_party/tsl/third_party/eigen3/workspace.bzl
+++ b/third_party/tsl/third_party/eigen3/workspace.bzl
@@ -14,6 +14,7 @@ def repo():
     tf_http_archive(
         name = "eigen_archive",
         build_file = "//third_party/eigen3:eigen_archive.BUILD",
+        patch_file = ["//third_party/eigen3:eigen.patch"],
         sha256 = EIGEN_SHA256,
         strip_prefix = "eigen-{commit}".format(commit = EIGEN_COMMIT),
         urls = tf_mirror_urls("https://gitlab.com/libeigen/eigen/-/archive/{commit}/eigen-{commit}.tar.gz".format(commit = EIGEN_COMMIT)),
diff --git a/third_party/tsl/third_party/gpus/cuda_configure.bzl b/third_party/tsl/third_party/gpus/cuda_configure.bzl
index f4ed97ac4e..daa327a3eb 100644
--- a/third_party/tsl/third_party/gpus/cuda_configure.bzl
+++ b/third_party/tsl/third_party/gpus/cuda_configure.bzl
@@ -37,9 +37,9 @@ load(
     "find_vc_path",
     "setup_vc_env_vars",
 )
-load("//third_party/clang_toolchain:download_clang.bzl", "download_clang")
+load("@xla//third_party/tsl/third_party/clang_toolchain:download_clang.bzl", "download_clang")
 load(
-    "//third_party/remote_config:common.bzl",
+    "@xla//third_party/tsl/third_party/remote_config:common.bzl",
     "config_repo_label",
     "err_out",
     "execute",
diff --git a/third_party/tsl/third_party/grpc/c_ares.patch b/third_party/tsl/third_party/grpc/c_ares.patch
new file mode 100644
index 0000000000..752965f933
--- /dev/null
+++ b/third_party/tsl/third_party/grpc/c_ares.patch
@@ -0,0 +1,200 @@
+diff --git a/bazel/grpc_deps.bzl b/bazel/grpc_deps.bzl
+index c2f2f43820..72de3cea94 100644
+--- a/bazel/grpc_deps.bzl
++++ b/bazel/grpc_deps.bzl
+@@ -182,9 +182,9 @@ def grpc_deps():
+         http_archive(
+             name = "com_github_cares_cares",
+             build_file = "@com_github_grpc_grpc//third_party:cares/cares.BUILD",
+-            sha256 = "e8c2751ddc70fed9dc6f999acd92e232d5846f009ee1674f8aee81f19b2b915a",
+-            strip_prefix = "c-ares-e982924acee7f7313b4baa4ee5ec000c5e373c30",
+-            url = "https://github.com/c-ares/c-ares/archive/e982924acee7f7313b4baa4ee5ec000c5e373c30.tar.gz",
++            sha256 = "321700399b72ed0e037d0074c629e7741f6b2ec2dda92956abe3e9671d3e268e",
++            strip_prefix = "c-ares-1.19.1",
++            url = "https://github.com/c-ares/c-ares/releases/download/cares-1_19_1/c-ares-1.19.1.tar.gz",
+         )
+
+     if "com_google_absl" not in native.existing_rules():
+diff --git a/third_party/cares/cares.BUILD b/third_party/cares/cares.BUILD
+index 203712b182..2561b1a4bc 100644
+--- a/third_party/cares/cares.BUILD
++++ b/third_party/cares/cares.BUILD
+@@ -109,84 +109,95 @@ genrule(
+ cc_library(
+     name = "ares",
+     srcs = [
+-        "ares__close_sockets.c",
+-        "ares__get_hostent.c",
+-        "ares__read_line.c",
+-        "ares__timeval.c",
+-        "ares_cancel.c",
+-        "ares_create_query.c",
+-        "ares_data.c",
+-        "ares_destroy.c",
+-        "ares_expand_name.c",
+-        "ares_expand_string.c",
+-        "ares_fds.c",
+-        "ares_free_hostent.c",
+-        "ares_free_string.c",
+-        "ares_getenv.c",
+-        "ares_gethostbyaddr.c",
+-        "ares_gethostbyname.c",
+-        "ares_getnameinfo.c",
+-        "ares_getopt.c",
+-        "ares_getsock.c",
+-        "ares_init.c",
+-        "ares_library_init.c",
+-        "ares_llist.c",
+-        "ares_mkquery.c",
+-        "ares_nowarn.c",
+-        "ares_options.c",
+-        "ares_parse_a_reply.c",
+-        "ares_parse_aaaa_reply.c",
+-        "ares_parse_mx_reply.c",
+-        "ares_parse_naptr_reply.c",
+-        "ares_parse_ns_reply.c",
+-        "ares_parse_ptr_reply.c",
+-        "ares_parse_soa_reply.c",
+-        "ares_parse_srv_reply.c",
+-        "ares_parse_txt_reply.c",
+-        "ares_platform.c",
+-        "ares_process.c",
+-        "ares_query.c",
+-        "ares_search.c",
+-        "ares_send.c",
+-        "ares_strcasecmp.c",
+-        "ares_strdup.c",
+-        "ares_strsplit.c",
+-        "ares_strerror.c",
+-        "ares_timeout.c",
+-        "ares_version.c",
+-        "ares_writev.c",
+-        "bitncmp.c",
+-        "inet_net_pton.c",
+-        "inet_ntop.c",
+-        "windows_port.c",
++        "src/lib/ares__read_line.c",
++        "src/lib/ares__get_hostent.c",
++        "src/lib/ares__close_sockets.c",
++        "src/lib/ares__timeval.c",
++        "src/lib/ares_gethostbyaddr.c",
++        "src/lib/ares_getenv.c",
++        "src/lib/ares_free_string.c",
++        "src/lib/ares_free_hostent.c",
++        "src/lib/ares_fds.c",
++        "src/lib/ares_expand_string.c",
++        "src/lib/ares_create_query.c",
++        "src/lib/ares_cancel.c",
++        "src/lib/ares_android.c",
++        "src/lib/ares_parse_txt_reply.c",
++        "src/lib/ares_parse_srv_reply.c",
++        "src/lib/ares_parse_soa_reply.c",
++        "src/lib/ares_parse_ptr_reply.c",
++        "src/lib/ares_parse_ns_reply.c",
++        "src/lib/ares_parse_naptr_reply.c",
++        "src/lib/ares_parse_mx_reply.c",
++        "src/lib/ares_parse_caa_reply.c",
++        "src/lib/ares_options.c",
++        "src/lib/ares_nowarn.c",
++        "src/lib/ares_mkquery.c",
++        "src/lib/ares_llist.c",
++        "src/lib/ares_getsock.c",
++        "src/lib/ares_getnameinfo.c",
++        "src/lib/bitncmp.c",
++        "src/lib/ares_writev.c",
++        "src/lib/ares_version.c",
++        "src/lib/ares_timeout.c",
++        "src/lib/ares_strerror.c",
++        "src/lib/ares_strcasecmp.c",
++        "src/lib/ares_search.c",
++        "src/lib/ares_platform.c",
++        "src/lib/windows_port.c",
++        "src/lib/inet_ntop.c",
++        "src/lib/ares__sortaddrinfo.c",
++        "src/lib/ares__readaddrinfo.c",
++        "src/lib/ares_parse_uri_reply.c",
++        "src/lib/ares__parse_into_addrinfo.c",
++        "src/lib/ares_parse_a_reply.c",
++        "src/lib/ares_parse_aaaa_reply.c",
++        "src/lib/ares_library_init.c",
++        "src/lib/ares_init.c",
++        "src/lib/ares_gethostbyname.c",
++        "src/lib/ares_getaddrinfo.c",
++        "src/lib/ares_freeaddrinfo.c",
++        "src/lib/ares_expand_name.c",
++        "src/lib/ares_destroy.c",
++        "src/lib/ares_data.c",
++        "src/lib/ares__addrinfo_localhost.c",
++        "src/lib/ares__addrinfo2hostent.c",
++        "src/lib/inet_net_pton.c",
++        "src/lib/ares_strsplit.c",
++        "src/lib/ares_strdup.c",
++        "src/lib/ares_send.c",
++        "src/lib/ares_rand.c",
++        "src/lib/ares_query.c",
++        "src/lib/ares_process.c",
+     ],
+     hdrs = [
+-        "ares.h",
+         "ares_build.h",
+         "ares_config.h",
+-        "ares_data.h",
+-        "ares_dns.h",
+-        "ares_getenv.h",
+-        "ares_getopt.h",
+-        "ares_inet_net_pton.h",
+-        "ares_iphlpapi.h",
+-        "ares_ipv6.h",
+-        "ares_library_init.h",
+-        "ares_llist.h",
+-        "ares_nowarn.h",
+-        "ares_platform.h",
+-        "ares_private.h",
+-        "ares_rules.h",
+-        "ares_setup.h",
+-        "ares_strcasecmp.h",
+-        "ares_strdup.h",
+-        "ares_strsplit.h",
+-        "ares_version.h",
+-        "ares_writev.h",
+-        "bitncmp.h",
+-        "config-win32.h",
+-        "nameser.h",
+-        "setup_once.h",
++        "include/ares_version.h",
++        "include/ares.h",
++        "include/ares_rules.h",
++        "include/ares_dns.h",
++        "include/ares_nameser.h",
++        "src/tools/ares_getopt.h",
++        "src/lib/ares_strsplit.h",
++        "src/lib/ares_android.h",
++        "src/lib/ares_private.h",
++        "src/lib/ares_llist.h",
++        "src/lib/ares_platform.h",
++        "src/lib/ares_ipv6.h",
++        "src/lib/config-dos.h",
++        "src/lib/bitncmp.h",
++        "src/lib/ares_strcasecmp.h",
++        "src/lib/setup_once.h",
++        "src/lib/ares_inet_net_pton.h",
++        "src/lib/ares_data.h",
++        "src/lib/ares_getenv.h",
++        "src/lib/config-win32.h",
++        "src/lib/ares_strdup.h",
++        "src/lib/ares_iphlpapi.h",
++        "src/lib/ares_setup.h",
++        "src/lib/ares_writev.h",
++        "src/lib/ares_nowarn.h",
+     ],
+     copts = [
+         "-D_GNU_SOURCE",
+@@ -202,7 +213,7 @@ cc_library(
+         "//conditions:default": [],
+     }),
+     defines = ["CARES_STATICLIB"],
+-    includes = ["."],
++    includes = ["include", "."],
+     linkopts = select({
+         ":windows": ["-defaultlib:ws2_32.lib"],
+         "//conditions:default": [],
diff --git a/third_party/tsl/third_party/grpc/upb_platform_fix.patch b/third_party/tsl/third_party/grpc/upb_platform_fix.patch
index 6edd66067e..022c9d1557 100644
--- a/third_party/tsl/third_party/grpc/upb_platform_fix.patch
+++ b/third_party/tsl/third_party/grpc/upb_platform_fix.patch
@@ -11,3 +11,12 @@ index ad85b202..2311b2e4 100644
  )
 
  config_setting(
+@@ -24,7 +24,7 @@ exports_files([
+
+ CPPOPTS = [
+     # copybara:strip_for_google3_begin
+-    "-Werror",
++    # "-Werror",
+     "-Wno-long-long",
+     # copybara:strip_end
+ ]
diff --git a/third_party/tsl/third_party/llvm/build.patch b/third_party/tsl/third_party/llvm/build.patch
index 479e08cde8..33f585b709 100644
--- a/third_party/tsl/third_party/llvm/build.patch
+++ b/third_party/tsl/third_party/llvm/build.patch
@@ -44,3 +44,24 @@ index 7770284e5543..0b45127495dc 100644
          "//conditions:default": [
              "BLAKE3_NO_AVX2",
              "BLAKE3_NO_AVX512",
+diff --git a/mlir/lib/Bytecode/Reader/BytecodeReader.cpp b/mlir/lib/Bytecode/Reader/BytecodeReader.cpp
+index 177372c68046..40d49dc13b2f 100644
+--- a/mlir/lib/Bytecode/Reader/BytecodeReader.cpp
++++ b/mlir/lib/Bytecode/Reader/BytecodeReader.cpp
+@@ -1549,6 +1549,8 @@ private:
+   const std::shared_ptr<llvm::SourceMgr> &bufferOwnerRef;
+ };
+
++// TODO: Disable clang opt since it crashes with clang-17 compiler.
++#pragma clang optimize off
+ LogicalResult BytecodeReader::Impl::read(
+     Block *block, llvm::function_ref<bool(Operation *)> lazyOpsCallback) {
+   EncodingReader reader(buffer.getBuffer(), fileLoc);
+@@ -1628,6 +1630,7 @@ LogicalResult BytecodeReader::Impl::read(
+   // Finally, process the IR section.
+   return parseIRSection(*sectionDatas[bytecode::Section::kIR], block);
+ }
++#pragma clang optimize on
+
+ LogicalResult BytecodeReader::Impl::parseVersion(EncodingReader &reader) {
+   if (failed(reader.parseVarInt(version)))
diff --git a/third_party/tsl/third_party/llvm/spirv.patch b/third_party/tsl/third_party/llvm/spirv.patch
new file mode 100644
index 0000000000..8643e91813
--- /dev/null
+++ b/third_party/tsl/third_party/llvm/spirv.patch
@@ -0,0 +1,76 @@
+diff --git a/llvm/lib/Passes/PassBuilderPipelines.cpp b/llvm/lib/Passes/PassBuilderPipelines.cpp
+index 78e0e6353056..4f9f51164bfe 100644
+--- a/llvm/lib/Passes/PassBuilderPipelines.cpp
++++ b/llvm/lib/Passes/PassBuilderPipelines.cpp
+@@ -183,6 +183,10 @@ static cl::opt<bool> EnableGlobalAnalyses(
+     "enable-global-analyses", cl::init(true), cl::Hidden,
+     cl::desc("Enable inter-procedural analyses"));
+ 
++static cl::opt<bool>
++    SYCLOptimizationMode("sycl-opt", cl::init(false), cl::Hidden,
++                         cl::desc("Enable SYCL optimization mode."));
++
+ static cl::opt<bool>
+     RunPartialInlining("enable-partial-inlining", cl::init(false), cl::Hidden,
+                        cl::desc("Run Partial inlinining pass"));
+@@ -406,6 +410,7 @@ PassBuilder::buildO1FunctionSimplificationPipeline(OptimizationLevel Level,
+   // Form canonically associated expression trees, and simplify the trees using
+   // basic mathematical properties. For example, this will form (nearly)
+   // minimal multiplication trees.
++  if (!SYCLOptimizationMode) {
+   FPM.addPass(ReassociatePass());
+ 
+   // Add the primary loop simplification pipeline.
+@@ -477,7 +482,7 @@ PassBuilder::buildO1FunctionSimplificationPipeline(OptimizationLevel Level,
+   FPM.addPass(createFunctionToLoopPassAdaptor(std::move(LPM2),
+                                               /*UseMemorySSA=*/false,
+                                               /*UseBlockFrequencyInfo=*/false));
+-
++  }
+   // Delete small array after loop unroll.
+   FPM.addPass(SROAPass(SROAOptions::ModifyCFG));
+ 
+@@ -580,6 +585,7 @@ PassBuilder::buildFunctionSimplificationPipeline(OptimizationLevel Level,
+   // Form canonically associated expression trees, and simplify the trees using
+   // basic mathematical properties. For example, this will form (nearly)
+   // minimal multiplication trees.
++  if (!SYCLOptimizationMode) {
+   FPM.addPass(ReassociatePass());
+ 
+   if (EnableConstraintElimination)
+@@ -657,7 +663,7 @@ PassBuilder::buildFunctionSimplificationPipeline(OptimizationLevel Level,
+   FPM.addPass(createFunctionToLoopPassAdaptor(std::move(LPM2),
+                                               /*UseMemorySSA=*/false,
+                                               /*UseBlockFrequencyInfo=*/false));
+-
++  }
+   // Delete small array after loop unroll.
+   FPM.addPass(SROAPass(SROAOptions::ModifyCFG));
+ 
+@@ -715,6 +721,9 @@ PassBuilder::buildFunctionSimplificationPipeline(OptimizationLevel Level,
+ 
+   invokeScalarOptimizerLateEPCallbacks(FPM, Level);
+ 
++  if (SYCLOptimizationMode)
++    FPM.addPass(SimplifyCFGPass());
++  else
+   FPM.addPass(SimplifyCFGPass(SimplifyCFGOptions()
+                                   .convertSwitchRangeToICmp(true)
+                                   .hoistCommonInsts(true)
+@@ -1385,6 +1394,7 @@ PassBuilder::buildModuleOptimizationPipeline(OptimizationLevel Level,
+ 
+   invokeVectorizerStartEPCallbacks(OptimizePM, Level);
+ 
++  if (!SYCLOptimizationMode) {
+   LoopPassManager LPM;
+   // First rotate loops that may have been un-rotated by prior passes.
+   // Disable header duplication at -Oz.
+@@ -1408,7 +1418,7 @@ PassBuilder::buildModuleOptimizationPipeline(OptimizationLevel Level,
+   OptimizePM.addPass(InjectTLIMappings());
+ 
+   addVectorPasses(Level, OptimizePM, /* IsFullLTO */ false);
+-
++  }
+   // LoopSink pass sinks instructions hoisted by LICM, which serves as a
+   // canonicalization pass that enables other optimizations. As a result,
+   // LoopSink pass needs to be a very late IR pass to avoid undoing LICM
diff --git a/third_party/tsl/third_party/llvm/workspace.bzl b/third_party/tsl/third_party/llvm/workspace.bzl
index 93795513a1..a6c0139636 100644
--- a/third_party/tsl/third_party/llvm/workspace.bzl
+++ b/third_party/tsl/third_party/llvm/workspace.bzl
@@ -17,6 +17,7 @@ def repo(name):
         ],
         build_file = "//third_party/llvm:llvm.BUILD",
         patch_file = [
+            "//third_party/llvm:spirv.patch",
             "//third_party/llvm:generated.patch",  # Autogenerated, don't remove.
             "//third_party/llvm:build.patch",
             "//third_party/llvm:mathextras.patch",
diff --git a/third_party/tsl/tsl/tsl.bzl b/third_party/tsl/tsl/tsl.bzl
index 30dbf6312f..8f2d85df3f 100644
--- a/third_party/tsl/tsl/tsl.bzl
+++ b/third_party/tsl/tsl/tsl.bzl
@@ -36,6 +36,10 @@ load(
     "@local_config_tensorrt//:build_defs.bzl",
     "if_tensorrt",
 )
+load(
+    "@local_config_sycl//sycl:build_defs.bzl",
+    "if_sycl_is_configured",
+)
 
 # Internally this loads a macro, but in OSS this is a function
 # buildifier: disable=out-of-order-load
@@ -282,7 +286,7 @@ def tsl_copts(
         if_tensorrt(["-DGOOGLE_TENSORRT=1"]) +
         if_rocm(["-DTENSORFLOW_USE_ROCM=1"]) +
         # Compile in oneDNN based ops when building for x86 platforms
-        if_mkl(["-DINTEL_MKL"]) +
+        if_sycl_is_configured([], if_mkl(["-DINTEL_MKL"])) +
         # Enable additional ops (e.g., ops with non-NHWC data layout) and
         # optimizations for Intel builds using oneDNN if configured
         if_enable_mkl(["-DENABLE_MKL"]) +
diff --git a/third_party/tsl/workspace2.bzl b/third_party/tsl/workspace2.bzl
index e23dcc3a4c..aaaf22ed81 100644
--- a/third_party/tsl/workspace2.bzl
+++ b/third_party/tsl/workspace2.bzl
@@ -348,6 +348,7 @@ def _tf_repositories():
         patch_file = [
             "//third_party/grpc:generate_cc_env_fix.patch",
             "//third_party/grpc:register_go_toolchain.patch",
+            "//third_party/grpc:c_ares.patch",
         ],
         system_link_files = {
             "//third_party/systemlibs:BUILD": "bazel/BUILD",
diff --git a/xla/backends/profiler/plugin/BUILD b/xla/backends/profiler/plugin/BUILD
index 169a4eaa4e..161e4e0452 100644
--- a/xla/backends/profiler/plugin/BUILD
+++ b/xla/backends/profiler/plugin/BUILD
@@ -62,6 +62,10 @@ cc_library(
     deps = [
         ":profiler_c_api_hdrs",
         ":profiler_error",
+        "//xla/backends/profiler/cpu:host_tracer",
+        "//xla/backends/profiler/cpu:metadata_collector",
+        "//xla/backends/profiler/cpu:python_tracer",
+        "@intel_extension_for_openxla//xla/profiler:sycl_device_tracer",
         "@tsl//tsl/platform:logging",
         "@tsl//tsl/profiler/lib:profiler_collection",
         "@tsl//tsl/profiler/lib:profiler_factory",
diff --git a/xla/pjrt/c/pjrt_c_api_gpu_internal.cc b/xla/pjrt/c/pjrt_c_api_gpu_internal.cc
index 211fcc5d53..53e2c5b38c 100644
--- a/xla/pjrt/c/pjrt_c_api_gpu_internal.cc
+++ b/xla/pjrt/c/pjrt_c_api_gpu_internal.cc
@@ -53,7 +53,7 @@ limitations under the License.
 namespace pjrt {
 namespace gpu_plugin {
 
-#define PJRT_GPU_PLUGIN_PLATFORM_NAME "CUDA"
+#define PJRT_GPU_PLUGIN_PLATFORM_NAME "SYCL"
 
 PJRT_Error* PJRT_Client_Create(PJRT_Client_Create_Args* args) {
   PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(
diff --git a/xla/pjrt/event_pool.cc b/xla/pjrt/event_pool.cc
index 34c37a1b00..a5e2f83c3e 100644
--- a/xla/pjrt/event_pool.cc
+++ b/xla/pjrt/event_pool.cc
@@ -25,7 +25,7 @@ namespace xla {
 
 EventPool::Handle::~Handle() {
   if (pool_ && event_) {
-    absl::MutexLock lock(&pool_->mu_);
+    absl::MutexLock lock(&pool_->mu_free_events_);
     pool_->free_events_.push(std::move(event_));
   }
 }
@@ -39,7 +39,7 @@ absl::StatusOr<EventPool::Handle> EventPool::AllocateEvent(
 
   if (allow_reuse_) {
     event.pool_ = this;
-    absl::MutexLock lock(&mu_);
+    absl::MutexLock lock(&mu_free_events_);
     if (!free_events_.empty()) {
       event.event_ = std::move(free_events_.top());
       free_events_.pop();
@@ -53,7 +53,7 @@ absl::StatusOr<EventPool::Handle> EventPool::AllocateEvent(
 }
 
 void EventPool::ThenRecordEvent(se::Stream* stream, EventPool::Handle& handle) {
-  absl::MutexLock lock(&mu_);
+  absl::MutexLock lock(&mu_sequence_number_);
   stream->RecordEvent(handle.event_.get()).IgnoreError();
   handle.sequence_number_ = next_sequence_number_++;
 }
diff --git a/xla/pjrt/event_pool.h b/xla/pjrt/event_pool.h
index 89b8f6d816..2ce9bc7cbc 100644
--- a/xla/pjrt/event_pool.h
+++ b/xla/pjrt/event_pool.h
@@ -86,9 +86,12 @@ class EventPool {
  private:
   const bool allow_reuse_;
 
-  absl::Mutex mu_;
-  std::stack<std::unique_ptr<se::Event>> free_events_ ABSL_GUARDED_BY(mu_);
-  uint64_t next_sequence_number_ ABSL_GUARDED_BY(mu_);
+  absl::Mutex mu_free_events_;
+  std::stack<std::unique_ptr<se::Event>> free_events_
+      ABSL_GUARDED_BY(mu_free_events_);
+
+  absl::Mutex mu_sequence_number_;
+  uint64_t next_sequence_number_ ABSL_GUARDED_BY(mu_sequence_number_);
 };
 
 }  // namespace xla
diff --git a/xla/pjrt/gpu/BUILD b/xla/pjrt/gpu/BUILD
index 8586df6643..08c476541c 100644
--- a/xla/pjrt/gpu/BUILD
+++ b/xla/pjrt/gpu/BUILD
@@ -40,6 +40,7 @@ cc_library(
     defines = if_cuda(["GOOGLE_CUDA=1"]) + if_rocm(["TENSORFLOW_USE_ROCM=1"]),
     visibility = internal_visibility(["//xla/pjrt:friends"]),
     deps = [
+        "@intel_extension_for_openxla//xla/stream_executor/sycl:hw_info",
         ":gpu_helpers",
         ":gpu_metrics",
         ":gpu_topology",
@@ -84,6 +85,7 @@ cc_library(
         "//xla/stream_executor:device_memory",
         "//xla/stream_executor:device_memory_allocator",
         "//xla/stream_executor:platform",
+        "//xla/stream_executor/gpu:gpu_malloc_allocator",
         "//xla/stream_executor/integrations:device_mem_allocator",
         "//xla/stream_executor/integrations:tf_allocator_adapter",
         "//xla/tsl/util:env_var",
diff --git a/xla/pjrt/gpu/gpu_helpers.cc b/xla/pjrt/gpu/gpu_helpers.cc
index c9c6fe4da4..a56a45f003 100644
--- a/xla/pjrt/gpu/gpu_helpers.cc
+++ b/xla/pjrt/gpu/gpu_helpers.cc
@@ -37,9 +37,10 @@ namespace xla {
 absl::StatusOr<LocalClient*> GetGpuXlaClient(
     const std::optional<std::string>& platform_name,
     const std::optional<std::set<int>>& allowed_devices) {
+  // SYCL: hardcode to SYCL
   TF_ASSIGN_OR_RETURN(
       se::Platform * platform,
-      PlatformUtil::GetPlatform(platform_name ? *platform_name : "gpu"));
+      PlatformUtil::GetPlatform(platform_name ? *platform_name : "SYCL"));
   if (platform->VisibleDeviceCount() <= 0) {
     return FailedPrecondition("No visible GPU devices.");
   }
diff --git a/xla/pjrt/gpu/se_gpu_pjrt_client.cc b/xla/pjrt/gpu/se_gpu_pjrt_client.cc
index 9dec918f99..884b799487 100644
--- a/xla/pjrt/gpu/se_gpu_pjrt_client.cc
+++ b/xla/pjrt/gpu/se_gpu_pjrt_client.cc
@@ -84,6 +84,9 @@ limitations under the License.
 #include "tsl/platform/threadpool.h"
 #include "tsl/profiler/lib/connected_traceme.h"
 #include "tsl/profiler/lib/traceme.h"
+#include "xla/stream_executor/gpu/gpu_malloc_allocator.h"
+#include "xla/stream_executor/sycl/hw_info.h"
+#include "xla/tsl/util/env_var.h"
 
 #if defined(GOOGLE_CUDA) || defined(TENSORFLOW_USE_ROCM)
 #include "xla/pjrt/compile_options.pb.h"
@@ -475,7 +478,7 @@ absl::string_view StreamExecutorGpuClient::platform_version() const {
 #elif GOOGLE_CUDA && defined(CUDART_VERSION)  // cuda
   return "cuda " STRINGIFY(CUDART_VERSION);
 #else
-  return "<unknown>";
+  return "sycl";
 #endif  // TENSORFLOW_USE_ROCM && defined(TF_ROCM_VERSION)
 }
 
@@ -750,6 +753,27 @@ StreamExecutorGpuClient::Load(std::unique_ptr<PjRtExecutable> executable) {
 
 namespace {
 
+absl::StatusOr<std::vector<se::MultiDeviceAdapter::AllocatorInfo>>
+CreateDefaultGPUAllocator(
+    se::Platform* platform,
+    const std::map<int, std::unique_ptr<LocalDeviceState>>&
+        addressable_devices) {
+  CHECK_GT(addressable_devices.size(), 0);
+  std::vector<se::MultiDeviceAdapter::AllocatorInfo> allocators;
+
+  for (auto& ordinal_and_device : addressable_devices) {
+    se::StreamExecutor* executor = ordinal_and_device.second->executor();
+    int device_ordinal = executor->device_ordinal();
+
+    auto allocator = std::make_unique<se::GpuMallocAllocator>(
+        executor, tsl::PlatformDeviceId(device_ordinal));
+    allocators.emplace_back(std::move(allocator),
+                            ordinal_and_device.second->compute_stream(),
+                            /*memory_space=*/0);
+  }
+  return allocators;
+}
+
 #if defined(GOOGLE_CUDA) && CUDA_VERSION >= 11020
 
 absl::StatusOr<std::vector<se::MultiDeviceAdapter::AllocatorInfo>>
@@ -864,17 +888,26 @@ GetStreamExecutorGpuDeviceAllocator(
       }
       break;
     }
-
-    case GpuAllocatorConfig::Kind::kPlatform:
-      LOG(INFO) << "Using platform allocator.";
+    case GpuAllocatorConfig::Kind::kPlatform: {
       if (allocator_config.collective_memory_size != 0) {
         LOG(WARNING)
             << "collective_memory_size is non-zero, but allocator kind is set "
                "to \"platform\". Collective memory will not be allocated.";
       }
+      auto allocators_or =
+          CreateDefaultGPUAllocator(platform, addressable_devices);
+      if (allocators_or.ok()) {
+        LOG(INFO) << "Using platform allocator.";
+        allocators = std::move(allocators_or.value());
+        break;
+      }
+      LOG(ERROR) << "Failed to initialize platform allocator: "
+                 << allocators_or.status() << "; Please choose other kind allocator.";
+
       // Returning null will cause the client to use the default backend
       // allocator.
       return nullptr;
+    }
   }
 
   // Add any additional allocators for alternate memory spaces.
diff --git a/xla/python/BUILD b/xla/python/BUILD
index e2c82bad04..8401ec77d8 100644
--- a/xla/python/BUILD
+++ b/xla/python/BUILD
@@ -1,6 +1,11 @@
 load("@bazel_skylib//rules:common_settings.bzl", "bool_flag")
 load("@local_config_cuda//cuda:build_defs.bzl", "if_cuda")
 load("@local_config_rocm//rocm:build_defs.bzl", "if_rocm")
+load("@local_config_sycl//sycl:build_defs.bzl", "if_sycl_is_configured")
+load(
+    "//xla/stream_executor:build_defs.bzl",
+    "if_gpu_is_configured",
+)
 load(
     "@tsl//tsl:tsl.bzl",
     "if_cuda_or_rocm",
@@ -441,10 +446,10 @@ cc_library(
 
 cc_library(
     name = "py_client_gpu",
-    srcs = if_cuda_or_rocm([
+    srcs = if_gpu_is_configured([
         "py_client_gpu.cc",
     ]),
-    hdrs = if_cuda_or_rocm([
+    hdrs = if_gpu_is_configured([
         "py_client_gpu.h",
     ]),
     compatible_with = [],
@@ -471,6 +476,9 @@ cc_library(
         "@local_config_cuda//cuda:cuda_headers",
     ]) + if_rocm([
         "@local_config_rocm//rocm:rocm_headers",
+    ]) + if_sycl_is_configured([
+        "@intel_extension_for_openxla//xla/stream_executor:sycl_platform",
+        "@local_config_sycl//sycl:sycl_headers",
     ]),
 )
 
diff --git a/xla/python/py_client.cc b/xla/python/py_client.cc
index 0afd053313..f2116e7f5a 100644
--- a/xla/python/py_client.cc
+++ b/xla/python/py_client.cc
@@ -91,9 +91,9 @@ limitations under the License.
 #include "tsl/platform/status.h"
 #include "tsl/platform/statusor.h"
 
-#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM
+#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM || TENSORFLOW_USE_SYCL
 #include "xla/python/py_client_gpu.h"
-#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM
+#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM || TENSORFLOW_USE_SYCL
 
 namespace xla {
 
@@ -670,7 +670,7 @@ PyClient::GetEmitPythonCallbackDescriptor(nb::callable callable,
 XLA_CPU_REGISTER_CUSTOM_CALL_TARGET_WITH_SYM("xla_python_cpu_callback",
                                              &XlaPythonCpuCallback);
 
-#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM
+#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM || TENSORFLOW_USE_SYCL
 XLA_REGISTER_CUSTOM_CALL_TARGET_WITH_SYM(
     "xla_python_gpu_callback", &XlaPythonGpuCallback,
     absl::AsciiStrToUpper(PlatformUtil::CanonicalPlatformName("gpu").value()));
diff --git a/xla/python/py_client_gpu.cc b/xla/python/py_client_gpu.cc
index 100d9fd599..642828a9ce 100644
--- a/xla/python/py_client_gpu.cc
+++ b/xla/python/py_client_gpu.cc
@@ -12,6 +12,7 @@ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 See the License for the specific language governing permissions and
 limitations under the License.
 ==============================================================================*/
+#include "xla/python/py_client_gpu.h"
 
 #include <vector>
 
@@ -20,6 +21,8 @@ limitations under the License.
 #include "tsl/platform/errors.h"
 #if TENSORFLOW_USE_ROCM
 #include "rocm/include/hip/hip_runtime.h"
+#elif TENSORFLOW_USE_SYCL
+#include "xla/stream_executor/sycl/sycl_gpu_runtime.h"
 #else
 #include "third_party/gpus/cuda/include/cuda.h"
 #include "third_party/gpus/cuda/include/cuda_runtime_api.h"
@@ -38,6 +41,13 @@ limitations under the License.
 #define gpuStreamSynchronize hipStreamSynchronize
 #define gpuMemcpyDeviceToHost hipMemcpyDeviceToHost
 #define gpuMemcpyHostToDevice hipMemcpyHostToDevice
+#elif TENSORFLOW_USE_SYCL
+#define gpuSuccess SYCL_SUCCESS
+#define gpuStreamHandle ::sycl::queue*
+#define gpuMemcpyAsync SYCLMemcpyAsync
+#define gpuStreamSynchronize SYCLStreamSynchronize
+#define gpuMemcpyDeviceToHost SYCLMemcpyDtoHAsync
+#define gpuMemcpyHostToDevice SYCLMemcpyHtoDAsync
 #else
 #define gpuSuccess cudaSuccess
 #define gpuStreamHandle CUstream
diff --git a/xla/python/py_client_gpu.h b/xla/python/py_client_gpu.h
index d7675e1b6a..17da528bec 100644
--- a/xla/python/py_client_gpu.h
+++ b/xla/python/py_client_gpu.h
@@ -18,6 +18,8 @@ limitations under the License.
 
 #if TENSORFLOW_USE_ROCM
 #include "rocm/include/hip/hip_runtime.h"
+#elif TENSORFLOW_USE_SYCL
+#include "xla/stream_executor/sycl/sycl_gpu_runtime.h"
 #else
 #include "third_party/gpus/cuda/include/cuda.h"
 #endif
@@ -25,8 +27,10 @@ limitations under the License.
 
 #if TENSORFLOW_USE_ROCM
 #define gpuStreamHandle hipStream_t
-#else
+#elif GOOGLE_CUDA
 #define gpuStreamHandle CUstream
+#else
+#define gpuStreamHandle ::sycl::queue*
 #endif
 
 namespace xla {
diff --git a/xla/service/BUILD b/xla/service/BUILD
index bcedb98906..952e4c5f6f 100644
--- a/xla/service/BUILD
+++ b/xla/service/BUILD
@@ -8,6 +8,10 @@ load(
     "if_rocm",
     "if_rocm_is_configured",
 )
+load(
+    "@local_config_sycl//sycl:build_defs.bzl",
+    "if_sycl_is_configured",
+)
 load("@tsl//tsl:tsl.bzl", "if_google", "if_libtpu", "internal_visibility", "tsl_copts")
 load("@tsl//tsl:tsl.default.bzl", "filegroup", "get_compatible_with_portable", "internal_hlo_deps")
 load(
@@ -1368,6 +1372,9 @@ cc_library(
     ]) + if_rocm_is_configured([
         "//xla/service/gpu:amdgpu_compiler",
         "//xla/stream_executor/rocm:stream_executor_rocm",
+    ]) + if_sycl_is_configured([
+        "//xla/service/gpu:spir_compiler",
+        "@intel_extension_for_openxla//xla/stream_executor/sycl:stream_executor_sycl",
     ]),
 )
 
@@ -3905,6 +3912,7 @@ cc_library(
         "//xla/stream_executor/cuda:cuda_platform_id",
         "//xla/stream_executor/host:host_platform_id",
         "//xla/stream_executor/rocm:rocm_platform_id",
+        "@intel_extension_for_openxla//xla/stream_executor/sycl:sycl_platform_id",
         "@com_google_absl//absl/container:flat_hash_map",
         "@com_google_absl//absl/memory",
         "@com_google_absl//absl/strings",
@@ -7236,6 +7244,19 @@ cc_library(
     ],
 )
 
+cc_library(
+    name = "scatter_promotion",
+    srcs = ["scatter_promotion.cc"],
+    hdrs = ["scatter_promotion.h"],
+    deps = [
+        ":hlo_creation_utils",
+        ":hlo_pass",
+        ":op_expander_pass",
+        "//xla:xla_data_proto_cc",
+        "//xla/hlo/ir:hlo",
+    ],
+)
+
 cc_library(
     name = "scatter_simplifier",
     srcs = ["scatter_simplifier.cc"],
@@ -7311,8 +7332,10 @@ cc_library(
     deps = [
         ":hlo_creation_utils",
         ":hlo_pass",
-        "//xla/service/cpu:onednn_matmul_rewriter",
-    ],
+    ] + if_sycl_is_configured(
+        [],
+        ["//xla/service/cpu:onednn_matmul_rewriter"]
+    ),
 )
 
 cc_library(
diff --git a/xla/service/algebraic_simplifier.h b/xla/service/algebraic_simplifier.h
index 6ec43b9928..c18255ae1b 100644
--- a/xla/service/algebraic_simplifier.h
+++ b/xla/service/algebraic_simplifier.h
@@ -27,6 +27,7 @@ limitations under the License.
 #include <vector>
 
 #include "absl/container/inlined_vector.h"
+#include "xla/tsl/util/env_var.h"
 #include "xla/hlo/ir/dfs_hlo_visitor_with_default.h"
 #include "xla/hlo/ir/hlo_instruction.h"
 #include "xla/hlo/ir/hlo_instructions.h"
@@ -442,7 +443,9 @@ class AlgebraicSimplifierVisitor : public DfsHloRewriteVisitor {
   virtual bool IsValidLayout(const Shape& shape) { return true; }
   // Allow backend targets to determine whether a layout is inefficient.
   virtual bool ShouldStrengthReduceDotToReduce(const HloInstruction* hlo) {
-    return true;
+    bool llm_flag = false;
+    tsl::ReadBoolFromEnvVar("LLM", false, &llm_flag);
+    return !llm_flag;
   }
 
  protected:
diff --git a/xla/service/computation_placer.cc b/xla/service/computation_placer.cc
index b896c7d10c..bd5dcea2b9 100644
--- a/xla/service/computation_placer.cc
+++ b/xla/service/computation_placer.cc
@@ -31,6 +31,7 @@ limitations under the License.
 #include "xla/stream_executor/cuda/cuda_platform_id.h"
 #include "xla/stream_executor/host/host_platform_id.h"
 #include "xla/stream_executor/rocm/rocm_platform_id.h"
+#include "xla/stream_executor/sycl/sycl_platform_id.h"
 #include "xla/types.h"
 #include "xla/util.h"
 #include "tsl/platform/errors.h"
@@ -164,6 +165,11 @@ absl::StatusOr<DeviceAssignment> ComputationPlacer::AssignDevices(
   absl::MutexLock lock(&ComputationPlacer::platform_computation_placer_mutex_);
   auto* computation_placers = GetPlatformComputationPlacers();
   if (computation_placers->find(platform_id) != computation_placers->end()) {
+    // FIXME(intel): Temporarily skip the registry to avoid linking warning.
+    // Will reopen this check once refine oneDNN custom call code.
+#ifdef TENSORFLOW_USE_SYCL
+    return;
+#endif
     // TODO(b/282059652): Consider logging the platform name using
     // PlatformManager::PlatformWithId(). No doing that for now to avoid
     // introducing unwanted dependency.
@@ -216,6 +222,8 @@ static bool InitModule() {
       stream_executor::cuda::kCudaPlatformId, &CreateComputationPlacer);
   xla::ComputationPlacer::RegisterComputationPlacer(
       stream_executor::rocm::kROCmPlatformId, &CreateComputationPlacer);
+  xla::ComputationPlacer::RegisterComputationPlacer(
+      stream_executor::sycl::kSyclPlatformId, &CreateComputationPlacer);
   return true;
 }
 static bool module_initialized = InitModule();
diff --git a/xla/service/dump.cc b/xla/service/dump.cc
index 5adf5957d5..90f5518d4d 100644
--- a/xla/service/dump.cc
+++ b/xla/service/dump.cc
@@ -625,6 +625,8 @@ void DumpToFileInDirOrStdout(const HloModule& module, string_view file_prefix,
   if (opts.dumping_to_stdout()) return op->dump();
 
   mlir::OpPrintingFlags print_flags = mlir::OpPrintingFlags();
+  // Avoid printing large constant weight.
+  print_flags.elideLargeElementsAttrs(8);
   // Enable debug info so that it is easier to see the corresponding HLO node.
   if (file_prefix == "lmhlo") {
     print_flags.enableDebugInfo(/*enable=*/true,
diff --git a/xla/service/elemental_ir_emitter.cc b/xla/service/elemental_ir_emitter.cc
index af3cc23abd..280bc10a4c 100644
--- a/xla/service/elemental_ir_emitter.cc
+++ b/xla/service/elemental_ir_emitter.cc
@@ -211,6 +211,28 @@ absl::StatusOr<llvm::Value*> EmitReducePrecisionIR(
   return result;
 }
 
+absl::StatusOr<llvm::Value*> DefaultEmitF32ToBF16Impl(llvm::Value* f32_value,
+                                                      llvm::IRBuilder<>* b) {
+  TF_ASSIGN_OR_RETURN(
+      auto reduced_precision,
+      EmitReducePrecisionIR(
+          /*src_ty=*/F32, f32_value,
+          /*dest_exponent_bits=*/primitive_util::ExponentWidth(BF16),
+          /*dest_mantissa_bits=*/primitive_util::SignificandWidth(BF16) - 1,
+          /*quiet_nans=*/true, b));
+  auto as_int32 = b->CreateBitCast(reduced_precision, b->getInt32Ty());
+  auto shifted = b->CreateLShr(as_int32, 16);
+  auto truncated = b->CreateTrunc(shifted, b->getInt16Ty());
+  return b->CreateBitCast(truncated, b->getInt16Ty());
+}
+
+llvm::Value* EmitBF16ToF32(llvm::Value* bf16_value, llvm::IRBuilder<>* b) {
+  auto as_int16 = b->CreateBitCast(bf16_value, b->getInt16Ty());
+  auto as_int32 = b->CreateZExt(as_int16, b->getInt32Ty());
+  auto shifted = b->CreateShl(as_int32, 16);
+  return b->CreateBitCast(shifted, b->getFloatTy());
+}
+
 absl::StatusOr<llvm::Value*> EmitF16ToF8e5m2(llvm::Value* f16_value,
                                              llvm::IRBuilder<>* b) {
   TF_ASSIGN_OR_RETURN(
@@ -589,6 +611,10 @@ absl::StatusOr<llvm::Value*> ElementalIrEmitter::EmitIntegerUnaryOp(
                        primitive_util::IsSignedIntegralType(from_type));
       }
       if (primitive_util::IsFloatingPointType(to_type)) {
+        if (to_type == BF16) {
+          return EmitF32ToBF16(EmitIntegralToFloating(operand_value, from_type,
+                                                      F32, module_, b_));
+        }
         if (to_type == F8E5M2) {
           return EmitF16ToF8e5m2(
               EmitIntegralToFloating(operand_value, from_type, F16, module_,
@@ -718,8 +744,7 @@ absl::StatusOr<llvm::Value*> ElementalIrEmitter::EmitFloatUnaryOp(
       }
       if (from_type == BF16) {
         TF_RET_CHECK(to_type != BF16);
-        // The code below expects the source type to be F32.
-        operand_value = b_->CreateFPExt(operand_value, b_->getFloatTy());
+        operand_value = EmitBF16ToF32(operand_value, b_);
         from_type = F32;
         if (from_type == to_type) {
           return operand_value;
@@ -774,11 +799,13 @@ absl::StatusOr<llvm::Value*> ElementalIrEmitter::EmitFloatUnaryOp(
             nullptr);
       }
       if (to_type == BF16) {
-        // F16 to BF16 has to go through an intermediate F32.
-        if (from_type == F16) {
-          operand_value = b_->CreateFPExt(operand_value, b_->getFloatTy());
+        // Cast to F32 first. Other floating point formats are not supported by
+        // EmitReducePrecisionIR.
+        if (from_type != F32) {
+          operand_value = b_->CreateFPCast(
+              operand_value, llvm_ir::PrimitiveTypeToIrType(F32, module_));
         }
-        return FPCast(operand_value, b_->getBFloatTy());
+        return EmitF32ToBF16(operand_value);
       }
       if (to_type == F8E5M2) {
         // Cast to F16 first. Casts to F8E5M2 must be from F16.
@@ -1293,7 +1320,10 @@ absl::StatusOr<llvm::Value*> ElementalIrEmitter::EmitFloatBinaryOp(
     // matches C++'s semantics.
     case HloOpcode::kCompare: {
       PrimitiveType operand_type = op->operand(0)->shape().element_type();
-      if (operand_type == F8E5M2) {
+      if (operand_type == BF16) {
+        lhs_value = EmitBF16ToF32(lhs_value, b_);
+        rhs_value = EmitBF16ToF32(rhs_value, b_);
+      } else if (operand_type == F8E5M2) {
         lhs_value = EmitF8e5m2ToF16(lhs_value, b_);
         rhs_value = EmitF8e5m2ToF16(rhs_value, b_);
       } else if (operand_type == F8E4M3FN) {
@@ -2979,8 +3009,8 @@ absl::StatusOr<llvm::Value*> ElementalIrEmitter::EmitElementalDot(
   TF_ASSIGN_OR_RETURN(llvm::Value * rhs_value, rhs_generator(rhs_index));
 
   if (primitive_type == BF16) {
-    lhs_value = b_->CreateFPExt(lhs_value, b_->getFloatTy());
-    rhs_value = b_->CreateFPExt(rhs_value, b_->getFloatTy());
+    lhs_value = EmitBF16ToF32(lhs_value, b_);
+    rhs_value = EmitBF16ToF32(rhs_value, b_);
   }
 
   llvm::Value* next_accumulator =
@@ -2992,7 +3022,7 @@ absl::StatusOr<llvm::Value*> ElementalIrEmitter::EmitElementalDot(
   llvm::Value* result =
       Load(accumulator_alloca->getAllocatedType(), accumulator_alloca);
 
-  return primitive_type == BF16 ? FPTrunc(result, b_->getBFloatTy()) : result;
+  return primitive_type == BF16 ? EmitF32ToBF16(result) : result;
 }
 
 llvm_ir::ElementGenerator ElementalIrEmitter::MakeElementGenerator(
@@ -3145,7 +3175,9 @@ llvm_ir::ElementGenerator ElementalIrEmitter::MakeElementGenerator(
               primitive_util::IsFloatingPointType(component_element_type))
               << component_element_type;
           llvm::Type* float_ir_type;
-          if (component_element_type == F8E4M3FNUZ) {
+          if (component_element_type == BF16) {
+            float_ir_type = llvm_ir::PrimitiveTypeToIrType(F32, module_);
+          } else if (component_element_type == F8E4M3FNUZ) {
             float_ir_type = llvm_ir::PrimitiveTypeToIrType(F16, module_);
           } else if (component_element_type == F8E5M2FNUZ) {
             float_ir_type = llvm_ir::PrimitiveTypeToIrType(F16, module_);
@@ -3155,8 +3187,10 @@ llvm_ir::ElementGenerator ElementalIrEmitter::MakeElementGenerator(
           }
           llvm::Value* float_val =
               b_->CreateUIToFP(elem_index_linear, float_ir_type);
-          if (component_element_type == F8E4M3FNUZ ||
-              component_element_type == F8E5M2FNUZ) {
+          if (component_element_type == BF16) {
+            TF_ASSIGN_OR_RETURN(iota_result, EmitF32ToBF16(float_val));
+          } else if (component_element_type == F8E4M3FNUZ ||
+                     component_element_type == F8E5M2FNUZ) {
             TF_ASSIGN_OR_RETURN(
                 iota_result, EmitFloatingToF8fnuz(F16, float_val,
                                                   component_element_type, b_));
@@ -3304,6 +3338,11 @@ llvm::Value* ElementalIrEmitter::EmitExtractImag(llvm::Value* value) {
   return ExtractValue(value, {1});
 }
 
+absl::StatusOr<llvm::Value*> ElementalIrEmitter::EmitF32ToBF16(
+    llvm::Value* f32_value) {
+  return DefaultEmitF32ToBF16Impl(f32_value, b_);
+}
+
 llvm::Value* ElementalIrEmitter::EmitComposeComplex(const HloInstruction* op,
                                                     llvm::Value* real,
                                                     llvm::Value* imag) {
diff --git a/xla/service/elemental_ir_emitter.h b/xla/service/elemental_ir_emitter.h
index b636cb82df..ef057e5d4d 100644
--- a/xla/service/elemental_ir_emitter.h
+++ b/xla/service/elemental_ir_emitter.h
@@ -76,6 +76,8 @@ class ElementalIrEmitter : public IrBuilderMixin<ElementalIrEmitter> {
   virtual llvm::Value* EmitExtractReal(llvm::Value* value);
   virtual llvm::Value* EmitExtractImag(llvm::Value* value);
 
+  virtual absl::StatusOr<llvm::Value*> EmitF32ToBF16(llvm::Value* f32_value);
+
  private:
   virtual absl::StatusOr<llvm::Value*> EmitUnaryOp(const HloInstruction* op,
                                                    llvm::Value* operand_value);
diff --git a/xla/service/float8_fnuz_ir_emitter.cc b/xla/service/float8_fnuz_ir_emitter.cc
index 42c421b7ee..d125142d2d 100644
--- a/xla/service/float8_fnuz_ir_emitter.cc
+++ b/xla/service/float8_fnuz_ir_emitter.cc
@@ -19,6 +19,7 @@ limitations under the License.
 
 #include "llvm/IR/Constants.h"
 #include "llvm/IR/Intrinsics.h"
+#include "llvm/TargetParser/Triple.h"
 #include "xla/primitive_util.h"
 #include "xla/status_macros.h"
 #include "xla/util.h"
@@ -73,7 +74,7 @@ absl::StatusOr<llvm::Type*> PrimitiveTypeToLLVMType(llvm::IRBuilder<>* b,
     case F8E5M2FNUZ:
       return b->getInt8Ty();
     case BF16:
-      return b->getBFloatTy();
+      return b->getInt16Ty();
     case F16:
       return b->getHalfTy();
     case F32:
@@ -609,13 +610,17 @@ absl::StatusOr<llvm::Value*> EmitF8fnuzToFloating(PrimitiveType input_type,
         llvm::Constant* result_lut_array =
             llvm::ConstantArray::get(result_lut_array_type, result_lut);
 
+        int addrspace = llvm::Triple(module->getTargetTriple()).isSPIR() ? 1 : 0;
         return new llvm::GlobalVariable(
             /*M=*/*module,
             /*Ty=*/result_lut_array_type,
             /*isConstant=*/true,
             /*Linkage=*/llvm::GlobalValue::PrivateLinkage,
             /*Initializer=*/result_lut_array,
-            /*Name=*/lut_name);
+            /*Name=*/lut_name,
+            /*InsertBefore*/nullptr,
+            /*TLMode=*/llvm::GlobalValue::NotThreadLocal,
+            /*AddressSpace=*/addrspace);
       });
 
   // Check for NaN, since it's a special case.
diff --git a/xla/service/gpu/BUILD b/xla/service/gpu/BUILD
index 1fcb30d32e..bc776288c6 100644
--- a/xla/service/gpu/BUILD
+++ b/xla/service/gpu/BUILD
@@ -32,6 +32,7 @@ load(
     "@tsl//tsl/platform/default:cuda_build_defs.bzl",
     "if_cuda_is_configured",
 )
+load("@local_config_sycl//sycl:build_defs.bzl", "if_sycl_is_configured")
 load("//xla:xla.bzl", "xla_cc_test", "xla_cub_deps", "xla_export_hlo_deps")
 load(
     "//xla/service/gpu:build_defs.bzl",
@@ -301,7 +302,8 @@ cc_library(
         "//xla/service:buffer_assignment",
         "//xla/service:name_uniquer",
         "//xla/service/gpu/model:indexing_map",
-        "//xla/service/gpu/runtime:nccl_collective_thunk",
+        # "//xla/service/gpu/runtime:nccl_collective_thunk",
+        "@intel_extension_for_openxla//xla/service/gpu:ccl_collective_thunks",
         "//xla/stream_executor:device_description",
         "@com_google_absl//absl/algorithm:container",
         "@com_google_absl//absl/container:flat_hash_map",
@@ -363,9 +365,9 @@ cc_library(
         "//xla/service/gpu/fusions:thunk_util",
         "//xla/service/gpu/kernels:custom_kernel",
         "//xla/service/gpu/kernels:topk_custom_kernel",
-        "//xla/service/gpu/runtime:command_buffer_cmd",
-        "//xla/service/gpu/runtime:command_buffer_cmd_emitter",
-        "//xla/service/gpu/runtime:command_buffer_thunk",
+        # "//xla/service/gpu/runtime:command_buffer_cmd",
+        # "//xla/service/gpu/runtime:command_buffer_cmd_emitter",
+        # "//xla/service/gpu/runtime:command_buffer_thunk",
         "//xla/service/gpu/runtime:conditional_thunk",
         "//xla/service/gpu/runtime:convolution_thunk",
         "//xla/service/gpu/runtime:copy_thunk",
@@ -375,15 +377,15 @@ cc_library(
         "//xla/service/gpu/runtime:gemm_thunk",
         "//xla/service/gpu/runtime:infeed_thunk",
         "//xla/service/gpu/runtime:kernel_thunk",
-        "//xla/service/gpu/runtime:nccl_all_gather_thunk",
-        "//xla/service/gpu/runtime:nccl_all_reduce_thunk",
-        "//xla/service/gpu/runtime:nccl_all_to_all_thunk",
+        # "//xla/service/gpu/runtime:nccl_all_gather_thunk",
+        # "//xla/service/gpu/runtime:nccl_all_reduce_thunk",
+        # "//xla/service/gpu/runtime:nccl_all_to_all_thunk",
         "//xla/service/gpu/runtime:nccl_api",
-        "//xla/service/gpu/runtime:nccl_collective_broadcast_thunk",
-        "//xla/service/gpu/runtime:nccl_collective_permute_thunk",
-        "//xla/service/gpu/runtime:nccl_collective_thunk",
-        "//xla/service/gpu/runtime:nccl_recv_thunk",
-        "//xla/service/gpu/runtime:nccl_send_thunk",
+        # "//xla/service/gpu/runtime:nccl_collective_broadcast_thunk",
+        # "//xla/service/gpu/runtime:nccl_collective_permute_thunk",
+        # "//xla/service/gpu/runtime:nccl_collective_thunk",
+        # "//xla/service/gpu/runtime:nccl_recv_thunk",
+        # "//xla/service/gpu/runtime:nccl_send_thunk",
         "//xla/service/gpu/runtime:norm_thunk",
         "//xla/service/gpu/runtime:outfeed_thunk",
         "//xla/service/gpu/runtime:replica_id_thunk",
@@ -431,10 +433,12 @@ cc_library(
         "@tsl//tsl/platform:human_readable_json",
         "@tsl//tsl/platform:statusor",
         "@tsl//tsl/protobuf:dnn_proto_cc",
+        "@intel_extension_for_openxla//xla/service/gpu:ccl_collective_thunks",
+        "@intel_extension_for_openxla//xla/service/gpu:sycl_custom_call",
     ] + if_gpu_is_configured([
-        ":ir_emitter_triton",
+        # ":ir_emitter_triton",
         "//xla/service/gpu/runtime:cholesky_thunk",
-        "//xla/service/gpu/runtime:cub_sort_thunk",
+        # "//xla/service/gpu/runtime:cub_sort_thunk",
         "//xla/service/gpu/runtime:gpublas_lt_matmul_thunk",
         "//xla/service/gpu/runtime:triangular_solve_thunk",
     ]) + if_rocm_is_configured([
@@ -870,6 +874,7 @@ cc_library(
     hdrs = ["nccl_clique_key.h"],
     compatible_with = get_compatible_with_portable(),
     deps = [
+        "//xla:executable_run_options",
         "//xla/service:global_device_id",
         "@com_google_absl//absl/algorithm:container",
         "@com_google_absl//absl/container:btree",
@@ -1176,6 +1181,7 @@ cc_library(
         "@tsl//tsl/platform:statusor",
         "@tsl//tsl/profiler/lib:scoped_annotation",
         "@tsl//tsl/profiler/lib:traceme",
+        "@intel_extension_for_openxla//xla/service/gpu:xetla_gpu_fused_mha_runner",
     ] + if_gpu_is_configured([
         ":make_batch_pointers",
     ]) + if_cuda_is_configured([
@@ -2385,6 +2391,8 @@ cc_library(
         "//xla/stream_executor/rocm:rocblas_wrapper",
         "//xla/stream_executor/rocm:rocsolver_wrapper",
         "//xla/stream_executor/rocm:hipsolver_wrapper",
+    ]) + if_sycl_is_configured([
+        "@local_config_sycl//sycl:sycl_headers",
     ]),
 )
 
@@ -3155,6 +3163,7 @@ cc_library(
         "//xla/stream_executor/cuda:cuda_platform_id",
         "//xla/stream_executor/host:host_platform_id",
         "//xla/stream_executor/rocm:rocm_platform_id",
+        "@intel_extension_for_openxla//xla/stream_executor/sycl:sycl_platform_id",
         "@com_google_absl//absl/base:core_headers",
         "@com_google_absl//absl/cleanup",
         "@com_google_absl//absl/container:node_hash_map",
@@ -3489,6 +3498,7 @@ cc_library(
         "TENSORFLOW_USE_ROCM=1",
     ]),
     deps = if_gpu_is_configured([
+        "@intel_extension_for_openxla//xla/service/gpu:dot_expand_dims",
         ":gpu_p2p_pipeliner",
         ":collective_permute_cycle_decomposer",
         ":address_computation_fusion_rewriter",
@@ -3645,6 +3655,7 @@ cc_library(
         "//xla/service:result_caster",
         "//xla/service:rng_bit_generator_expander",
         "//xla/service:rng_expander",
+        "//xla/service:scatter_promotion",
         "//xla/service:scatter_simplifier",
         "//xla/service:sharding_propagation",
         "//xla/service:sharding_remover",
@@ -3962,6 +3973,61 @@ xla_test(
     ],
 )
 
+cc_library(
+    name = "spir_compiler_impl",
+    srcs = [
+        "spir_compiler.cc",
+    ],
+    hdrs = [
+        "spir_compiler.h",
+    ],
+    deps = [
+        "@intel_extension_for_openxla//xla/service/gpu:gemm_impl_picker",
+        "@intel_extension_for_openxla//xla/service/gpu:redundant_convert_mover",
+        "@intel_extension_for_openxla//xla/stream_executor/sycl:hw_info",
+        "@intel_extension_for_openxla//xla/stream_executor/sycl:sycl_platform_id",
+        "@com_google_absl//absl/base",
+        "@com_google_absl//absl/container:node_hash_map",
+        "@com_google_absl//absl/types:optional",
+        "@llvm-project//llvm:IRReader",
+        "@llvm-project//llvm:Support",
+        "//xla/service:dot_dimension_merger",
+        "//xla/service:float_normalization",
+        "//xla/service:float_support",
+        "//xla/service:hlo_constant_folding",
+        "//xla/service:hlo_cse",
+        "//xla/service:hlo_dce",
+        "//xla/service:hlo_pass",
+        "//xla/service:hlo_pass_pipeline",
+        "//xla/service:hlo_proto_cc",
+        "//xla/service:hlo_verifier",
+        "//xla/service:llvm_compiler",
+        "//xla/service:reshape_mover",
+        "//xla/service:tuple_simplifier",
+        "//xla/service/gpu:cudnn_fused_conv_rewriter",
+        "//xla/service/gpu:cudnn_fused_mha_rewriter",
+        "//xla/service/gpu:cusolver_rewriter",
+        "//xla/service/gpu:gpu_compiler",
+        "//xla/service/gpu:gpu_conv_padding_legalization",
+        "//xla/service/gpu:target_constants",
+        "//xla/service/gpu:triangular_solve_rewriter",
+        "//xla/service/gpu/llvm_gpu_backend",
+    ],
+)
+
+cc_library(
+    name = "spir_compiler",
+    srcs = [
+        "spir_compiler_registration.cc",
+    ],
+    deps = [
+        ":spir_compiler_impl",
+        "@intel_extension_for_openxla//xla/stream_executor/sycl:sycl_platform_id",
+        "@tsl//tsl/platform:path",
+    ],
+    alwayslink = True,  # Contains compiler registration
+)
+
 xla_cc_test(
     name = "gpu_aot_compilation_test",
     srcs = if_gpu_is_configured([
diff --git a/xla/service/gpu/buffer_sharing.cc b/xla/service/gpu/buffer_sharing.cc
index 4bdd18ae63..c845a910c2 100644
--- a/xla/service/gpu/buffer_sharing.cc
+++ b/xla/service/gpu/buffer_sharing.cc
@@ -216,6 +216,15 @@ std::optional<bool> CanShareBufferHint(const HloInstruction* user,
                 ->gemm_backend_config();
         return (config.beta() != 0.) && user->operand(2) == operand;
       }
+      // SYCL: inplace sum for onednn conv with side input.
+      if (user->custom_call_target() ==
+          kCudnnConvBiasActivationForwardCallTarget) {
+        CudnnConvBackendConfig config =
+            std::move(user->backend_config<GpuBackendConfig>())
+                ->cudnn_conv_backend_config();
+        return (config.side_input_scale() != 0.) &&
+               (user->operand(user->operand_count() - 1) == operand);
+      }
       // The operand of cholesky can be shared with the first output.
       if (user->custom_call_target() == kCusolverCholeskyCallTarget) {
         return user_index.size() == 1 && user_index[0] == 0;
diff --git a/xla/service/gpu/cudnn_fused_conv_rewriter.cc b/xla/service/gpu/cudnn_fused_conv_rewriter.cc
index a1f9bc04a0..1e4651d7ae 100644
--- a/xla/service/gpu/cudnn_fused_conv_rewriter.cc
+++ b/xla/service/gpu/cudnn_fused_conv_rewriter.cc
@@ -858,6 +858,13 @@ absl::StatusOr<bool> FuseBiasOrSideInput(HloComputation* comp) {
         conv->CloneWithNewOperands(conv->shape(), new_operands));
     comp->parent()->SetAndUniquifyInstrName(new_conv, conv->name());
     TF_RETURN_IF_ERROR(new_conv->set_backend_config(gpu_config));
+#if TENSORFLOW_USE_SYCL
+    if (can_accept_side_input) {
+      xla::Cast<HloCustomCallInstruction>(new_conv)
+          ->set_output_to_operand_aliasing(
+              {{{0}, {static_cast<long>(new_operands.size()) - 1, {}}}});
+    }
+#endif
     TF_ASSIGN_OR_RETURN(HloInstruction * new_instr,
                         MakeGetTupleElementHlo(new_conv, 0));
     TF_RETURN_IF_ERROR(comp->ReplaceInstruction(instr, new_instr));
diff --git a/xla/service/gpu/cudnn_fused_mha_rewriter.cc b/xla/service/gpu/cudnn_fused_mha_rewriter.cc
index 632eb42f2b..92e7ad5b44 100644
--- a/xla/service/gpu/cudnn_fused_mha_rewriter.cc
+++ b/xla/service/gpu/cudnn_fused_mha_rewriter.cc
@@ -237,12 +237,14 @@ auto GetUnfusedReduceMaxSumSoftmaxPattern(
   auto unfused_softmax_max_subpattern = m::SharedSubpattern(
       m::Subtract(
           m::Op(),
-          m::Broadcast(OptionalConvert(
-              m::Op()
-                  .WithPredicate(IsReduceMax)
-                  .WithOneUse()
-                  .WithOperand(0, OptionalBitcast(OptionalConvert(
-                                      m::Op(softmax_input).WithNumUser(2)))))))
+          m::Broadcast(
+              OptionalBitcast(OptionalConvert(OptionalBitcast(OptionalConvert(
+                  m::Op()
+                      .WithPredicate(IsReduceMax)
+                      .WithOneUse()
+                      .WithOperand(
+                          0, OptionalBitcast(OptionalConvert(
+                                 m::Op(softmax_input).WithNumUser(2))))))))))
           .WithOneUse());
   // The reduce-add part of the softmax
   // reduce_sum and reduce_sum_broadcast should have 2 users in training
@@ -251,12 +253,12 @@ auto GetUnfusedReduceMaxSumSoftmaxPattern(
       OptionalBitcast(m::Exp(unfused_softmax_max_subpattern)),
       m::Broadcast(
           softmax_reduce_sum_bcast,
-          OptionalConvert(
+          OptionalBitcast(OptionalConvert(
               m::Op(softmax_reduce_sum)
                   .WithOperand(0, OptionalBitcast(OptionalConvert(
                                       m::Exp(unfused_softmax_max_subpattern))))
                   .WithPredicate(IsReduceSum)
-                  .WithAtMostNumUser(2)))
+                  .WithAtMostNumUser(2))))
           .WithAtMostNumUser(2)));
   return unfused_softmax_sum_subpattern;
 }
@@ -429,6 +431,9 @@ absl::StatusOr<bool> IsFusedAttention(
     stream_executor::CudaComputeCapability cc,
     stream_executor::dnn::VersionInfo cudnn_version) {
   // otherwise check if it is supported by regular attention
+#ifdef TENSORFLOW_USE_SYCL
+  return true;
+#endif
   int64_t s_q = qkv_layout.seqlen_q;
   int64_t s_kv = qkv_layout.seqlen_kv;
   int64_t hidden_dim = qkv_layout.hidden_dim;
@@ -447,6 +452,7 @@ absl::StatusOr<bool> IsFlashAttention(
   int64_t s_q = qkv_layout.seqlen_q;
   int64_t s_kv = qkv_layout.seqlen_kv;
   int64_t hidden_dim = qkv_layout.hidden_dim;
+#if !TENSORFLOW_USE_SYCL
   // start with most relaxed constraint
   bool is_seqlen_supported = (s_q > 512 || s_kv > 512) &&
                              (!is_training || (s_q % 2 == 0 && s_kv % 2 == 0));
@@ -483,6 +489,11 @@ absl::StatusOr<bool> IsFlashAttention(
     VLOG(2) << "Require cuDNN 8.9.4 to run flash attention.";
     return false;
   }
+#else
+  auto is_hidden_dim_supported =
+      hidden_dim <= 256 && hidden_dim % 2 == 0;
+  auto is_flash_attention = is_hidden_dim_supported;
+#endif
   return is_flash_attention;
 }
 
@@ -785,6 +796,12 @@ MatchFwdResult MatchFwdMHAPatternsForCanonicalization(HloInstruction* instr) {
       continue;
     }
     has_dropout = match_result.matched_dropout_rate > 0.0;
+#if TENSORFLOW_USE_SYCL
+    if (has_dropout) {
+      match_result.has_match = false;
+      return match_result;
+    }
+#endif
     match_result = MatchBmm1UnfusedBiasSoftmaxBmm2(
         match_result, match_result.matched_softmax_input, has_dropout);
     if (match_result.has_match) {
@@ -1252,6 +1269,7 @@ absl::StatusOr<bool> IsMHABlockSupported(
   TF_ASSIGN_OR_RETURN(
       is_flash_attention,
       IsFlashAttention(qkv_layout.value(), is_training, cc, cudnn_version));
+#if !TENSORFLOW_USE_SYCL
   if (is_flash_attention) {
     if (is_causal_mask) {
       // if bias is causal mask, needs to remove bias from name
@@ -1268,6 +1286,11 @@ absl::StatusOr<bool> IsMHABlockSupported(
     }
     return true;
   }
+#else
+  if (!is_flash_attention || is_causal_mask) {
+    return false;
+  }
+#endif
   // check if matched attention block is supported by cuDNN fused attention.
   TF_ASSIGN_OR_RETURN(
       bool is_fused_attention,
@@ -1852,6 +1875,8 @@ absl::StatusOr<bool> CudnnFusedMHARewriter::Run(
         comp->parent()->config().debug_options();
     const se::dnn::VersionInfo cudnn_version =
         GetDnnVersionInfo(stream_executor_, cudnn_version_);
+
+#if !TENSORFLOW_USE_SYCL
 #if !defined(GOOGLE_CUDA) || CUDA_VERSION < 12000
     // CUDA needs to be >= 12.0 for cuDNN to work with all supported hardware.
     // Some cuDNN versions work with CUDA 11, but it is impractical for us to
@@ -1864,6 +1889,7 @@ absl::StatusOr<bool> CudnnFusedMHARewriter::Run(
             stream_executor::dnn::VersionInfo(8, 8, 0))) {
       return false;
     }
+#endif  // !TENSORFLOW_USE_SYCL
     for (HloInstruction* instr : comp->MakeInstructionPostOrder()) {
       bool v_transposed = false;
       bool changed = false;
@@ -1949,6 +1975,7 @@ absl::StatusOr<bool> CudnnFusedMHARewriter::Run(
                               matched_result.need_canonicalization));
           continue;
         }
+#if !TENSORFLOW_USE_SYCL
         // if fwd uses mask input, then bwd needs cudnn 8.9.1 to take in a mask
         // input if cudnn version < 8.9.1 we won't lower the bwd pass
         if (matched_result.matched_mask != nullptr &&
@@ -1978,6 +2005,26 @@ absl::StatusOr<bool> CudnnFusedMHARewriter::Run(
                               matched_result.need_canonicalization));
           continue;
         }
+#else
+        if (matched_result.matched_mask != nullptr) {
+          // restore fwd graph if bwd pattern match mask
+          TF_RETURN_IF_ERROR(
+              RestoreFwdGraph(comp, fwd_fmha_call, original_bmm2, activation,
+                              original_bmm2_producer0, original_bmm2_producer1,
+                              original_activation_producers,
+                              matched_result.need_canonicalization));
+          continue;
+        }
+        if (matched_bwd_result.matched_dbias != nullptr) {
+          // restore fwd graph if bwd pattern match dbias
+          TF_RETURN_IF_ERROR(
+              RestoreFwdGraph(comp, fwd_fmha_call, original_bmm2, activation,
+                              original_bmm2_producer0, original_bmm2_producer1,
+                              original_activation_producers,
+                              matched_result.need_canonicalization));
+          continue;
+        }
+#endif
         // Canonicalize gemms
         if (matched_bwd_result.bmm_1_grad_1_need_canonicalization) {
           TF_ASSIGN_OR_RETURN(
diff --git a/xla/service/gpu/cusolver_context.cc b/xla/service/gpu/cusolver_context.cc
index 8f9642ba4e..37ddfb9e3c 100644
--- a/xla/service/gpu/cusolver_context.cc
+++ b/xla/service/gpu/cusolver_context.cc
@@ -40,6 +40,7 @@ limitations under the License.
 namespace xla {
 namespace gpu {
 
+#if !TENSORFLOW_USE_SYCL
 namespace {
 
 // Type traits to get CUDA complex types from std::complex<T>.
@@ -549,5 +550,19 @@ absl::Status GpuSolverContext::Potrf(
 }
 #endif  // TENSORFLOW_USE_HIPSOLVER
 
+#else // !TENSORFLOW_USE_SYCL
+
+StatusOr<GpuSolverContext> GpuSolverContext::Create() {
+  return GpuSolverContext();
+}
+
+Status GpuSolverContext::SetStream(se::Stream* stream) {
+  gpu_stream_ = stream_executor::gpu::AsGpuStreamValue(stream);
+  return OkStatus();
+}
+
+GpuSolverContext::GpuSolverContext() {}
+
+#endif // !TENSORFLOW_USE_SYCL
 }  // namespace gpu
 }  // namespace xla
diff --git a/xla/service/gpu/cusolver_context.h b/xla/service/gpu/cusolver_context.h
index 74f287254f..c531f0013a 100644
--- a/xla/service/gpu/cusolver_context.h
+++ b/xla/service/gpu/cusolver_context.h
@@ -31,6 +31,12 @@ limitations under the License.
 #define TENSORFLOW_USE_CUSOLVER_OR_HIPSOLVER \
   (!TENSORFLOW_USE_ROCM || TENSORFLOW_USE_HIPSOLVER)
 
+#if TENSORFLOW_USE_SYCL
+#include "oneapi/mkl/blas.hpp"
+#include "oneapi/mkl/lapack.hpp"
+#include "oneapi/mkl/dfti.hpp"
+#include "oneapi/mkl/exceptions.hpp"
+#else // TENSORFLOW_USE_SYCL
 #if !TENSORFLOW_USE_ROCM
 #include "third_party/gpus/cuda/include/cusolverDn.h"
 using gpusolverHandle_t = cusolverDnHandle_t;
@@ -46,15 +52,17 @@ using gpusolverHandle_t = hipsolverHandle_t;
 using gpusolverHandle_t = rocblas_handle;
 #endif  // TF_ROCM_VERSION >= 40500
 #endif  // TENSORFLOW_USE_ROCM
+#endif  // TENSORFLOW_USE_SYCL
 
 #include "xla/stream_executor/blas.h"
 #include "xla/stream_executor/stream_executor.h"
 #include "xla/xla_data.pb.h"
+#include "xla/stream_executor/gpu/gpu_types.h"
 
 namespace xla {
 namespace gpu {
-
 namespace se = ::stream_executor;
+#if !TENSORFLOW_USE_SYCL
 
 class GpuSolverContext {
  public:
@@ -125,6 +133,90 @@ class GpuSolverContext {
   std::unique_ptr<std::remove_pointer_t<gpusolverHandle_t>, Deleter> handle_;
 };
 
+#else // !TENSORFLOW_USE_SYCL
+class GpuSolverContext {
+ public:
+  static absl::StatusOr<GpuSolverContext> Create();
+  absl::Status SetStream(se::Stream* stream);
+
+  template <typename T>
+  absl::Status PotrfBatched(se::blas::UpperLower uplo, int n, se::DeviceMemory<T*> as,
+                      int lda, se::DeviceMemory<int> lapack_info,
+                      int batch_size, T* a_base) {
+    T* scratch_data = static_cast<T*>(as.opaque());
+    int64_t scratchpad_size = as.size() / sizeof(T);
+
+    const int64_t stride_a = n * n;
+
+    oneapi::mkl::uplo params_uplo;
+    switch (uplo) {
+      case se::blas::UpperLower::kLower:
+        params_uplo = oneapi::mkl::uplo::L;
+        break;
+      case se::blas::UpperLower::kUpper:
+        params_uplo = oneapi::mkl::uplo::U;
+        break;
+      default:
+        params_uplo = static_cast<oneapi::mkl::uplo>(uplo);
+    }
+
+    try {
+      oneapi::mkl::lapack::potrf_batch(*gpu_stream_, params_uplo, n, a_base,
+                                       lda, stride_a, batch_size, scratch_data,
+                                       scratchpad_size);
+    } catch (oneapi::mkl::lapack::batch_error const& be) {
+      int i = 0;
+      auto& ids = be.ids();
+      for (auto const& e : be.exceptions()) {
+        try {
+          std::rethrow_exception(e);
+        } catch (oneapi::mkl::lapack::exception& e) {
+          LOG(ERROR) << "Exception " << ids[i++]
+                     << " in a batch says: " << e.what()
+                     << " (info code: " << e.info() << ")";
+        }
+      }
+    }
+    return absl::OkStatus();
+  }
+
+  template <typename T>
+  absl::Status Potrf(se::blas::UpperLower uplo, int n,
+                     se::DeviceMemory<T> a, int lda,
+                     se::DeviceMemory<int> lapack_info,
+                     se::DeviceMemory<T> workspace){
+    T* a_data = static_cast<T*>(a.opaque());
+    T* scratch_data = static_cast<T*>(workspace.opaque());
+    int64_t scratchpad_size = workspace.size() / sizeof(T);
+
+    oneapi::mkl::uplo params_uplo;
+    switch (uplo) {
+      case se::blas::UpperLower::kLower:
+        params_uplo = oneapi::mkl::uplo::L;
+        break;
+      case se::blas::UpperLower::kUpper:
+        params_uplo = oneapi::mkl::uplo::U;
+        break;
+      default:
+        params_uplo = static_cast<oneapi::mkl::uplo>(uplo);
+    }
+
+    try {
+      oneapi::mkl::lapack::potrf(*gpu_stream_, params_uplo, n, a_data,
+                                 lda, scratch_data, scratchpad_size);
+    } catch (oneapi::mkl::lapack::computation_error const& ce) {
+        LOG(ERROR) << "Exception " << ce.what()
+                   << " (info code: " << ce.info() << ")";
+    }
+    return absl::OkStatus();
+  }
+
+ private:
+  explicit GpuSolverContext();
+  stream_executor::gpu::GpuStreamHandle gpu_stream_;
+};
+
+#endif // !TENSORFLOW_USE_SYCL
 }  // namespace gpu
 }  // namespace xla
 
diff --git a/xla/service/gpu/cusolver_rewriter.cc b/xla/service/gpu/cusolver_rewriter.cc
index ddfda66382..f5c317f5f1 100644
--- a/xla/service/gpu/cusolver_rewriter.cc
+++ b/xla/service/gpu/cusolver_rewriter.cc
@@ -71,6 +71,7 @@ absl::StatusOr<HloInstruction*> CreateCholesky(GpuSolverContext* context,
   absl::c_iota(batch_dim_ids, 0);
   int64_t batch_size = absl::c_accumulate(batch_dims, 1, std::multiplies<>{});
 
+#if !TENSORFLOW_USE_SYCL
   // Find the workspace size.
   se::blas::UpperLower uplo = options.lower() ? se::blas::UpperLower::kLower
                                               : se::blas::UpperLower::kUpper;
@@ -78,7 +79,36 @@ absl::StatusOr<HloInstruction*> CreateCholesky(GpuSolverContext* context,
   TF_ASSIGN_OR_RETURN(
       workspace_size,
       context->PotrfBufferSize(a_shape.element_type(), uplo, n, n, batch_size));
-
+#else
+  // Find the workspace size.
+  int64_t workspace_size = 0;
+  oneapi::mkl::uplo uplo =
+      options.lower() ? oneapi::mkl::uplo::L : oneapi::mkl::uplo::U;
+  sycl::property_list propList{sycl::property::queue::in_order()};
+  sycl::queue queue(sycl::gpu_selector{}, propList);
+  switch (a_shape.element_type()) {
+    case F32:
+      workspace_size = oneapi::mkl::lapack::potrf_batch_scratchpad_size<float>(
+          queue, uplo, n, n, n * n, batch_size);
+      break;
+    case F64:
+      workspace_size = oneapi::mkl::lapack::potrf_batch_scratchpad_size<double>(
+          queue, uplo, n, n, n * n, batch_size);
+      break;
+    case C64:
+      workspace_size =
+          oneapi::mkl::lapack::potrf_batch_scratchpad_size<std::complex<float>>(
+              queue, uplo, n, n, n * n, batch_size);
+      break;
+    case C128:
+      workspace_size = oneapi::mkl::lapack::potrf_batch_scratchpad_size<
+          std::complex<double>>(queue, uplo, n, n, n * n, batch_size);
+      break;
+    default:
+      return InvalidArgument("Invalid type for cholesky %s",
+                             PrimitiveType_Name(a_shape.element_type()));
+  }
+#endif
   // TODO(phawkins): Ideally we would relax this constraint. What we actually
   // want is that:
   // a) the batch dimensions are major, in no particular order.
@@ -105,6 +135,9 @@ absl::StatusOr<HloInstruction*> CreateCholesky(GpuSolverContext* context,
   TF_RETURN_IF_ERROR(custom_call->set_backend_config(options));
   HloInstruction* out = computation->AddInstruction(
       HloInstruction::CreateGetTupleElement(a_shape, custom_call, 0));
+#if TENSORFLOW_USE_SYCL
+  return out;
+#else
   HloInstruction* info = computation->AddInstruction(
       HloInstruction::CreateGetTupleElement(info_shape, custom_call, 2));
 
@@ -134,6 +167,7 @@ absl::StatusOr<HloInstruction*> CreateCholesky(GpuSolverContext* context,
       computation->AddInstruction(HloInstruction::CreateTernary(
           a_shape, HloOpcode::kSelect, ok, out, nans));
   return select;
+#endif
 }
 
 // Tries to rewrite a single convolution into a call to cudnn.
diff --git a/xla/service/gpu/elemental_ir_emitter.cc b/xla/service/gpu/elemental_ir_emitter.cc
index 6d6675f21e..c4a5287d3c 100644
--- a/xla/service/gpu/elemental_ir_emitter.cc
+++ b/xla/service/gpu/elemental_ir_emitter.cc
@@ -332,6 +332,11 @@ absl::StatusOr<llvm::Value*> GpuElementalIrEmitter::EmitCbrt(
                             prim_type);
 }
 
+absl::StatusOr<llvm::Value*> GpuElementalIrEmitter::EmitF32ToBF16(
+    llvm::Value* f32_value) {
+  return ElementalIrEmitter::EmitF32ToBF16(f32_value);
+}
+
 absl::StatusOr<std::vector<llvm::Value*>>
 GpuElementalIrEmitter::EmitThreadLocalCall(
     const HloComputation& callee, absl::Span<llvm::Value* const> parameters,
diff --git a/xla/service/gpu/elemental_ir_emitter.h b/xla/service/gpu/elemental_ir_emitter.h
index 83f82818f2..a66142c063 100644
--- a/xla/service/gpu/elemental_ir_emitter.h
+++ b/xla/service/gpu/elemental_ir_emitter.h
@@ -100,6 +100,8 @@ class GpuElementalIrEmitter : public ElementalIrEmitter {
       const HloComputation& callee, absl::Span<llvm::Value* const> parameters,
       absl::string_view, bool /*is_reducer*/) override;
 
+  absl::StatusOr<llvm::Value*> EmitF32ToBF16(llvm::Value* f32_value) override;
+
   bool fast_min_max() override {
     return ir_emitter_context_.debug_options().xla_gpu_enable_fast_min_max();
   }
diff --git a/xla/service/gpu/fusions/fusion_emitter.cc b/xla/service/gpu/fusions/fusion_emitter.cc
index e652532fd0..c8bb8f9fd9 100644
--- a/xla/service/gpu/fusions/fusion_emitter.cc
+++ b/xla/service/gpu/fusions/fusion_emitter.cc
@@ -65,6 +65,10 @@ limitations under the License.
 #include "tsl/platform/errors.h"
 #include "tsl/platform/statusor.h"
 
+#if TENSORFLOW_USE_SYCL
+#include "xla/service/gpu/ir_emission_utils.h"
+#endif
+
 namespace xla {
 namespace gpu {
 namespace {
@@ -234,6 +238,7 @@ BuildKernelPrototype(IrEmitterContext& ir_emitter_context,
   auto* llvm_module = ir_emitter_context.llvm_module();
   llvm::LLVMContext& context = llvm_module->getContext();
   // Explicitly set global addrspace for SPIR backend.
+  // TODD SYCL may wrong
   int addrspace = llvm::Triple(llvm_module->getTargetTriple()).isSPIR() ? 1 : 0;
   llvm::FunctionType* kernel_type = llvm::FunctionType::get(
       /*Result=*/llvm::Type::getVoidTy(context),
@@ -248,6 +253,17 @@ BuildKernelPrototype(IrEmitterContext& ir_emitter_context,
       ir_emitter_context.gpu_device_info(), launch_dimensions, kernel_name,
       llvm_module));
 
+  // SYCL: Set function metadata
+  if (IsSPIR(llvm_module)) {
+    llvm::LLVMContext& context = llvm_module->getContext();
+    llvm::IntegerType* i32 = llvm::Type::getInt32Ty(context);
+    kernel->setMetadata(
+        "intel_reqd_sub_group_size",
+        llvm::MDNode::get(context,
+                          {llvm::ConstantAsMetadata::get(
+                              llvm::ConstantInt::get(i32, WarpSize()))}));
+  }
+
   // TODO(b/65380986): Investigate if adding fast math flags for generated
   // kernels makes sense.
 
diff --git a/xla/service/gpu/fusions/reduction.cc b/xla/service/gpu/fusions/reduction.cc
index 193fc36d20..5003350ea0 100644
--- a/xla/service/gpu/fusions/reduction.cc
+++ b/xla/service/gpu/fusions/reduction.cc
@@ -288,20 +288,12 @@ ReductionGroupEmitter::ReductionGroupEmitter(
            << reduction_emitter_.fusion_.ToString();
 
   auto* builder = reduction_emitter_.builder_;
+  const Tiling& tiling = reduction_info.GetTiling();
   for (const HloReduceInstruction* reduce_hlo : reduce_instr_index_group) {
     for (int op_result_idx = 0;
          op_result_idx < GetNumOutputs(reduce_hlo->shape()); op_result_idx++) {
       Shape result_shape = OutputShape(reduce_hlo->shape(), op_result_idx);
 
-      llvm::Type* element_type = llvm_ir::PrimitiveTypeToIrType(
-          result_shape.element_type(), builder->GetInsertBlock()->getModule());
-      llvm::AllocaInst* reduction_input_address =
-          llvm_ir::EmitAllocaAtFunctionEntry(
-              element_type, "reduction_input_address", builder);
-
-      llvm::AllocaInst* result_address = llvm_ir::EmitAllocaAtFunctionEntry(
-          element_type, "partial_reduction_result", builder);
-
       const HloInstruction* init_value =
           reduce_hlo->init_values()[op_result_idx];
 
@@ -310,7 +302,35 @@ ReductionGroupEmitter::ReductionGroupEmitter(
           *init_value))(llvm_ir::IrArray::Index(builder->getInt32Ty()))
                                        .value();
 
-      builder->CreateStore(init_ir_value, result_address);
+      llvm::Type* element_type = llvm_ir::PrimitiveTypeToIrType(
+          result_shape.element_type(), builder->GetInsertBlock()->getModule());
+
+      llvm::AllocaInst *reduction_input_address, *result_address;
+      if (reduction_info.IsRowReduction()) {
+        reduction_input_address = llvm_ir::EmitAllocaAtFunctionEntry(
+            element_type, "reduction_input_address", builder);
+        result_address = llvm_ir::EmitAllocaAtFunctionEntry(
+            element_type, "partial_reduction_result", builder);
+        builder->CreateStore(init_ir_value, result_address);
+      } else {
+        auto vectorize_size =
+            tiling
+                .GetThreadTileSize()[ReductionDimensions::kVectorizedDimension];
+        reduction_input_address = llvm_ir::EmitAllocaAtFunctionEntryWithCount(
+            element_type,
+            llvm::ConstantInt::get(builder->getInt32Ty(), vectorize_size),
+            "reduction_input_address", builder);
+        result_address = llvm_ir::EmitAllocaAtFunctionEntryWithCount(
+            element_type,
+            llvm::ConstantInt::get(builder->getInt32Ty(), vectorize_size),
+            "partial_reduction_result", builder);
+        for (int id = 0; id < vectorize_size; id++) {
+          auto slot = builder->CreateInBoundsGEP(element_type, result_address,
+                                                 {builder->getInt32(id)});
+          builder->CreateStore(init_ir_value, slot);
+        }
+      }
+
       const Tiling& tiling = reduction_info.GetTiling();
       auto shared_cache = [&]() -> std::optional<llvm_ir::SharedMemoryTile> {
         auto* module = reduction_emitter.ir_emitter_context_.llvm_module();
@@ -604,8 +624,9 @@ llvm_ir::IrArray::Index ReductionGroupEmitter::GetOutputIndexForReduction(
         offset[ReductionDimensions::kColMinorKeptDimension],
         thread_ids[ReductionDimensions::kColReducedDimension]);
     return {{major_idx, minor_idx},
-            ShapeUtil::DeleteDimension(
-                ReductionDimensions::kColReducedDimension, shape),
+            {shape.dimensions(ReductionDimensions::kColMajorKeptDimension),
+             shape.dimensions(ReductionDimensions::kColMinorKeptDimension) *
+                 shape.dimensions(ReductionDimensions::kVectorizedDimension)},
             index_ty};
   }();
 
@@ -792,8 +813,11 @@ void ReductionGroupEmitter::EmitReductionOutputForColumnReduction(
   auto* builder = reduction_emitter_.builder_;
   KernelSupportLibrary ksl(builder);
   const HloComputation* reducer = reduction->to_apply();
-  const auto& thread_id_info = tiling_kernel_info.thread_id_info;
-  const auto& thread_ids = thread_id_info.thread_ids;
+  TilingKernelInfo reduction_tiling_info = tiling_kernel_info;
+  auto& tile_origin = reduction_tiling_info.tile_origin;
+  auto& output_tile_bounds = reduction_tiling_info.output_tile_bounds;
+  auto& thread_id_info = reduction_tiling_info.thread_id_info;
+  auto& thread_ids = thread_id_info.thread_ids;
 
   auto constant = [&](uint64_t c) -> llvm::Constant* {
     return llvm::ConstantInt::get(reduction_emitter_.index_ty_, c);
@@ -803,55 +827,80 @@ void ReductionGroupEmitter::EmitReductionOutputForColumnReduction(
   };
   const auto& reduction_info = reduction_emitter_.reduction_codegen_info_;
   const Tiling& tiling = reduction_info.GetTiling();
+  const auto& tile_size = tiling.GetThreadTileSize();
   int num_outputs = reducer->num_parameters() / 2;
 
   auto* kept_index = thread_ids[ReductionDimensions::kColMinorKeptDimension];
   auto* reduced_index = thread_ids[ReductionDimensions::kColReducedDimension];
 
-  // Store the transpose in shared memory.
-  for (int output_idx = 0; output_idx < num_outputs; output_idx++) {
-    const auto& state = GetCalculationStateFor(reduction, output_idx);
-    auto* current_output_value =
-        builder->CreateLoad(state.partial_result_address->getAllocatedType(),
-                            state.partial_result_address);
-    state.shared_cache->Store(current_output_value, {kept_index, reduced_index},
-                              builder);
-  }
-
-  reduction_emitter_.EmitSyncThreads();
-
-  // Get transposed element from shared memory.
-  absl::InlinedVector<TypedPointer, 2> shmem_transposed_addrs;
-  for (int output_idx = 0; output_idx < num_outputs; output_idx++) {
-    const auto& state = GetCalculationStateFor(reduction, output_idx);
-    auto* shmem_transposed_addr =
-        state.shared_cache->Address({reduced_index, kept_index}, builder);
-    shmem_transposed_addrs.push_back(
-        {shmem_transposed_addr, state.shared_cache->GetElementType()});
-  }
-
-  EmitFullWarpShuffleDownLoopForReduce(reducer,
-                                       absl::MakeSpan(shmem_transposed_addrs),
-                                       tiling.GetNumThreadsPerBlock(),
-                                       /*num_results_per_warp=*/1);
-
   // Some warps in the block are completely outside of the bound of the
   // tensor, so they should not write any output at all.
   llvm::Value* has_output = builder->CreateAnd(
       builder->CreateICmpULT(
           reduced_index,
-          tiling_kernel_info
-              .output_tile_bounds[ReductionDimensions::kColMinorKeptDimension]),
+          output_tile_bounds[ReductionDimensions::kColMinorKeptDimension]),
       builder->CreateICmpULT(
           kept_index,
-          tiling_kernel_info
-              .output_tile_bounds[ReductionDimensions::kColReducedDimension]));
-
-  ksl.If("reduction_write_output",
-         builder->CreateAnd(has_output, is_zero(thread_id_info.lane_id)), [&] {
-           WriteReductionOutput(tiling_kernel_info, reduction, roots,
-                                shmem_transposed_addrs);
-         });
+          output_tile_bounds[ReductionDimensions::kColReducedDimension]));
+
+  constexpr int kVectorizedDimension =
+      ReductionDimensions::kVectorizedDimension;
+  if (tile_size[kVectorizedDimension] > 1) {
+    std::vector<llvm::Value*> tile_index = tile_origin.multidim();
+    tile_index[ReductionDimensions::kColMinorKeptDimension] =
+        builder->CreateMul(
+            tile_index[ReductionDimensions::kColMinorKeptDimension],
+            output_tile_bounds[kVectorizedDimension]);
+    tile_origin = llvm_ir::IrArray::Index(tile_index, tiling.GetShape(),
+                                          reduction_emitter_.index_ty_);
+  }
+
+  for (int vec_dim = 0; vec_dim < tile_size[kVectorizedDimension]; vec_dim++) {
+    // Store the transpose in shared memory.
+    for (int output_idx = 0; output_idx < num_outputs; output_idx++) {
+      const auto& state = GetCalculationStateFor(reduction, output_idx);
+      auto* partial_result_address = builder->CreateInBoundsGEP(
+          state.partial_result_address->getAllocatedType(),
+          state.partial_result_address, {builder->getInt32(vec_dim)});
+      auto* current_output_value =
+          builder->CreateLoad(state.partial_result_address->getAllocatedType(),
+                              partial_result_address);
+      state.shared_cache->Store(current_output_value,
+                                {kept_index, reduced_index}, builder);
+    }
+
+    reduction_emitter_.EmitSyncThreads();
+
+    // Get transposed element from shared memory.
+    absl::InlinedVector<TypedPointer, 2> shmem_transposed_addrs;
+    for (int output_idx = 0; output_idx < num_outputs; output_idx++) {
+      const auto& state = GetCalculationStateFor(reduction, output_idx);
+      auto* shmem_transposed_addr =
+          state.shared_cache->Address({reduced_index, kept_index}, builder);
+      shmem_transposed_addrs.push_back(
+          {shmem_transposed_addr, state.shared_cache->GetElementType()});
+    }
+    EmitFullWarpShuffleDownLoopForReduce(reducer,
+                                         absl::MakeSpan(shmem_transposed_addrs),
+                                         tiling.GetNumThreadsPerBlock(),
+                                         /*num_results_per_warp=*/1);
+
+    thread_ids[ReductionDimensions::kColReducedDimension] = builder->CreateAdd(
+        builder->CreateMul(reduced_index,
+                           output_tile_bounds[kVectorizedDimension]),
+        llvm::ConstantInt::get(reduction_emitter_.index_ty_, vec_dim));
+
+    ksl.If("reduction_write_output",
+           builder->CreateAnd(has_output, is_zero(thread_id_info.lane_id)),
+           [&] {
+             WriteReductionOutput(reduction_tiling_info, reduction, roots,
+                                  shmem_transposed_addrs);
+           });
+
+    if (tile_size[kVectorizedDimension] > 1) {
+      reduction_emitter_.EmitSyncThreads();
+    }
+  }
 }
 
 // Generate a single element of the tile (update the accumulator state) for a
@@ -861,6 +910,8 @@ void ReductionGroupEmitter::GenerateElementForReducer(
     const llvm_ir::IrArray::Index& index) const {
   HloComputation* reducer = reduction->to_apply();
   auto* builder = reduction_emitter_.builder_;
+  const ReductionInfo& reduction_info =
+      reduction_emitter_.reduction_codegen_info_;
   CHECK_EQ(reducer->num_parameters() % 2, 0);
 
   absl::InlinedVector<llvm::Value*, 2> reduction_accumulators;
@@ -868,12 +919,23 @@ void ReductionGroupEmitter::GenerateElementForReducer(
   for (int red_idx = 0; red_idx < reducer->num_parameters() / 2; red_idx++) {
     const auto& state = GetCalculationStateFor(reduction, red_idx);
 
-    llvm::AllocaInst* input_address = state.input_address;
+    llvm::Value* input_address = state.input_address;
+    llvm::Value* partial_result_address = state.partial_result_address;
+    if (!reduction_info.IsRowReduction()) {
+      constexpr int kVectorizedDimension =
+          ReductionDimensions::kVectorizedDimension;
+      input_address = builder->CreateInBoundsGEP(
+          state.input_address->getAllocatedType(), input_address,
+          {index[kVectorizedDimension]});
+      partial_result_address = builder->CreateInBoundsGEP(
+          state.partial_result_address->getAllocatedType(),
+          partial_result_address, {index[kVectorizedDimension]});
+    }
     auto input_index =
         index.SourceIndexOfBitcast(reduction->operand(0)->shape(), builder);
     llvm::Value* const input_ir_value = *state.input_gen(input_index);
     builder->CreateStore(input_ir_value, input_address);
-    reduction_accumulators.push_back(state.partial_result_address);
+    reduction_accumulators.push_back(partial_result_address);
     reduction_input_value.push_back(input_address);
   }
 
diff --git a/xla/service/gpu/fusions/reduction_base.cc b/xla/service/gpu/fusions/reduction_base.cc
index 86bb721129..a69e580359 100644
--- a/xla/service/gpu/fusions/reduction_base.cc
+++ b/xla/service/gpu/fusions/reduction_base.cc
@@ -66,39 +66,99 @@ int RowReductionGetRowsPerWarp(int reduced_dimension_size) {
   return WarpSize() / reduced_dimension_size;
 }
 
+int64_t ComputeColReductionActiveCore(Vector3 reduction_dimensions,
+                                      int64_t tile_y, int64_t num_threads_y,
+                                      int64_t num_threads_x, int vector_size) {
+  constexpr int kColMajorKept = ReductionDimensions::kColMajorKeptDimension;
+  constexpr int kColReduced = ReductionDimensions::kColReducedDimension;
+  constexpr int kColMinorKept = ReductionDimensions::kColMinorKeptDimension;
+  // The number of blocks is strongly related to the number of active cores.
+  // Make block counts larger than sm core counts can reach better memory
+  // bandwidth. We don't consider how many threads are scheduled in an sm core
+  // due to it is not as important as active core ratio for memory-bound kernels
+  // and hope to relax the vectorization restrictions of column reduction.
+  int64_t blocks_x = CeilOfRatio(reduction_dimensions[kColMinorKept],
+                                 num_threads_x * vector_size);
+  int64_t block_tile_y = num_threads_y * tile_y;
+  int64_t blocks_y =
+      CeilOfRatio(reduction_dimensions[kColReduced], block_tile_y);
+  int64_t blocks = reduction_dimensions[kColMajorKept] * blocks_x * blocks_y;
+  return blocks;
+}
+
 int GetVectorSize(const HloFusionAnalysis& analysis,
                   const ReductionDimensions& reduction_dimensions,
-                  int num_threads, Vector3 reduction_tiling) {
-  if (!reduction_dimensions.is_row_reduction) {
+                  int num_threads) {
+  // If the minor dimension is not divisible by 2, we can't currently vectorize.
+  int64_t minor_dim = reduction_dimensions.dimensions.back();
+  if (minor_dim % 2 != 0) {
     return 1;
   }
-
-  constexpr int kRowMinorReduced =
-      ReductionDimensions::kRowMinorReducedDimension;
-  if (reduction_dimensions.dimensions[kRowMinorReduced] % 2 != 0 ||
-      MayPreventVectorization(analysis.fusion())) {
+  // Only enable vectorization if all threads will still have work.
+  if (num_threads * 2 > minor_dim) {
     return 1;
   }
 
-  // Enabling vectorization if number of threads is <= warpsize leads to half or
-  // more of the threads not doing any work.
-  if (num_threads <= WarpSize()) {
+  // 16 byte vector loads are often slower than 8 byte loads.
+  if (analysis.input_output_info().smallest_input_dtype_bits >= 32) {
+    return 2;
+  }
+  if (analysis.input_output_info().smallest_input_dtype_bits >= 64) {
     return 1;
   }
+  // Like above, if the size of the minor dimension is not sufficiently large,
+  // the vectorization is not helpful.
+  if (num_threads * 4 > minor_dim) {
+    return 2;
+  }
+  return minor_dim % 4 == 0 ? 4 : 2;
+}
 
-  const auto* cuda_cc = std::get_if<se::CudaComputeCapability>(
-      &analysis.device_info().gpu_compute_capability());
-  if (cuda_cc == nullptr) return 1;
-  if (cuda_cc->IsAtLeast(se::CudaComputeCapability::VOLTA)) return 2;
-  if (cuda_cc->IsAtLeast(se::CudaComputeCapability::PASCAL_)) {
-    return analysis.input_output_info().smallest_input_dtype_bits <= 32 &&
-                   reduction_dimensions.dimensions[kRowMinorReduced] %
-                           (reduction_tiling[kRowMinorReduced] * num_threads) ==
-                       0
-               ? 2
-               : 1;
+std::tuple<Vector3, int, bool> AdjustColReductionTilingConfig(
+    const HloFusionAnalysis& analysis, Vector3 reduction_dimensions,
+    Vector3 reduction_tiling, int64_t num_threads_y, int64_t num_threads_x,
+    int vector_size) {
+  constexpr int kColReduced = ReductionDimensions::kColReducedDimension;
+  auto core_count = analysis.device_info().core_count();
+  constexpr int minimum_tile_size = 8;
+
+  auto actual_tile_size =
+      CeilOfRatio(reduction_dimensions[kColReduced], num_threads_y);
+  reduction_tiling[kColReduced] = actual_tile_size;
+  // Early return if all of the sm cores are active.
+  if (ComputeColReductionActiveCore(
+          reduction_dimensions, reduction_tiling[kColReduced], num_threads_y,
+          num_threads_x, vector_size) >= core_count) {
+    return {reduction_tiling, vector_size, false};
   }
-  return 1;
+
+  auto roots = analysis.fusion().GetRoots();
+  for (auto [root, hero] : llvm::zip(roots, analysis.fusion_heroes())) {
+    // Only adjust tile_y if hero is reduction and output element type is F32.
+    // F32 atomic is fast so that we can ignore the extra atomic overhead
+    // by adjusting tile_y to increase the parallelism of the kernel.
+    if (hero->opcode() == HloOpcode::kReduce) {
+      if (hero != (&root.instruction()) ||
+          hero->shape().element_type() != F32) {
+        // If we can not adjust tile_y but sm core active ratio is low, reset
+        // vector size as 1.
+        return {reduction_tiling, 1, false};
+      }
+    }
+  }
+
+  auto current_tile_size = actual_tile_size;
+  while (current_tile_size >= minimum_tile_size * 2) {
+    if (ComputeColReductionActiveCore(reduction_dimensions, current_tile_size,
+                                      num_threads_y, num_threads_x,
+                                      vector_size) > core_count)
+      break;
+    current_tile_size = current_tile_size / 2;
+  }
+  bool tile_size_decreased = current_tile_size != actual_tile_size;
+  reduction_tiling[kColReduced] = current_tile_size;
+  return {reduction_tiling, tile_size_decreased ? vector_size : 1,
+          tile_size_decreased};
 }
 
 ReductionGroups GroupDisjointReductions(const HloFusionAnalysis& analysis) {
@@ -266,7 +326,11 @@ ReductionInfo ReductionInfo::Create(const HloFusionAnalysis& analysis) {
   // parallelizing the z dimension (major reduced dimensions). The general
   // recommendation is to use between 128 and 512 threads, so we just go for
   // 256. See https://forums.developer.nvidia.com/t/55529
+#ifdef TENSORFLOW_USE_SYCL
+  constexpr int64_t kThreadsPerBlockTarget = 32;
+#else
   constexpr int64_t kThreadsPerBlockTarget = 256;
+#endif
   if (reduction_dimensions.is_row_reduction &&
       num_threads_x * 2 <= kThreadsPerBlockTarget) {
     int64_t kept_size =
@@ -284,15 +348,23 @@ ReductionInfo ReductionInfo::Create(const HloFusionAnalysis& analysis) {
     }
   }
 
-  int vector_size = GetVectorSize(analysis, reduction_dimensions, num_threads_x,
-                                  reduction_tiling);
+  int vector_size = GetVectorSize(analysis, reduction_dimensions, num_threads_x);
+  bool tile_size_decreased = false;
+  if (!reduction_dimensions.is_row_reduction) {
+    // Adjust tile_y and vector size for column reduction.
+    std::tie(reduction_tiling, vector_size, tile_size_decreased) =
+        AdjustColReductionTilingConfig(analysis, shape, reduction_tiling,
+                                       num_threads_y, num_threads_x,
+                                       vector_size);
+  }
 
   absl::InlinedVector<int64_t, 4> num_threads{1, num_threads_y, num_threads_x};
   absl::InlinedVector<int64_t, 4> tiled_shape{shape[0], shape[1],
                                               shape[2] / vector_size};
   absl::InlinedVector<int64_t, 4> tile_per_thread{
       reduction_tiling[0], reduction_tiling[1],
-      reduction_tiling[2] / vector_size};
+      reduction_dimensions.is_row_reduction ? reduction_tiling[2] / vector_size
+                                            : reduction_tiling[2]};
   if (rows_per_warp > 1) {
     // If we produce more than one element per thread, that means the reduced
     // dimension is small and it can't be tiled - we already have more threads
@@ -301,7 +373,7 @@ ReductionInfo ReductionInfo::Create(const HloFusionAnalysis& analysis) {
     // uses the thread ID as the coordinate.
     tile_per_thread[2] = 1;
   }
-  if (vector_size != 1) {
+  if (!reduction_dimensions.is_row_reduction || vector_size != 1) {
     num_threads.push_back(1);  // The vector dimension is a loop.
     tiled_shape.push_back(vector_size);
     tile_per_thread.push_back(vector_size);
@@ -311,6 +383,8 @@ ReductionInfo ReductionInfo::Create(const HloFusionAnalysis& analysis) {
                 /*loops_to_unroll=*/{false, false, true, false});
   bool reduction_is_race_free = ReductionIsRaceFree(
       hero_reduction->GetModule()->config(), reduction_dimensions);
+  // If tile_y is decreased, reduction is not race free.
+  reduction_is_race_free = reduction_is_race_free && !tile_size_decreased;
   return ReductionInfo(analysis, tiling, reduction_dimensions.is_row_reduction,
                        reduction_is_race_free,
                        GroupDisjointReductions(analysis), hero_reduction);
diff --git a/xla/service/gpu/fusions/transpose.cc b/xla/service/gpu/fusions/transpose.cc
index ca7b3f7ff7..59fe840277 100644
--- a/xla/service/gpu/fusions/transpose.cc
+++ b/xla/service/gpu/fusions/transpose.cc
@@ -47,6 +47,7 @@ limitations under the License.
 #include "xla/service/gpu/target_util.h"
 #include "xla/service/llvm_ir/fused_ir_emitter.h"
 #include "xla/service/llvm_ir/ir_array.h"
+#include "xla/service/llvm_ir/kernel_support_library.h"
 #include "xla/service/llvm_ir/llvm_util.h"
 #include "xla/status.h"
 #include "xla/util.h"
@@ -72,7 +73,7 @@ Tiling ComputeTransposeTiling(const TransposeDescription& tiled_transpose) {
                                              transposed_dims[permutation[2]]};
 
   // We tile along the minor dimensions pre- and post-transpose.
-  absl::InlinedVector<int64_t, 4> tile_sizes{1, 1, 1};
+  absl::InlinedVector<int64_t, 4> tile_sizes{1, 1, 2};
   tile_sizes[permutation[2]] = WarpSize() / kNumRows;
   absl::InlinedVector<int64_t, 4> num_threads{1, 1, WarpSize()};
   num_threads[permutation[2]] = kNumRows;
@@ -177,61 +178,145 @@ absl::Status TransposeFusion::EmitKernel(IrEmitterContext& ir_emitter_context,
         tile_size, absl::StrCat("tr_tile_", tile_idx));
   }
 
+  KernelSupportLibrary ksl(builder, llvm_ir::UnrollMode::kDefaultUnroll);
+
   auto tile_generator = [&](const TilingThreadIdInfo& thread_id_info,
                             const llvm_ir::IrArray::Index& tile_start_index,
                             absl::Span<llvm::Value* const> tile_dimensions) {
     // Copy input parameter values to shared memory buffers:
-    // tile[thread_id_y, thread_id_x] = input[index]
-    EmitTile(builder, tiling_, thread_id_info, tile_dimensions,
-             [&](absl::Span<llvm::Value* const> index_in_tile) {
-               auto index = tile_start_index.AddOffset(index_in_tile, builder);
-               for (const auto& tr : transposes) {
-                 auto input_gen =
-                     *fused_emitter.GetGenerator(*tr.instr->operand(0));
-                 auto input_index = index.SourceIndexOfBitcast(
-                     tr.instr->operand(0)->shape(), builder);
-                 llvm::Value* value = *input_gen(input_index);
-                 tiles[tr.instr].Store(value, index_in_tile, builder);
-               }
-
-               // Compute all extra output values before writing them. This
-               // avoids overwriting aliased input/output values before all
-               // reads occurred.
-               std::vector<std::tuple<llvm_ir::IrArray, llvm_ir::IrArray::Index,
-                                      llvm::Value*>>
-                   scheduled_writes;
-               for (const auto& [output_idx, root] : extra_outputs) {
-                 auto extra_output_index =
-                     index.SourceIndexOfBitcast(root->shape(), builder);
-                 auto output_gen = *fused_emitter.GetGenerator(*root);
-                 llvm::Value* output_value = *output_gen(extra_output_index);
-                 scheduled_writes.emplace_back(
-                     outputs[output_idx], extra_output_index, output_value);
-               }
-
-               for (const auto& [output, idx, value] : scheduled_writes) {
-                 output.EmitWriteArrayElement(idx, value, builder);
-               }
-             });
+    // tile[thread_id_y, thread_id_x * 2] = input[index]
+    // tile[thread_id_y, thread_id_x * 2 + 1] = input[index + 1]
+    llvm::Type* index_ty = thread_id_info.thread_id->getType();
+    auto constant = [&](int64_t val) {
+      return llvm::ConstantInt::get(index_ty, val);
+    };
+    absl::InlinedVector<llvm::Value*, 4> tile_dimensions_Div2{
+        tile_dimensions[0], tile_dimensions[1],
+        builder->CreateUDiv(builder->CreateAdd(tile_dimensions[2], constant(1)),
+                            constant(2))};
+    EmitTile(
+        builder, tiling_, thread_id_info, tile_dimensions_Div2,
+        [&](absl::Span<llvm::Value* const> index_in_tile) {
+          absl::InlinedVector<llvm::Value*, 4> index_in_tile_1{
+              index_in_tile[0], index_in_tile[1],
+              builder->CreateMul(index_in_tile[2], constant(2))};
+          absl::InlinedVector<llvm::Value*, 4> index_in_tile_2{
+              index_in_tile[0], index_in_tile[1],
+              builder->CreateAdd(index_in_tile_1[2], constant(1))};
+
+          auto index_1 = tile_start_index.AddOffset(index_in_tile_1, builder);
+          auto index_2 = tile_start_index.AddOffset(index_in_tile_2, builder);
+
+          auto boundary = tile_start_index.AddOffset(tile_dimensions, builder);
+          auto boundary_multidim = boundary.multidim();
+          // auto index_1_multidim = index_1.multidim();
+          // auto* is_read_1 =
+          //     builder->CreateICmpULT(index_1_multidim[2], boundary_multidim[2]);
+          auto index_2_multidim = index_2.multidim();
+          auto* is_read_2 =
+              builder->CreateICmpULT(index_2_multidim[2], boundary_multidim[2]);
+
+          for (const auto& tr : transposes) {
+            auto input_gen = *fused_emitter.GetGenerator(*tr.instr->operand(0));
+            auto input_index_1 = index_1.SourceIndexOfBitcast(
+                tr.instr->operand(0)->shape(), builder);
+            auto input_index_2 = index_2.SourceIndexOfBitcast(
+                tr.instr->operand(0)->shape(), builder);
+            // ksl.If("is_read_1", is_read_1, [&]() {
+            llvm::Value* value_1 = *input_gen(input_index_1);
+            tiles[tr.instr].Store(value_1, index_in_tile_1, builder);
+            // });
+            ksl.If("is_read_2", is_read_2, [&]() {
+              llvm::Value* value_2 = *input_gen(input_index_2);
+              tiles[tr.instr].Store(value_2, index_in_tile_2, builder);
+            });
+          }
 
+          // Compute all extra output values before writing them. This
+          // avoids overwriting aliased input/output values before all
+          // reads occurred.
+          // std::vector<std::tuple<llvm_ir::IrArray, llvm_ir::IrArray::Index,
+          //                        llvm::Value*>>
+          //     scheduled_writes;
+          // FIXME(Intel): It's not needed now because in-place optimization is
+          // not supported. May need to fix it in the future.
+          for (const auto& [output_idx, root] : extra_outputs) {
+            auto extra_output_index_1 =
+                index_1.SourceIndexOfBitcast(root->shape(), builder);
+            auto extra_output_index_2 =
+                index_2.SourceIndexOfBitcast(root->shape(), builder);
+            auto output_gen = *fused_emitter.GetGenerator(*root);
+            llvm::Value* output_value_1 = *output_gen(extra_output_index_1);
+            llvm::Value* output_value_2 = *output_gen(extra_output_index_2);
+            // ksl.If("is_read_1", is_read_1, [&]() {
+            outputs[output_idx].EmitWriteArrayElement(
+                extra_output_index_1, output_value_1, builder);
+            // });
+            ksl.If("is_read_2", is_read_2, [&]() {
+              outputs[output_idx].EmitWriteArrayElement(
+                  extra_output_index_2, output_value_2, builder);
+            });
+          }
+        });
     EmitSyncThreads(builder, ir_emitter_context);
 
     auto output_tile_index = PermuteIndex(tile_start_index, permutation);
     auto transposed_tile_dimensions = Permute(tile_dimensions, permutation);
+    absl::InlinedVector<llvm::Value*, 4> transposed_tile_dimensions_Div2{
+        transposed_tile_dimensions[0], transposed_tile_dimensions[1],
+        builder->CreateUDiv(
+            builder->CreateAdd(transposed_tile_dimensions[2], constant(1)),
+            constant(2))};
 
     EmitTile(
-        builder, tiling_, thread_id_info, transposed_tile_dimensions,
+        builder, tiling_, thread_id_info, transposed_tile_dimensions_Div2,
         /*emit_elem_function=*/
         [&](absl::Span<llvm::Value* const> index_in_tile) {
-          auto index = output_tile_index.AddOffset(index_in_tile, builder);
+          absl::InlinedVector<llvm::Value*, 4> index_in_tile_1{
+              index_in_tile[0], index_in_tile[1],
+              builder->CreateMul(index_in_tile[2], constant(2))};
+          absl::InlinedVector<llvm::Value*, 4> index_in_tile_2{
+              index_in_tile[0], index_in_tile[1],
+              builder->CreateAdd(index_in_tile_1[2], constant(1))};
+
+          auto index_1 = output_tile_index.AddOffset(index_in_tile_1, builder);
+          auto index_2 = output_tile_index.AddOffset(index_in_tile_2, builder);
+
+          auto boundary =
+              output_tile_index.AddOffset(transposed_tile_dimensions, builder);
+          auto boundary_multidim = boundary.multidim();
+          // auto index_1_multidim = index_1.multidim();
+          // auto* is_write_1 =
+          //     builder->CreateICmpULT(index_1_multidim[2], boundary_multidim[2]);
+          auto index_2_multidim = index_2.multidim();
+          auto* is_write_2 =
+              builder->CreateICmpULT(index_2_multidim[2], boundary_multidim[2]);
+
           for (const auto& tr : transposes) {
-            llvm::Value* loaded = tiles[tr.instr].Load(
-                Permute(index_in_tile, permutation), builder);
+            llvm::Value* loaded_1 = tiles[tr.instr].Load(
+                Permute(index_in_tile_1, permutation), builder);
+            llvm::Value* loaded_2 = tiles[tr.instr].Load(
+                Permute(index_in_tile_2, permutation), builder);
+
+            for (const auto& [output_idx, root] :
+                 transposes_to_roots[tr.instr]) {
+              if (root != tr.instr) continue;
+              auto output_index_1 =
+                  index_1.SourceIndexOfBitcast(tr.instr->shape(), builder);
+              auto output_index_2 =
+                  index_2.SourceIndexOfBitcast(tr.instr->shape(), builder);
+              // ksl.If("is_write_1", is_write_1, [&]() {
+              outputs[output_idx].EmitWriteArrayElement(output_index_1,
+                                                        loaded_1, builder);
+              // });
+              ksl.If("is_write_2", is_write_2, [&]() {
+                outputs[output_idx].EmitWriteArrayElement(output_index_2,
+                                                          loaded_2, builder);
+              });
+            }
 
             FusedIrEmitter fused_emitter(elemental_emitter);
-            fused_emitter.BindGenerator(
-                *tr.instr,
-                [&](const llvm_ir::IrArray::Index&) { return loaded; });
+
             for (int64_t i = 0;
                  i < fusion.fused_instructions_computation()->num_parameters();
                  ++i) {
@@ -248,24 +333,30 @@ absl::Status TransposeFusion::EmitKernel(IrEmitterContext& ir_emitter_context,
             // Compute all output values before writing them. This avoids
             // overwriting aliased input/output values before all reads
             // occurred.
-            std::vector<std::tuple<llvm_ir::IrArray, llvm_ir::IrArray::Index,
-                                   llvm::Value*>>
-                scheduled_writes;
             for (const auto& [output_idx, root] :
                  transposes_to_roots[tr.instr]) {
+              if (root == tr.instr) continue;
               TF_ASSIGN_OR_RETURN(llvm_ir::ElementGenerator gen,
                                   fused_emitter.GetGenerator(*root));
 
               // Both for emission and writing it should be
               // index-as-transformed by the computation.
-              auto untiled_index =
-                  index.SourceIndexOfBitcast(root->shape(), builder);
-              TF_ASSIGN_OR_RETURN(llvm::Value * generated, gen(untiled_index));
-              scheduled_writes.emplace_back(outputs[output_idx], untiled_index,
-                                            generated);
-            }
-            for (const auto& [output, idx, value] : scheduled_writes) {
-              output.EmitWriteArrayElement(idx, value, builder);
+              auto untiled_index_1 =
+                  index_1.SourceIndexOfBitcast(root->shape(), builder);
+              auto untiled_index_2 =
+                  index_2.SourceIndexOfBitcast(root->shape(), builder);
+              TF_ASSIGN_OR_RETURN(llvm::Value * generated_1,
+                                  gen(untiled_index_1));
+              TF_ASSIGN_OR_RETURN(llvm::Value * generated_2,
+                                  gen(untiled_index_2));
+              // ksl.If("is_write_1", is_write_1, [&]() {
+              outputs[output_idx].EmitWriteArrayElement(untiled_index_1,
+                                                        generated_1, builder);
+              // });
+              ksl.If("is_write_2", is_write_2, [&]() {
+                outputs[output_idx].EmitWriteArrayElement(untiled_index_2,
+                                                          generated_2, builder);
+              });
             }
           }
           return absl::OkStatus();
diff --git a/xla/service/gpu/gemm_rewriter.cc b/xla/service/gpu/gemm_rewriter.cc
index 0aa610fc92..3c4b34aced 100644
--- a/xla/service/gpu/gemm_rewriter.cc
+++ b/xla/service/gpu/gemm_rewriter.cc
@@ -613,6 +613,7 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {
 
   absl::Status HandleMultiply(HloInstruction *instr) override {
     HloInstruction *alpha, *existing_gemm;
+#if !TENSORFLOW_USE_SYCL
     if (Match(instr,
               m::MultiplyAnyOrder(
                   GemmOrCublasLtMatmulMaybeF8(&existing_gemm).WithOneUser(),
@@ -636,11 +637,13 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {
         return ReplaceInstruction(instr, existing_gemm);
       }
     }
+#endif  // !TENSORFLOW_USE_SYCL
 
     // Attempt to match approximate GELU activation
     // (https://arxiv.org/abs/1606.08415), where:
     // approx_gelu(x) = x * cdf(x)
     // cdf(x) = 0.5 * (1 + tanh(sqrt(2 / pi) * (x + 0.044715 * x**3))
+    // SYCL: OptionalBitcast?
     HloInstruction *cdf, *slice_or_bitcast = nullptr;
     if (Match(instr, m::MultiplyAnyOrder(
                          m::AnyOf<HloInstruction>(
@@ -680,6 +683,14 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {
                                   .WithOneUser())
                               .WithOneUser())
                           .WithOneUser())))) {
+      // SYCL
+      // gemm - bitcast - gelu - bitcast
+      if (instr->user_count() == 1) {
+        auto bitcast = instr->users()[0];
+        if (bitcast->opcode() == HloOpcode::kBitcast &&
+            ShapeUtil::Compatible(bitcast->shape(), existing_gemm->shape()))
+          return FuseGeluActivation(bitcast, existing_gemm);
+      }
       return FuseGeluActivation(instr, existing_gemm, slice_or_bitcast);
     }
     return absl::OkStatus();
@@ -1368,8 +1379,8 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {
       return in_out_alias_config.ParameterHasAlias(bias->parameter_number(),
                                                    /*param_index=*/{});
     }();
-    bool want_to_fuse_bias = IsCublasLtMatmulF8(*gemm) ||
-                             IsCublasLtMatmul(*gemm) || can_overwrite_bias;
+    // SYCL: cannot always fuse bias.
+    bool want_to_fuse_bias = can_overwrite_bias;
 
     auto gpu_config = gemm->backend_config<GpuBackendConfig>().value();
     GemmBackendConfig &config = *gpu_config.mutable_gemm_backend_config();
@@ -1678,13 +1689,14 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {
   absl::StatusOr<absl::string_view> GetNonFp8GemmCustomCallTarget(
       const HloInstruction &instr,
       const GemmBackendConfig &gemm_backend_config) const {
-    if (!instr.GetModule()
-             ->config()
-             .debug_options()
-             .xla_gpu_enable_cublaslt()) {
-      // cublasLt is not enabled.
-      return absl::string_view(kGemmCallTarget);
-    }
+    // SYCL: disable fallback
+    // if (!instr.GetModule()
+    //          ->config()
+    //          .debug_options()
+    //          .xla_gpu_enable_cublaslt()) {
+    //   // cublasLt is not enabled.
+    //   return absl::string_view(kGemmCallTarget);
+    // }
 
     // cublasLt is enabled, check if other internal conditions are met.
     const HloInstruction *lhs = instr.operand(0);
@@ -2026,11 +2038,6 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {
     for (auto batch_dimension : batch_dimensions) {
       batch_count *= lhs->shape().dimensions(batch_dimension);
     }
-    if (batch_count > kMaxBatchCount) {
-      // This is not supported by cublasLt.
-      return false;
-    }
-
     TF_ASSIGN_OR_RETURN(bool output_is_column_major,
                         MatrixIsColumnMajor(instr, gemm_backend_config));
 
diff --git a/xla/service/gpu/gpu_compiler.cc b/xla/service/gpu/gpu_compiler.cc
index d0c20aa1c8..c9d06ecf36 100644
--- a/xla/service/gpu/gpu_compiler.cc
+++ b/xla/service/gpu/gpu_compiler.cc
@@ -209,6 +209,7 @@ limitations under the License.
 #include "xla/service/rng_bit_generator_expander.h"
 #include "xla/service/rng_expander.h"
 #include "xla/service/scatter_expander.h"
+#include "xla/service/scatter_promotion.h"
 #include "xla/service/scatter_simplifier.h"
 #include "xla/service/sharding_propagation.h"
 #include "xla/service/sharding_remover.h"
@@ -268,6 +269,8 @@ limitations under the License.
 #include "xla/hlo/experimental/auto_sharding/auto_sharding.h"
 #endif  // PLATFORM_GOOGLE
 
+#include "xla/service/gpu/dot_expand_dims.h"
+
 namespace xla {
 namespace gpu {
 namespace {
@@ -594,7 +597,9 @@ AlgebraicSimplifierOptions LayoutInsensitiveAlgebraicSimplifierOptions(
   layout_insensitive_algsimp_opts
       .set_unconditionally_simplify_reduce_of_transpose_or_reshape(true);
 
-  if (gpu_target_config.platform_name == "ROCM") {
+  // SYCL: Conv swap has accuracy issue in some cases.
+  if (gpu_target_config.platform_name == "ROCM" ||
+      gpu_target_config.platform_name == "SYCL") {
     layout_insensitive_algsimp_opts.set_enable_conv_operand_swap(false);
   }
   layout_insensitive_algsimp_opts
@@ -654,9 +659,10 @@ absl::Status RunSPMDPasses(
     spmd_simplify.AddPass<ScatterSimplifier>();
     spmd_simplify.AddPass<ScatterExpander>(
         ScatterExpander::kEliminateSimpleScatters);
-    spmd_simplify.AddPass<GatherSimplifier>();
-    spmd_simplify.AddPass<GatherExpander>(
-        GatherExpander::kEliminateSimpleGathers);
+    // FIXME(intel): Reopen below pass when fix sharding error.
+    // spmd_simplify.AddPass<GatherSimplifier>();
+    // spmd_simplify.AddPass<GatherExpander>(
+    //     GatherExpander::kEliminateSimpleGathers);
     spmd_simplify.AddPass<WhileLoopConstantSinking>();
     spmd_simplify.AddPass<WhileLoopSimplifier>();
 
@@ -847,6 +853,10 @@ absl::Status RunOptimizationPasses(
 
     pipeline.AddPass<GatherSimplifier>();
     pipeline.AddPass<GatherExpander>(GatherExpander::kEliminateSimpleGathers);
+    // promote 16 bit integer scatter to 32-bit to avoid 16-bit atomic.
+    const std::pair<PrimitiveType, PrimitiveType> ar_promoted_types[] = {
+        {F16, F32}, {BF16, F32}};
+    pipeline.AddPass<ScatterPromotion>(ar_promoted_types);
     pipeline.AddPass<ScatterSimplifier>();
     pipeline.AddPass<ScatterExpander>(
         ScatterExpander::kEliminateSimpleScatters);
@@ -1016,6 +1026,10 @@ absl::Status RunLayoutAssignmentPasses(HloModule* hlo_module,
   HloPassPipeline pipeline("layout assignment");
   // Layout assignment uses alias analysis, which requires the call graph to
   // be flattened.
+  // SYCL: LLM passes.
+  bool llm_flag = false;
+  tsl::ReadBoolFromEnvVar("LLM", false, &llm_flag);
+  if (llm_flag) pipeline.AddPass<DotExpandDims>();
   pipeline.AddPass<FlattenCallGraph>();
   ChannelLayoutConstraints layout_constraints;
   pipeline.AddPass<GpuLayoutAssignment>(
@@ -1239,6 +1253,8 @@ absl::Status GpuCompiler::OptimizeHloModule(
   se::GpuComputeCapability gpu_version =
       gpu_target_config.device_description.gpu_compute_capability();
   se::dnn::VersionInfo dnn_version = gpu_target_config.dnn_version_info;
+  // SYCL: do not check dnn version
+#if 0
   if (stream_exec != nullptr) {
     gpu_version = GetGpuVersion(stream_exec);
     se::dnn::DnnSupport* dnn = stream_exec->AsDnn();
@@ -1249,7 +1265,7 @@ absl::Status GpuCompiler::OptimizeHloModule(
     }
     TF_ASSIGN_OR_RETURN(dnn_version, dnn->GetVersion());
   }
-
+#endif
   TF_RETURN_IF_ERROR(OptimizeHloConvolutionCanonicalization(
       hlo_module, gpu_version, dnn_version, options.device_allocator));
 
@@ -1411,10 +1427,13 @@ absl::Status GpuCompiler::OptimizeHloPostLayoutAssignment(
 
     // Rewrite FP8 GEMMs ahead of Triton which currently lacks support for FP8
     // and may rewrite quantized FP8 GEMMs as higher-precision GEMMs.
-    pipeline.AddPass<GemmRewriter>(gpu_version, /*f8_rewrite=*/true);
+    // pipeline.AddPass<GemmRewriter>(gpu_version, /*f8_rewrite=*/true);
+    // SYCL doesn't support fp8 gemm yet.
+    pipeline.AddPass<GemmRewriter>(gpu_version, /*f8_rewrite=*/false);
     if (debug_options.xla_gpu_enable_triton_gemm() && cuda_cc != nullptr &&
         cuda_cc->IsAtLeast(se::CudaComputeCapability::AMPERE)) {
-      pipeline.AddPass<GemmFusion>(gpu_version);
+      // SYCL: disable triton gemm
+      // pipeline.AddPass<GemmFusion>(gpu_version);
     }
     // Rewrite non-FP8 GEMMs.
     pipeline.AddPass<GemmRewriter>(gpu_version, /*f8_rewrite=*/false);
@@ -1436,8 +1455,9 @@ absl::Status GpuCompiler::OptimizeHloPostLayoutAssignment(
     if (debug_options.xla_gpu_enable_triton_softmax_fusion() &&
         cuda_cc != nullptr &&
         cuda_cc->IsAtLeast(se::CudaComputeCapability::AMPERE)) {
-      pipeline.AddPass<HloPassFix<AlgebraicSimplifier>>(simplifier_options);
-      pipeline.AddPass<SoftmaxRewriterTriton>(gpu_version);
+      // SYCL: disable triton
+      // pipeline.AddPass<HloPassFix<AlgebraicSimplifier>>(simplifier_options);
+      // pipeline.AddPass<SoftmaxRewriterTriton>(gpu_version);
     }
 
     pipeline.AddPass<ReductionDimensionGrouper>();
@@ -1770,6 +1790,11 @@ GpuCompiler::CompileSingleModule(const HloModuleConfig& module_config,
 
   // Write PTX to IR dump directory, if IR dumping was requested.
   if (should_dump) {
+    // SYCL: dump spv
+    auto spir_vector = result.binary;
+    std::string spir(spir_vector.begin(), spir_vector.end());
+    DumpToFileInDirOrStdout(*debug_module, "", "spv", spir);
+
     absl::string_view ptx = result.asm_text;
     if (debug_module) {
       DumpToFileInDirOrStdout(*debug_module, "",
@@ -2084,6 +2109,7 @@ absl::StatusOr<std::unique_ptr<Executable>> GpuCompiler::RunBackend(
 absl::StatusOr<std::vector<std::unique_ptr<AotCompilationResult>>>
 GpuCompiler::CompileAheadOfTime(std::unique_ptr<HloModuleGroup> module_group,
                                 const AotCompilationOptions& options) {
+#if 0
 #if GOOGLE_CUDA
   CHECK(options.PlatformId() == se::cuda::kCudaPlatformId);
 #elif TENSORFLOW_USE_ROCM
@@ -2137,6 +2163,7 @@ GpuCompiler::CompileAheadOfTime(std::unique_ptr<HloModuleGroup> module_group,
   }
 
   return std::move(results);
+#endif
 }
 
 HloCostAnalysis::ShapeSizeFunction GpuCompiler::ShapeSizeBytesFunction() const {
@@ -2215,12 +2242,16 @@ absl::Status GpuCompiler::RunPostSchedulingPipelines(
     auto driver_version = se::gpu::GpuDriver::GetDriverVersion();
 #if GOOGLE_CUDA
     constexpr int toolkit_version = CUDA_VERSION;
-#else
+#elif TENSORFLOW_USE_ROCM
     constexpr int toolkit_version = TF_ROCM_VERSION;
+#else
+    constexpr int toolkit_version = -1;
 #endif
+#if 0
     pipeline.AddPass<CommandBufferScheduling>(
         gpu_device_info, toolkit_version,
         driver_version.value_or(toolkit_version));
+#endif
     pipeline.AddPass<GpuSanitizeConstantNames>();
     TF_RETURN_IF_ERROR(pipeline.Run(module).status());
   }
diff --git a/xla/service/gpu/gpu_executable.cc b/xla/service/gpu/gpu_executable.cc
index 25c03adb2e..c684b26849 100644
--- a/xla/service/gpu/gpu_executable.cc
+++ b/xla/service/gpu/gpu_executable.cc
@@ -224,7 +224,9 @@ absl::Status GpuExecutable::CheckCompatibilityWithServiceExecutableRunOptions(
         << "}, but was {" << std::get<se::CudaComputeCapability>(cc).ToString()
         << "}";
   } else {
+#if !TENSORFLOW_USE_SYCL
     return Internal("Unknown platform");
+#endif
   }
 
   return absl::OkStatus();
@@ -390,6 +392,9 @@ absl::Status ExecuteThunks(
   se::StreamExecutor* executor = main_stream->parent();
   stream_executor::StreamPriority stream_priority =
       stream_executor::StreamPriority::Default;
+#if TENSORFLOW_USE_SYCL
+  use_highest_priority_for_async_stream = false;
+#endif
   if (use_highest_priority_for_async_stream) {
     stream_priority = stream_executor::StreamPriority::Highest;
   }
@@ -644,10 +649,16 @@ GpuExecutable::ResolveConstantGlobals(se::Stream* stream) {
   // The CUDA driver isn't able to load a PTX and a binary which are both empty.
   // It's okay if we skip loading in this case; if the module isn't loaded, all
   // symbol lookups will fail, just as they should for an empty module.
+#if !TENSORFLOW_USE_SYCL
   if (!(executor->platform()->id() == stream_executor::cuda::kCudaPlatformId &&
         binary().empty() && text().empty())) {
     TF_RETURN_IF_ERROR(executor->LoadModule(module_spec, &module_handle));
   }
+#else
+  if (module_spec.has_cuda_cubin_in_memory()) {
+    TF_RETURN_IF_ERROR(executor->LoadModule(module_spec, &module_handle));
+  }
+#endif
 
   // A flag signalling if constant initialization submitted memcpy operations
   // to the `stream`.
@@ -676,6 +687,26 @@ GpuExecutable::ResolveConstantGlobals(se::Stream* stream) {
         submitted_mem_copies = true;
       }
     } else {
+#if TENSORFLOW_USE_SYCL
+      // SYCL: content may be empty
+      if (info.content.span().empty()) {
+        // LLVM module contains the const variable, but it still fails to look
+        // up symbol. So allocate an empty buffer here.
+        void* opaque = nullptr;
+        size_t bytes = 0;
+        global = se::DeviceMemoryBase(opaque, bytes);
+      } else {
+        TF_ASSIGN_OR_RETURN(
+            auto shared, executor->CreateOrShareConstant(stream, info.content.span()));
+        global = *shared;
+        VLOG(3) << "Allocated (or shared) global " << info.symbol_name << " at "
+                << global.opaque();
+        // XLA will continue to own this global at least until this executable
+        // is destroyed (longer if another, longer-lived executable shares the
+        // same constant).
+        shared_constants_.push_back(std::move(shared));
+      }
+#else
       // The constant was not defined in the PTX and therefore must be both
       // allocated and initialized by XLA here.
       CHECK(!info.content.span().empty());
@@ -689,6 +720,7 @@ GpuExecutable::ResolveConstantGlobals(se::Stream* stream) {
       // destroyed (longer if another, longer-lived executable shares the same
       // constant).
       shared_constants_.push_back(std::move(shared));
+#endif
     }
 
     if (info.allocation_index != -1) {
diff --git a/xla/service/gpu/gpu_executable.h b/xla/service/gpu/gpu_executable.h
index e2e0daafe7..36b35f6c61 100644
--- a/xla/service/gpu/gpu_executable.h
+++ b/xla/service/gpu/gpu_executable.h
@@ -58,6 +58,25 @@ namespace gpu {
 // Returns whether GpuExecutable runs with Xla Runtime.
 bool IsXlaRuntimeExecutableEnabled(const HloModuleConfig& config);
 
+// SYCL: dummy GpuRuntimeProgram for compilation
+#if TENSORFLOW_USE_SYCL
+struct GpuRuntimeProgram {
+  GpuRuntimeProgram(std::string entry_point, std::string module,
+                    std::vector<int64_t> buffer_sizes,
+                    DebugOptions debug_options)
+      : entry_point(std::move(entry_point)),
+        module(std::move(module)),
+        buffer_sizes(std::move(buffer_sizes)),
+        debug_options(std::move(debug_options)) {}
+
+  std::string entry_point;
+  std::string module;
+  std::vector<int64_t> buffer_sizes;
+  DebugOptions debug_options;
+};
+class GpuRuntimeExecutable {};
+#endif
+
 // GPU-targeting implementation of the XLA Executable interface.
 //
 // Launches the given GPU kernel via the StreamExecutor.
diff --git a/xla/service/gpu/gpu_fused_mha_runner.cc b/xla/service/gpu/gpu_fused_mha_runner.cc
index ab157e81ba..4094fe724a 100644
--- a/xla/service/gpu/gpu_fused_mha_runner.cc
+++ b/xla/service/gpu/gpu_fused_mha_runner.cc
@@ -172,6 +172,8 @@ void AssignScale(GpufMHAConfig &config,
   double fmha_scale = 0.0;
 
   switch (config.kind) {
+    // SYCL: supports bias + softmax
+    case CudnnfMHAKind::kSoftmax:
     case CudnnfMHAKind::kScaleBiasMaskSoftmax:
     case CudnnfMHAKind::kScaleBiasMaskSoftmaxDropout:
     case CudnnfMHAKind::kScaleMaskSoftmax:
diff --git a/xla/service/gpu/gpu_fusible.cc b/xla/service/gpu/gpu_fusible.cc
index 02344a7fb1..3f030f1543 100644
--- a/xla/service/gpu/gpu_fusible.cc
+++ b/xla/service/gpu/gpu_fusible.cc
@@ -447,6 +447,13 @@ FusionDecision IsProducerConsumerFusible(const HloInstruction& producer,
   }
 
   if (IsInputFusibleReduction(producer)) {
+#if TENSORFLOW_USE_SYCL
+    // TODO: Check on latest XLA. Reduction epilogue fusion may cause
+    // regression for row reductions cases with large dim.
+    // It changes fusion kind from kInput to kLoop, and
+    // will fail to enter the row vectorization pass.
+    return "Reduction epilogue fusion is not enabled.";
+#endif
     if (!producer.GetModule()
              ->config()
              .debug_options()
diff --git a/xla/service/gpu/gpu_layout_assignment.cc b/xla/service/gpu/gpu_layout_assignment.cc
index 8a26b0419b..8b9b8d5518 100644
--- a/xla/service/gpu/gpu_layout_assignment.cc
+++ b/xla/service/gpu/gpu_layout_assignment.cc
@@ -86,7 +86,7 @@ HeuristicLayoutAssignment(const HloInstruction* instr,
       std::make_tuple(DataLayout::kBatchDepthYX4, FilterLayout::kOutputInputYX4,
                       DataLayout::kBatchDepthYX4);
   constexpr auto kAllNHWC =
-      std::make_tuple(DataLayout::kBatchYXDepth, FilterLayout::kOutputYXInput,
+      std::make_tuple(DataLayout::kBatchYXDepth, FilterLayout::kYXInputOutput,
                       DataLayout::kBatchYXDepth);
 
   // Integer convolution must use NHWC or NCHW_VECT_C.
diff --git a/xla/service/gpu/gpu_sanitize_constant_names.cc b/xla/service/gpu/gpu_sanitize_constant_names.cc
index d948882dd0..8de4d4a185 100644
--- a/xla/service/gpu/gpu_sanitize_constant_names.cc
+++ b/xla/service/gpu/gpu_sanitize_constant_names.cc
@@ -42,7 +42,7 @@ absl::StatusOr<bool> GpuSanitizeConstantNames::Run(
         continue;
       }
 
-      instr->UniquifyName(&instr_name_uniquer);
+      instr_name_uniquer.GetUniqueName(instr->name());
     }
   }
 
diff --git a/xla/service/gpu/gpu_transfer_manager.cc b/xla/service/gpu/gpu_transfer_manager.cc
index b890c8b63c..dcbc0c1132 100644
--- a/xla/service/gpu/gpu_transfer_manager.cc
+++ b/xla/service/gpu/gpu_transfer_manager.cc
@@ -46,6 +46,7 @@ limitations under the License.
 #include "xla/stream_executor/memory_allocation.h"
 #include "xla/stream_executor/platform.h"
 #include "xla/stream_executor/rocm/rocm_platform_id.h"
+#include "xla/stream_executor/sycl/sycl_platform_id.h"
 #include "xla/stream_executor/stream_executor.h"
 #include "xla/util.h"
 #include "tsl/platform/errors.h"
@@ -344,11 +345,20 @@ static std::unique_ptr<xla::TransferManager> CreateAMDGPUTransferManager() {
           .getPointerSize(0 /* default address space */));
 }
 
+static std::unique_ptr<xla::TransferManager> CreateSYCLTransferManager() {
+  return std::make_unique<xla::gpu::GpuTransferManager>(
+      /*id=*/stream_executor::sycl::kSyclPlatformId,
+      /*pointer_size=*/llvm::DataLayout(xla::gpu::spir::DataLayout())
+          .getPointerSize(0 /* default address space */));
+}
+
 static bool InitModule() {
   xla::TransferManager::RegisterTransferManager(
       stream_executor::cuda::kCudaPlatformId, &CreateNVPTXTransferManager);
   xla::TransferManager::RegisterTransferManager(
       stream_executor::rocm::kROCmPlatformId, &CreateAMDGPUTransferManager);
+  xla::TransferManager::RegisterTransferManager(
+      stream_executor::sycl::kSyclPlatformId, &CreateSYCLTransferManager);
   return true;
 }
 
diff --git a/xla/service/gpu/ir_emission_utils.cc b/xla/service/gpu/ir_emission_utils.cc
index 6265c5845d..a627bb2adf 100644
--- a/xla/service/gpu/ir_emission_utils.cc
+++ b/xla/service/gpu/ir_emission_utils.cc
@@ -121,13 +121,12 @@ bool IsMatrixMultiplication(const HloInstruction& dot) {
   const DotDimensionNumbers& dim_numbers = dot.dot_dimension_numbers();
 
   PrimitiveType output_primitive_type = dot.shape().element_type();
+  // Disable F64, C64, C128
   bool type_is_allowed =
       (output_primitive_type == F8E4M3FN || output_primitive_type == F8E5M2 ||
        output_primitive_type == F8E4M3FNUZ ||
        output_primitive_type == F8E5M2FNUZ || output_primitive_type == F16 ||
-       output_primitive_type == BF16 || output_primitive_type == F32 ||
-       output_primitive_type == F64 || output_primitive_type == C64 ||
-       output_primitive_type == C128) ||
+       output_primitive_type == BF16 || output_primitive_type == F32) ||
       (output_primitive_type == S32 && lhs_shape.element_type() == S8 &&
        rhs_shape.element_type() == S8);
   bool shapes_are_valid =
@@ -150,11 +149,11 @@ bool IsMatrixVectorMultiplication(const HloInstruction& dot) {
   const DotDimensionNumbers& dim_numbers = dot.dot_dimension_numbers();
 
   PrimitiveType output_primitive_type = dot.shape().element_type();
+  // Disable F64, C64, C128
   bool type_is_allowed =
       (output_primitive_type == F8E4M3FN || output_primitive_type == F8E5M2 ||
        output_primitive_type == F16 || output_primitive_type == BF16 ||
-       output_primitive_type == F32 || output_primitive_type == F64 ||
-       output_primitive_type == C64 || output_primitive_type == C128) ||
+       output_primitive_type == F32) ||
       (output_primitive_type == S32 && lhs_shape.element_type() == S8 &&
        rhs_shape.element_type() == S8);
 
diff --git a/xla/service/gpu/ir_emitter_context.cc b/xla/service/gpu/ir_emitter_context.cc
index 38a5e71306..8c8a74da85 100644
--- a/xla/service/gpu/ir_emitter_context.cc
+++ b/xla/service/gpu/ir_emitter_context.cc
@@ -67,6 +67,8 @@ void IrEmitterContext::emit_constant(int64_t num_elements,
       return llvm::ConstantAggregateZero::get(global_type);
     }
 
+    // SYCL: always set info.content.
+    info.content = content;
     std::vector<uint8_t> padded(kMinConstAllocationInBytes, 0);
     absl::c_copy(content.span(), padded.begin());
     return llvm::ConstantDataArray::get<uint8_t>(
@@ -77,8 +79,8 @@ void IrEmitterContext::emit_constant(int64_t num_elements,
   }();
 
   // Explicitly set global addrspace for SPIR backend.
-  int addrspace =
-      llvm::Triple(llvm_module_->getTargetTriple()).isSPIR() ? 1 : 0;
+  auto is_spir = llvm::Triple(llvm_module_->getTargetTriple()).isSPIR();
+  int addrspace = is_spir ? 1 : 0;
   // These globals will be looked up by name by GpuExecutable so we need to
   // give them an external linkage.  Not all of their uses are visible in
   // the LLVM IR so we can't give then a linkage that merely preserves their
@@ -95,6 +97,27 @@ void IrEmitterContext::emit_constant(int64_t num_elements,
       /*AddressSpace=*/addrspace,
       /*isExternallyInitialized=*/false);
   global_for_const->setAlignment(llvm::Align(kConstantBufferAlignBytes));
+
+  if (is_spir) {
+    // SYCL: Add spirv.Decorations for global variable. See document about the
+    // annotation:
+    // https://github.com/intel/llvm/blob/sycl/sycl/doc/design/spirv-extensions/SPV_INTEL_global_variable_decorations.asciidoc
+    llvm::LLVMContext& context = llvm_module_->getContext();
+    llvm::SmallVector<llvm::Metadata*, 4> metadatas;
+    std::vector<llvm::Metadata*> ops;
+
+    auto* kind = llvm::ConstantAsMetadata::get(llvm::ConstantInt::get(
+        llvm::Type::getInt32Ty(context), /*IDecHostAccessINTEL*/ 6147));
+    ops.push_back(kind);
+    auto* const acc_mode = llvm::ConstantAsMetadata::get(llvm::ConstantInt::get(
+        llvm::Type::getInt32Ty(context), /*AccessMode*/ 2));
+    ops.push_back(acc_mode);
+    ops.push_back(llvm::MDString::get(context, symbol_name));
+    metadatas.push_back(llvm::MDNode::get(context, ops));
+
+    llvm::MDNode* md_list = llvm::MDNode::get(context, metadatas);
+    global_for_const->setMetadata("spirv.Decorations", md_list);
+  }
   llvm_module_->insertGlobalVariable(global_for_const);
 
   info.symbol_name.assign(symbol_name);
diff --git a/xla/service/gpu/ir_emitter_context.h b/xla/service/gpu/ir_emitter_context.h
index afbf212bad..54e07a26db 100644
--- a/xla/service/gpu/ir_emitter_context.h
+++ b/xla/service/gpu/ir_emitter_context.h
@@ -35,7 +35,8 @@ limitations under the License.
 #include "xla/service/gpu/gpu_executable.h"
 #include "xla/service/gpu/ir_emission_utils.h"
 #include "xla/service/gpu/kernel_reuse_cache.h"
-#include "xla/service/gpu/runtime/nccl_collective_thunk.h"
+// #include "xla/service/gpu/runtime/nccl_collective_thunk.h"
+#include "xla/service/gpu/ccl_collective_thunk.h"
 #include "xla/service/name_uniquer.h"
 #include "xla/stream_executor/device_description.h"
 
diff --git a/xla/service/gpu/ir_emitter_nested.cc b/xla/service/gpu/ir_emitter_nested.cc
index 7403531a82..4a57c815a6 100644
--- a/xla/service/gpu/ir_emitter_nested.cc
+++ b/xla/service/gpu/ir_emitter_nested.cc
@@ -326,7 +326,8 @@ void EmitAMDGPUAtomicAdd(llvm::IRBuilder<>* builder,
 
   builder->CreateAtomicRMW(
       llvm::AtomicRMWInst::FAdd, output_ptr, source, llvm::MaybeAlign(),
-      llvm::AtomicOrdering::SequentiallyConsistent,
+      // SYCL: set Monotonic.
+      llvm::AtomicOrdering::Monotonic,
       builder->getContext().getOrInsertSyncScopeID("agent"));
 }
 
@@ -398,7 +399,7 @@ bool MaybeEmitDirectAtomicOperation(llvm::IRBuilder<>* builder,
       if (atomic_add_supported) {
         builder->CreateAtomicRMW(llvm::AtomicRMWInst::FAdd, output_address,
                                  source, llvm::MaybeAlign(),
-                                 llvm::AtomicOrdering::SequentiallyConsistent);
+                                 llvm::AtomicOrdering::Monotonic);
         return true;
       }
     }
@@ -412,11 +413,19 @@ bool MaybeEmitDirectAtomicOperation(llvm::IRBuilder<>* builder,
       return true;
     }
 
+    if (target_triple.isSPIR() &&
+        element_type == F32) {
+      builder->CreateAtomicRMW(llvm::AtomicRMWInst::FAdd, output_address,
+                               source, llvm::MaybeAlign(),
+                               llvm::AtomicOrdering::Monotonic);
+      return true;
+    }
+
     if (is_atomic_integral) {
       // integral + integral
       builder->CreateAtomicRMW(
           llvm::AtomicRMWInst::Add, output_address, source, llvm::MaybeAlign(),
-          llvm::AtomicOrdering::SequentiallyConsistent, sync_scope);
+          llvm::AtomicOrdering::Monotonic, sync_scope);
       return true;
     }
   }
@@ -433,7 +442,7 @@ bool MaybeEmitDirectAtomicOperation(llvm::IRBuilder<>* builder,
                         : llvm::AtomicRMWInst::UMax;
       builder->CreateAtomicRMW(
           opcode, output_address, source, llvm::MaybeAlign(),
-          llvm::AtomicOrdering::SequentiallyConsistent, sync_scope);
+          llvm::AtomicOrdering::Monotonic, sync_scope);
       return true;
     } else if (element_type == F32) {
       // max(float, float) via AtomicMax and AtomicMin on int
@@ -486,7 +495,7 @@ bool MaybeEmitDirectAtomicOperation(llvm::IRBuilder<>* builder,
                     builder->CreateAtomicRMW(
                         llvm::AtomicRMWInst::Max, output_address,
                         source_float_as_int, llvm::MaybeAlign(),
-                        llvm::AtomicOrdering::SequentiallyConsistent,
+                        llvm::AtomicOrdering::Monotonic,
                         sync_scope);
                   },
                   [&]() {
@@ -494,7 +503,7 @@ bool MaybeEmitDirectAtomicOperation(llvm::IRBuilder<>* builder,
                     builder->CreateAtomicRMW(
                         llvm::AtomicRMWInst::UMin, output_address,
                         source_float_as_int, llvm::MaybeAlign(),
-                        llvm::AtomicOrdering::SequentiallyConsistent,
+                        llvm::AtomicOrdering::Monotonic,
                         sync_scope);
                   });
             });
@@ -510,7 +519,7 @@ bool MaybeEmitDirectAtomicOperation(llvm::IRBuilder<>* builder,
                       ? llvm::AtomicRMWInst::Min
                       : llvm::AtomicRMWInst::UMin;
     builder->CreateAtomicRMW(opcode, output_address, source, llvm::MaybeAlign(),
-                             llvm::AtomicOrdering::SequentiallyConsistent,
+                             llvm::AtomicOrdering::Monotonic,
                              sync_scope);
     return true;
   }
@@ -679,8 +688,8 @@ absl::Status EmitAtomicOperationUsingCAS(llvm::IRBuilder<>* builder,
   //                                       cas_new_output);
   llvm::Value* ret_value = builder->CreateAtomicCmpXchg(
       atomic_memory_address, cas_old_output, cas_new_output, llvm::MaybeAlign(),
-      llvm::AtomicOrdering::SequentiallyConsistent,
-      llvm::AtomicOrdering::SequentiallyConsistent, DetermineSyncScope(module));
+      llvm::AtomicOrdering::Monotonic,
+      llvm::AtomicOrdering::Monotonic, DetermineSyncScope(module));
 
   // Extract the memory value returned from atomicCAS and store it as
   // cas_old_output.
diff --git a/xla/service/gpu/ir_emitter_unnested.cc b/xla/service/gpu/ir_emitter_unnested.cc
index 4ba739dba4..4b7920bde4 100644
--- a/xla/service/gpu/ir_emitter_unnested.cc
+++ b/xla/service/gpu/ir_emitter_unnested.cc
@@ -1,4 +1,6 @@
-/*Copyright 2022 The OpenXLA Authors.
+/* Copyright (c) 2024 Intel Corporation
+
+Copyright 2022 The OpenXLA Authors.
 
 Licensed under the Apache License, Version 2.0 (the "License");
 you may not use this file except in compliance with the License.
@@ -109,10 +111,14 @@ limitations under the License.
 #include "xla/service/gpu/kernels/topk_custom_kernel.h"
 #include "xla/service/gpu/launch_dimensions.h"
 #include "xla/service/gpu/matmul_utils.h"
+// TODO SYCL: CCL may need rewrite
+#include "xla/service/gpu/ccl_all_to_all_thunk.h"
+#include "xla/service/gpu/ccl_collective_broadcast_thunk.h"
+#include "xla/service/gpu/ccl_collective_permute_thunk.h"
+#include "xla/service/gpu/ccl_collective_thunk.h"
+#include "xla/service/gpu/ccl_all_gather_thunk.h"
+#include "xla/service/gpu/ccl_all_reduce_thunk.h"
 #include "xla/service/gpu/parallel_loop_emitter.h"
-#include "xla/service/gpu/runtime/command_buffer_cmd.h"
-#include "xla/service/gpu/runtime/command_buffer_cmd_emitter.h"
-#include "xla/service/gpu/runtime/command_buffer_thunk.h"
 #include "xla/service/gpu/runtime/conditional_thunk.h"
 #include "xla/service/gpu/runtime/convolution_thunk.h"
 #include "xla/service/gpu/runtime/copy_thunk.h"
@@ -122,19 +128,19 @@ limitations under the License.
 #include "xla/service/gpu/runtime/gemm_thunk.h"
 #include "xla/service/gpu/runtime/infeed_thunk.h"
 #include "xla/service/gpu/runtime/kernel_thunk.h"
-#include "xla/service/gpu/runtime/nccl_all_gather_thunk.h"
-#include "xla/service/gpu/runtime/nccl_all_reduce_thunk.h"
-#include "xla/service/gpu/runtime/nccl_all_to_all_thunk.h"
-#include "xla/service/gpu/runtime/nccl_api.h"
-#include "xla/service/gpu/runtime/nccl_collective_broadcast_thunk.h"
-#include "xla/service/gpu/runtime/nccl_collective_permute_thunk.h"
-#include "xla/service/gpu/runtime/nccl_collective_thunk.h"
-#include "xla/service/gpu/runtime/nccl_recv_thunk.h"
-#include "xla/service/gpu/runtime/nccl_send_thunk.h"
+// #include "xla/service/gpu/runtime/nccl_all_gather_thunk.h"
+// #include "xla/service/gpu/runtime/nccl_all_reduce_thunk.h"
+// #include "xla/service/gpu/runtime/nccl_all_to_all_thunk.h"
+// #include "xla/service/gpu/runtime/nccl_api.h"
+// #include "xla/service/gpu/runtime/nccl_collective_broadcast_thunk.h"
+// #include "xla/service/gpu/runtime/nccl_collective_permute_thunk.h"
+// #include "xla/service/gpu/runtime/nccl_collective_thunk.h"
+// #include "xla/service/gpu/runtime/nccl_recv_thunk.h"
+// #include "xla/service/gpu/runtime/nccl_send_thunk.h"
 #include "xla/service/gpu/runtime/norm_thunk.h"
 #include "xla/service/gpu/runtime/outfeed_thunk.h"
 #include "xla/service/gpu/runtime/replica_id_thunk.h"
-#include "xla/service/gpu/runtime/send_recv_thunk.h"
+// #include "xla/service/gpu/runtime/send_recv_thunk.h"
 #include "xla/service/gpu/runtime/sequential_thunk.h"
 #include "xla/service/gpu/runtime/thunk.h"
 #include "xla/service/gpu/runtime/wait_for_streams_thunk.h"
@@ -164,16 +170,16 @@ limitations under the License.
 #include "tsl/protobuf/dnn.pb.h"
 #include "triton/Dialect/Triton/IR/Dialect.h"
 
-#if GOOGLE_CUDA || TF_HIPBLASLT
+#if GOOGLE_CUDA || TF_HIPBLASLT || TENSORFLOW_USE_SYCL
 #include "xla/service/gpu/runtime/gpublas_lt_matmul_thunk.h"
-#endif  // GOOGLE_CUDA || TF_HIPBLASLT
+#endif  // GOOGLE_CUDA || TF_HIPBLASLT || TENSORFLOW_USE_SYCL
 
-#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM
-#include "xla/service/gpu/ir_emitter_triton.h"
+#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM || TENSORFLOW_USE_SYCL
+// #include "xla/service/gpu/ir_emitter_triton.h"
 #include "xla/service/gpu/runtime/cholesky_thunk.h"
-#include "xla/service/gpu/runtime/cub_sort_thunk.h"
+// #include "xla/service/gpu/runtime/cub_sort_thunk.h"
 #include "xla/service/gpu/runtime/triangular_solve_thunk.h"
-#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM
+#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM || TENSORFLOW_USE_SYCL
 
 namespace xla {
 namespace gpu {
@@ -557,6 +563,7 @@ absl::Status IrEmitterUnnested::EmitSliceToDynamic(
 
 absl::Status IrEmitterUnnested::EmitCommandBufferThunk(
     const HloInstruction* instr) {
+#ifndef TENSORFLOW_USE_SYCL
   // Spawn a new IrEmitterUnnested to emit thunks for the command buffer
   // computation. Then convert emitted thunks to a sequence of CommandBufferCmd.
   // The resulting thunk added to the thunk sequence is a CommandBufferThunk.
@@ -583,6 +590,7 @@ absl::Status IrEmitterUnnested::EmitCommandBufferThunk(
   AddThunkToThunkSequence(std::make_unique<CommandBufferThunk>(
       std::move(cmd_sequence), Thunk::ThunkInfo::WithProfileAnnotation(instr),
       std::move(*thunk_sequence)));
+#endif  // TENSORFLOW_USE_SYCL
 
   return absl::OkStatus();
 }
@@ -665,7 +673,7 @@ absl::Status IrEmitterUnnested::EmitGemmThunk(
   return absl::OkStatus();
 }
 
-#if GOOGLE_CUDA || TF_HIPBLASLT
+#if GOOGLE_CUDA || TF_HIPBLASLT || TENSORFLOW_USE_SYCL
 
 absl::Status IrEmitterUnnested::EmitCublasLtMatmulThunk(
     const HloCustomCallInstruction* instr) {
@@ -936,6 +944,8 @@ absl::Status IrEmitterUnnested::EmitNormThunk(
   return absl::OkStatus();
 }
 
+#endif  // GOOGLE_CUDA
+
 absl::Status IrEmitterUnnested::EmitFusedMHAThunk(
     const HloCustomCallInstruction* instr) {
   const HloInstruction* lhs_bmm1 = instr->operand(0);
@@ -1202,8 +1212,6 @@ absl::Status IrEmitterUnnested::EmitFusedMHABackwardThunk(
   return absl::OkStatus();
 }
 
-#endif  // GOOGLE_CUDA
-
 absl::StatusOr<BufferAllocation::Slice>
 IrEmitterUnnested::GetAllocationSliceForHlo(const HloInstruction* instr,
                                             const ShapeIndex& index) const {
@@ -1211,8 +1219,8 @@ IrEmitterUnnested::GetAllocationSliceForHlo(const HloInstruction* instr,
                                       instr, index);
 }
 
-#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM
-
+#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM || TENSORFLOW_USE_SYCL
+#if 0
 absl::Status IrEmitterUnnested::EmitCubDeviceRadixSort(
     const HloCustomCallInstruction* instr) {
   if (instr->operand_count() != 1 && instr->operand_count() != 2) {
@@ -1253,7 +1261,7 @@ absl::Status IrEmitterUnnested::EmitCubDeviceRadixSort(
   AddThunkToThunkSequence(std::move(thunk));
   return absl::OkStatus();
 }
-
+#endif
 absl::Status IrEmitterUnnested::EmitCholeskyThunk(const HloInstruction* instr) {
   TF_ASSIGN_OR_RETURN(CholeskyOptions options,
                       instr->backend_config<CholeskyOptions>());
@@ -1301,7 +1309,144 @@ absl::Status IrEmitterUnnested::EmitCholeskyThunk(const HloInstruction* instr) {
 
   return absl::OkStatus();
 }
-#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM
+#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM || TENSORFLOW_USE_SYCL
+
+#ifdef TENSORFLOW_USE_SYCL
+absl::StatusOr<CustomCallThunk::AttributesMap> BuildAttributesMap(
+    const HloCustomCallInstruction* instr) {
+  CustomCallThunk::AttributesMap attrs;
+  if (IsCustomCallToDnnConvolution(*instr)) {
+    TF_ASSIGN_OR_RETURN(auto gpu_config,
+                        instr->backend_config<GpuBackendConfig>());
+    const CudnnConvBackendConfig& backend_config =
+        gpu_config.cudnn_conv_backend_config();
+    TF_ASSIGN_OR_RETURN(CudnnConvKind kind, GetCudnnConvKind(instr));
+    attrs["conv_result_scale"] =
+        static_cast<float>(backend_config.conv_result_scale());
+    attrs["side_input_scale"] =
+        static_cast<float>(backend_config.side_input_scale());
+    attrs["activation_mode"] =
+        static_cast<int32_t>(backend_config.activation_mode());
+    attrs["leakyrelu_alpha"] =
+        static_cast<float>(backend_config.leakyrelu_alpha());
+
+    const Window& window = instr->window();
+    const ConvolutionDimensionNumbers& dnums =
+        instr->convolution_dimension_numbers();
+    const int num_dimensions = window.dimensions_size();
+    const Shape& operand0_shape = instr->operand(0)->shape();
+    const Shape& operand1_shape = instr->operand(1)->shape();
+    const Shape& result_shape = instr->shape().tuple_shapes(0);
+
+    attrs["window_ShortDebugString"] = window.ShortDebugString();
+    attrs["window_num_dimensions"] = window.dimensions_size();
+    for (int i = 0; i < window.dimensions_size(); ++i) {
+      attrs["window_padding_low_" + std::to_string(i)] =
+          window.dimensions(i).padding_low();
+      attrs["window_padding_high_" + std::to_string(i)] =
+          window.dimensions(i).padding_high();
+      attrs["window_stride_" + std::to_string(i)] =
+          window.dimensions(i).stride();
+      attrs["window_dilation_" + std::to_string(i)] =
+          window.dimensions(i).window_dilation();
+    }
+
+    attrs["dnums_ShortDebugString"] = dnums.ShortDebugString();
+    attrs["input_feature_dimension"] = dnums.input_feature_dimension();
+    attrs["input_batch_dimension"] = dnums.input_batch_dimension();
+    attrs["output_feature_dimension"] = dnums.output_feature_dimension();
+    attrs["kernel_input_feature_dimension"] =
+        dnums.kernel_input_feature_dimension();
+    attrs["kernel_output_feature_dimension"] =
+        dnums.kernel_output_feature_dimension();
+    for (int i = 0; i < num_dimensions; ++i) {
+      attrs["input_spatial_dimensions_" + std::to_string(i)] =
+          dnums.input_spatial_dimensions(i);
+      attrs["output_spatial_dimensions_" + std::to_string(i)] =
+          dnums.output_spatial_dimensions(i);
+      attrs["kernel_spatial_dimensions_" + std::to_string(i)] =
+          dnums.kernel_spatial_dimensions(i);
+      attrs["kernel_spatial_dimensions_" + std::to_string(i)] =
+          dnums.kernel_spatial_dimensions(i);
+    }
+    stream_executor::dnn::DataLayout input_dl;
+    stream_executor::dnn::FilterLayout filter_dl;
+    stream_executor::dnn::DataLayout output_dl;
+    if (kind == CudnnConvKind::kForward ||
+        kind == CudnnConvKind::kForwardActivation) {
+      TF_ASSIGN_OR_RETURN(
+          std::tie(input_dl, filter_dl, output_dl),
+          XlaConvShapesToStreamExecutorLayouts(dnums, operand0_shape,
+                                               operand1_shape, result_shape));
+    } else if (kind == CudnnConvKind::kBackwardInput) {
+      TF_ASSIGN_OR_RETURN(
+          std::tie(input_dl, filter_dl, output_dl),
+          XlaConvShapesToStreamExecutorLayouts(dnums, result_shape,
+                                               operand1_shape, operand0_shape));
+    } else if (kind == CudnnConvKind::kBackwardFilter) {
+      TF_ASSIGN_OR_RETURN(
+          std::tie(input_dl, filter_dl, output_dl),
+          XlaConvShapesToStreamExecutorLayouts(dnums, operand0_shape,
+                                               result_shape, operand1_shape));
+    } else {
+      return Internal("Unkown convolution kind");
+    }
+    attrs["input_dl"] = static_cast<int32_t>(input_dl);
+    attrs["filter_dl"] = static_cast<int32_t>(filter_dl);
+    attrs["output_dl"] = static_cast<int32_t>(output_dl);
+  } else if (IsLegacyCublasMatmul(*instr) || IsCublasLtMatmul(*instr)) {
+    TF_ASSIGN_OR_RETURN(const auto gpu_config,
+                        instr->backend_config<xla::gpu::GpuBackendConfig>());
+    xla::gpu::GemmBackendConfig config = gpu_config.gemm_backend_config();
+    xla::gpu::GemmBackendConfig_Epilogue epilogue = config.epilogue();
+    attrs["epilogue"] = static_cast<int32_t>(epilogue);
+
+    TF_ASSIGN_OR_RETURN(
+        auto gemm_config,
+        GemmConfig::For(static_cast<const HloInstruction*>(instr)));
+
+    attrs["lhs_layout_dtype"] =
+        static_cast<int32_t>(gemm_config.lhs_layout.dtype);
+    attrs["lhs_order"] = static_cast<int32_t>(gemm_config.lhs_layout.order);
+    attrs["lhs_num_cols"] = gemm_config.lhs_layout.num_cols;
+    attrs["lhs_num_rows"] = gemm_config.lhs_layout.num_rows;
+    attrs["lhs_batch_stride"] = gemm_config.lhs_layout.batch_stride;
+    attrs["lhs_leading_dim_stride"] = gemm_config.lhs_layout.leading_dim_stride;
+
+    attrs["rhs_layout_dtype"] =
+        static_cast<int32_t>(gemm_config.rhs_layout.dtype);
+    attrs["rhs_order"] = static_cast<int32_t>(gemm_config.rhs_layout.order);
+    attrs["rhs_num_cols"] = gemm_config.rhs_layout.num_cols;
+    attrs["rhs_num_rows"] = gemm_config.rhs_layout.num_rows;
+    attrs["rhs_batch_stride"] = gemm_config.rhs_layout.batch_stride;
+    attrs["rhs_leading_dim_stride"] = gemm_config.rhs_layout.leading_dim_stride;
+
+    attrs["output_layout_dtype"] =
+        static_cast<int32_t>(gemm_config.output_layout.dtype);
+    attrs["output_order"] =
+        static_cast<int32_t>(gemm_config.output_layout.order);
+    attrs["output_num_cols"] = gemm_config.output_layout.num_cols;
+    attrs["output_num_rows"] = gemm_config.output_layout.num_rows;
+    attrs["output_batch_stride"] = gemm_config.output_layout.batch_stride;
+    attrs["output_leading_dim_stride"] =
+        gemm_config.output_layout.leading_dim_stride;
+
+    attrs["batch_size"] =
+        static_cast<int64_t>(gemm_config.output_layout.batch_size);
+    attrs["alpha"] = static_cast<float>(gemm_config.alpha.real());
+    attrs["beta"] = static_cast<float>(gemm_config.beta);
+
+    // config.algorithm is less than 0, thus 0 means no algorithm
+    if (gemm_config.algorithm.has_value()) {
+      attrs["algorithm"] = static_cast<int64_t>(gemm_config.algorithm.value());
+    } else
+      attrs["algorithm"] = static_cast<int64_t>(0);
+  } else {
+    return absl::InternalError("Unknown CustomCall To SYCL FFI Call");
+  }
+  return attrs;
+}
+#endif  // TENSORFLOW_USE_SYCL
 
 absl::Status IrEmitterUnnested::EmitCustomCallThunk(
     const HloCustomCallInstruction* instr) {
@@ -1431,6 +1576,12 @@ absl::Status IrEmitterUnnested::EmitCustomCallThunk(
       break;
 
     case CustomCallApiVersion::API_VERSION_TYPED_FFI:
+#ifdef TENSORFLOW_USE_SYCL
+    {
+      TF_ASSIGN_OR_RETURN(attributes, BuildAttributesMap(instr));
+      break;
+    }
+#else
       if (!backend_config_str.empty()) {
         mlir::Attribute attr = mlir::parseAttribute(
             backend_config_str, ir_emitter_context_->mlir_context());
@@ -1443,7 +1594,7 @@ absl::Status IrEmitterUnnested::EmitCustomCallThunk(
             "dictionary attribute");
       }
       break;
-
+#endif  // TENSORFLOW_USE_SYCL
     default:
       return Internal("Unknown custom-call API version enum value: %d",
                       instr->api_version());
@@ -1484,7 +1635,7 @@ absl::Status IrEmitterUnnested::EmitFftThunk(const HloFftInstruction* instr) {
   return absl::OkStatus();
 }
 
-#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM
+#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM || TENSORFLOW_USE_SYCL
 
 absl::Status IrEmitterUnnested::EmitTriangularSolveCustomCall(
     const HloInstruction* instr) {
@@ -1564,7 +1715,7 @@ absl::Status IrEmitterUnnested::EmitTriangularSolveCustomCall(
   }
   return absl::OkStatus();
 }
-#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM
+#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM || TENSORFLOW_USE_SYCL
 
 absl::Status IrEmitterUnnested::EmitTopKCustomCall(
     const HloCustomCallInstruction* instr) {
@@ -2617,6 +2768,7 @@ absl::Status IrEmitterUnnested::EmitCopyStartThunk(
 }
 
 absl::Status IrEmitterUnnested::EmitSendThunk(const HloSendInstruction* instr) {
+#if 0
   if (!instr->channel_id().has_value())
     return absl::InternalError("Unknown send instruction channel id");
 
@@ -2669,12 +2821,14 @@ absl::Status IrEmitterUnnested::EmitSendThunk(const HloSendInstruction* instr) {
       *instr->channel_id(), send_recv_events_,
       ConvertFrontendAttributes(instr->frontend_attributes()),
       DeviceConstraint(instr)));
+#endif  // 0
 
-  return absl::OkStatus();
+  return absl::UnimplementedError("SendThunk unsupported in SYCL");
 }
 
 absl::Status IrEmitterUnnested::EmitSendDoneThunk(
     const HloSendDoneInstruction* instr) {
+#if 0
   if (!instr->channel_id().has_value())
     return absl::InternalError("Unknown send done instruction channel id");
 
@@ -2685,11 +2839,13 @@ absl::Status IrEmitterUnnested::EmitSendDoneThunk(
   AddThunkToThunkSequence(std::make_unique<SendDoneThunk>(
       Thunk::ThunkInfo::WithProfileAnnotation(instr), *instr->channel_id(),
       send_recv_events_, DeviceConstraint(instr)));
+#endif  // 0
 
-  return absl::OkStatus();
+  return absl::UnimplementedError("SendDoneThunk unsupported in SYCL");
 }
 
 absl::Status IrEmitterUnnested::EmitRecvThunk(const HloRecvInstruction* instr) {
+#if 0
   if (!instr->channel_id().has_value())
     return absl::InternalError("Unknown recv instruction channel id");
   TF_RET_CHECK(instr->shape().IsTuple());
@@ -2744,11 +2900,13 @@ absl::Status IrEmitterUnnested::EmitRecvThunk(const HloRecvInstruction* instr) {
       ConvertFrontendAttributes(instr->frontend_attributes()),
       DeviceConstraint(instr)));
 
-  return absl::OkStatus();
+#endif  // 0
+  return absl::UnimplementedError("RecvThunk unsupported in SYCL");
 }
 
 absl::Status IrEmitterUnnested::EmitRecvDoneThunk(
     const HloRecvDoneInstruction* instr) {
+#if 0
   if (!instr->channel_id().has_value())
     return absl::InternalError("Unknown recv done instruction channel id");
 
@@ -2759,8 +2917,9 @@ absl::Status IrEmitterUnnested::EmitRecvDoneThunk(
   AddThunkToThunkSequence(std::make_unique<RecvDoneThunk>(
       Thunk::ThunkInfo::WithProfileAnnotation(instr), *instr->channel_id(),
       send_recv_events_, DeviceConstraint(instr)));
+#endif  // 0
 
-  return absl::OkStatus();
+  return absl::UnimplementedError("RecvDoneThunk unsupported in SYCL");
 }
 
 absl::Status IrEmitterUnnested::EmitHloInstruction(
@@ -2871,47 +3030,67 @@ absl::Status IrEmitterUnnested::EmitHloInstruction(
     case HloOpcode::kCustomCall: {
       auto* custom_call = Cast<HloCustomCallInstruction>(instr);
       if (IsLegacyCublasMatmul(*instr)) {
+#if TENSORFLOW_USE_SYCL
+        const_cast<HloCustomCallInstruction*>(custom_call)
+            ->set_api_version(CustomCallApiVersion::API_VERSION_TYPED_FFI);
+        return EmitCustomCallThunk(custom_call);
+#else
         return EmitGemmThunk(custom_call);
+#endif
       }
-#if GOOGLE_CUDA || TF_HIPBLASLT
+#if GOOGLE_CUDA || TF_HIPBLASLT || TENSORFLOW_USE_SYCL
       if (IsCublasLtMatmul(*instr)) {
+#if TENSORFLOW_USE_SYCL
+        const_cast<HloCustomCallInstruction*>(custom_call)
+            ->set_api_version(CustomCallApiVersion::API_VERSION_TYPED_FFI);
+        return EmitCustomCallThunk(custom_call);
+#else
         return EmitCublasLtMatmulThunk(custom_call);
+#endif
       }
+#endif  // GOOGLE_CUDA || TF_HIPBLASLT || TENSORFLOW_USE_SYCL
+#if GOOGLE_CUDA
       if (IsCublasLtMatmulF8(*instr)) {
         return EmitCublasLtMatmulThunkF8(custom_call);
       }
-#endif  // GOOGLE_CUDA || TF_HIPBLASLT
-#if GOOGLE_CUDA
       if (IsCudnnConvolutionReorder(*instr)) {
         return EmitConvolutionReorderThunk(custom_call);
       }
       if (IsCustomCallToDnnNorm(*instr)) {
         return EmitNormThunk(custom_call);
       }
+#endif  // GOOGLE_CUDA
       if (IsFwdCustomCallTofMHA(*instr)) {
         return EmitFusedMHAThunk(custom_call);
       }
       if (IsBwdCustomCallTofMHA(*instr)) {
         return EmitFusedMHABackwardThunk(custom_call);
       }
-#endif  // GOOGLE_CUDA
       if (IsCustomCallToTopK(*instr)) {
         return EmitTopKCustomCall(custom_call);
       }
       if (IsCustomCallToDnnConvolution(*instr)) {
+#ifdef TENSORFLOW_USE_SYCL
+        const_cast<HloCustomCallInstruction*>(custom_call)
+            ->set_api_version(CustomCallApiVersion::API_VERSION_TYPED_FFI);
+        return EmitCustomCallThunk(custom_call);
+#else
         return EmitConvolutionThunk(custom_call);
+#endif
       }
-#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM
+#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM || TENSORFLOW_USE_SYCL
       if (IsCustomCallToCusolver(*instr)) {
         return EmitCholeskyThunk(instr);
       }
       if (IsTriangularSolve(*instr)) {
         return EmitTriangularSolveCustomCall(instr);
       }
+#if 0
       if (IsCubDeviceRadixSort(*instr)) {
         return EmitCubDeviceRadixSort(custom_call);
       }
-#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM
+#endif
+#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM || TENSORFLOW_USE_SYCL
       if (custom_call->custom_call_target() == "PadToStatic") {
         return EmitPadToStatic(custom_call);
       }
diff --git a/xla/service/gpu/ir_emitter_unnested.h b/xla/service/gpu/ir_emitter_unnested.h
index f91db2db5e..bd05b8ec05 100644
--- a/xla/service/gpu/ir_emitter_unnested.h
+++ b/xla/service/gpu/ir_emitter_unnested.h
@@ -1,4 +1,6 @@
-/* Copyright 2018 The OpenXLA Authors.
+/* Copyright (c) 2024 Intel Corporation
+
+Copyright 2019 The TensorFlow Authors. All Rights Reserved.
 
 Licensed under the Apache License, Version 2.0 (the "License");
 you may not use this file except in compliance with the License.
@@ -137,21 +139,22 @@ class IrEmitterUnnested : public IrEmitter {
   absl::Status EmitConditional(const HloInstruction* instr);
   absl::Status EmitConvolutionThunk(const HloCustomCallInstruction* instr);
   absl::Status EmitGemmThunk(const HloCustomCallInstruction* instr);
-#if GOOGLE_CUDA || TF_HIPBLASLT
+  absl::Status EmitFusedMHAThunk(const HloCustomCallInstruction* instr);
+  absl::Status EmitFusedMHABackwardThunk(const HloCustomCallInstruction* instr);
+#if GOOGLE_CUDA || TF_HIPBLASLT || TENSORFLOW_USE_SYCL
   absl::Status EmitCublasLtMatmulThunk(const HloCustomCallInstruction* instr);
   absl::Status EmitCublasLtMatmulThunkF8(const HloCustomCallInstruction* instr);
-#endif  // GOOGLE_CUDA || TF_HIPBLASLT
+#endif  // GOOGLE_CUDA || TF_HIPBLASLT || TENSORFLOW_USE_SYCL
 #if GOOGLE_CUDA
   absl::Status EmitConvolutionReorderThunk(
       const HloCustomCallInstruction* instr);
   absl::Status EmitNormThunk(const HloCustomCallInstruction* instr);
-  absl::Status EmitFusedMHAThunk(const HloCustomCallInstruction* instr);
-  absl::Status EmitFusedMHABackwardThunk(const HloCustomCallInstruction* instr);
 #endif  // GOOGLE_CUDA
-#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM
+  absl::Status EmitFusedMHAThunk(mlir::Operation* op);
+#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM || TENSORFLOW_USE_SYCL
   absl::Status EmitCubDeviceRadixSort(const HloCustomCallInstruction* instr);
   absl::Status EmitCholeskyThunk(const HloInstruction* instr);
-#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM
+#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM || TENSORFLOW_USE_SYCL
   absl::Status EmitCustomCallThunk(const HloCustomCallInstruction* instr);
   absl::Status EmitFftThunk(const HloFftInstruction* instr);
   absl::Status EmitFusion(const HloFusionInstruction* instr,
@@ -165,9 +168,9 @@ class IrEmitterUnnested : public IrEmitter {
       const HloRngGetAndUpdateStateInstruction* instr);
 
   absl::Status EmitSort(const HloSortInstruction* sort);
-#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM
+#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM || TENSORFLOW_USE_SYCL
   absl::Status EmitTriangularSolveCustomCall(const HloInstruction* instr);
-#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM
+#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM || TENSORFLOW_USE_SYCL
   absl::Status EmitTopKCustomCall(const HloCustomCallInstruction* instr);
   absl::Status EmitTritonCustomCall(const HloCustomCallInstruction* instr);
 
diff --git a/xla/service/gpu/kernels/BUILD b/xla/service/gpu/kernels/BUILD
index 7f800cd132..798f68b174 100644
--- a/xla/service/gpu/kernels/BUILD
+++ b/xla/service/gpu/kernels/BUILD
@@ -147,26 +147,27 @@ cc_library(
         "@tsl//tsl/platform:errors",
         "@tsl//tsl/platform:logging",
         "@tsl//tsl/platform:statusor",
-    ] + if_gpu_is_configured([
-        ":topk_kernel_gpu",
-    ]),
+    ]
+    # ] + if_gpu_is_configured([
+    #     ":topk_kernel_gpu",
+    # ]),
 )
 
-gpu_kernel_library(
-    name = "topk_kernel_gpu",
-    srcs = if_gpu_is_configured([
-        "topk_kernel_bfloat16.cu.cc",
-        "topk_kernel_float.cu.cc",
-        "topk_kernel.cu.h",
-    ]),
-    hdrs = if_gpu_is_configured(["topk_kernel_common.h"]),
-    compatible_with = [],
-    deps = [
-        "//xla:types",
-        "//xla/stream_executor/gpu:gpu_types_header",
-        "@tsl//tsl/lib/math:math_util",
-    ],
-)
+# gpu_kernel_library(
+#     name = "topk_kernel_gpu",
+#     srcs = if_gpu_is_configured([
+#         "topk_kernel_bfloat16.cu.cc",
+#         "topk_kernel_float.cu.cc",
+#         "topk_kernel.cu.h",
+#     ]),
+#     hdrs = if_gpu_is_configured(["topk_kernel_common.h"]),
+#     compatible_with = [],
+#     deps = [
+#         "//xla:types",
+#         "//xla/stream_executor/gpu:gpu_types_header",
+#         "@tsl//tsl/lib/math:math_util",
+#     ],
+# )
 
 xla_cc_test(
     name = "topk_kernel_test",
@@ -212,9 +213,10 @@ cc_library(
         "@com_google_absl//absl/status:statusor",
         "@com_google_absl//absl/strings",
         "@tsl//tsl/platform:statusor",
-    ] + if_gpu_is_configured([
-        ":topk_kernel_gpu",
-    ]),
+    ]
+    # ] + if_gpu_is_configured([
+    #     ":topk_kernel_gpu",
+    # ]),
 )
 
 xla_test(
diff --git a/xla/service/gpu/launch_dimensions.cc b/xla/service/gpu/launch_dimensions.cc
index b31b0e532c..dd106364c5 100644
--- a/xla/service/gpu/launch_dimensions.cc
+++ b/xla/service/gpu/launch_dimensions.cc
@@ -95,6 +95,9 @@ struct BlockSizes {
 BlockSizes GetBlockSizes(LaunchDimensionsConfig dim_config,
                          const se::DeviceDescription& gpu_device_info,
                          const Shape& shape, int64_t num_elements) {
+#if !TENSORFLOW_USE_SYCL
+  // TODO: It will set 128 threads per block by default. We prefer to use
+  // the max value (1024 on PVC). It can benefit instructions like scatter.
   if (!dim_config.row_vectorized && !dim_config.few_waves) {
     BlockSizes result;
     const int kWarpSchedulers = 4;
@@ -105,6 +108,7 @@ BlockSizes GetBlockSizes(LaunchDimensionsConfig dim_config,
         num_elements, result.threads_per_block_x * result.threads_per_block_y);
     return result;
   }
+#endif
 
   int64_t threads_per_block_row_vectorized =
       ThreadsPerBlockRowVectorized(shape, gpu_device_info, dim_config);
diff --git a/xla/service/gpu/llvm_gpu_backend/BUILD b/xla/service/gpu/llvm_gpu_backend/BUILD
index f942228279..efc71a9255 100644
--- a/xla/service/gpu/llvm_gpu_backend/BUILD
+++ b/xla/service/gpu/llvm_gpu_backend/BUILD
@@ -2,6 +2,10 @@ load(
     "@local_config_rocm//rocm:build_defs.bzl",
     "if_rocm_is_configured",
 )
+load(
+    "@local_config_sycl//sycl:build_defs.bzl",
+    "if_sycl_is_configured",
+)
 load("@tsl//tsl:tsl.bzl", "internal_visibility")
 load("//xla:xla.bzl", "xla_cc_test")
 
@@ -37,6 +41,7 @@ cc_library(
         "//xla/service/gpu:metrics",
         "//xla/service/llvm_ir:llvm_command_line_options",
         "//xla/service/llvm_ir:llvm_type_conversion_util",
+        "//xla/service/llvm_ir:llvm_util",
         "//xla/stream_executor:device_description",
         "//xla/tsl/util:env_var",
         "@com_google_absl//absl/base",
@@ -70,6 +75,8 @@ cc_library(
     ] + if_rocm_is_configured([
         "@local_config_rocm//rocm:rocm_headers",
         "@llvm-project//llvm:AMDGPUCodeGen",
+    ]) + if_sycl_is_configured([
+        "@llvm_spir//:llvm_spir_translator",
     ]),
 )
 
diff --git a/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc b/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc
index 6c6a6c20fb..5216830449 100644
--- a/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc
+++ b/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc
@@ -59,6 +59,7 @@ limitations under the License.
 #include "xla/service/gpu/metrics.h"
 #include "xla/service/llvm_ir/llvm_command_line_options.h"
 #include "xla/service/llvm_ir/llvm_type_conversion_util.h"
+#include "xla/service/llvm_ir/llvm_util.h"
 #include "xla/status_macros.h"
 #include "xla/stream_executor/device_description.h"
 #include "xla/tsl/util/env_var.h"
@@ -76,6 +77,9 @@ limitations under the License.
 #include "rocm/rocm_config.h"
 #endif
 
+#include "LLVMSPIRVLib.h"
+#include "LLVMSPIRVOpts.h"
+
 namespace xla {
 namespace gpu {
 namespace {
@@ -156,7 +160,6 @@ std::unique_ptr<llvm::TargetMachine> GetTargetMachine(
                << " -- " << error;
     return nullptr;
   }
-
   llvm::TargetOptions target_options =
       llvm::codegen::InitTargetOptionsFromCodeGenFlags(llvm::Triple());
 
@@ -364,10 +367,13 @@ absl::Status LinkAndOptimizeModule(
   llvm::CGSCCAnalysisManager cgam;
   llvm::ModuleAnalysisManager mam;
 
-  fam.registerPass([&] { return target_machine->getTargetIRAnalysis(); });
+  if (target_machine)
+    fam.registerPass([&] { return target_machine->getTargetIRAnalysis(); });
 
+  // SYCL: customized config
   llvm::PipelineTuningOptions pto;
-  pto.SLPVectorization = true;
+  pto.SLPVectorization = false;
+  pto.LoopVectorization = false;
   pto.InlinerThreshold = inline_threshold;
 
   llvm::PassInstrumentationCallbacks pic;
@@ -1030,5 +1036,107 @@ absl::StatusOr<std::vector<uint8_t>> CompileToHsaco(
 
 }  // namespace amdgpu
 
+namespace {
+std::unique_ptr<llvm::TargetMachine> SPIRGetTargetMachine(
+    llvm::Triple target_triple, se::GpuComputeCapability gpu_version,
+    const DebugOptions& debug_options) {
+  return nullptr;
+}
+
+Status SPIRTargetModuleLinker(llvm::Module* module,
+                              se::GpuComputeCapability gpu_version,
+                              const DebugOptions& debug_options,
+                              const std::string& device_bitcode_dir_path) {
+  return OkStatus();
+}
+
+StatusOr<std::string> EmitModuleToSpir(llvm::Module* module,
+                                       se::GpuComputeCapability gpu_version,
+                                       const DebugOptions& debug_options) {
+  SPIRV::TranslatorOpts::ExtensionsStatusMap ExtensionsStatus;
+  SPIRV::TranslatorOpts opts(SPIRV::VersionNumber::MaximumVersion,
+                             ExtensionsStatus);
+  opts.enableAllExtensions();  // enable all SPIR-V extension first
+
+  std::ostringstream oss;
+  std::string err;
+  bool success = llvm::writeSpirv(module, opts, oss, err);
+  if (!success) {
+    return xla::Internal("Fails to convert LLVM as SPIR-V: %s", err);
+  }
+  return oss.str();
+}
+
+void SPIRBackendInit(const DebugOptions& debug_options) {
+
+  FeedLLVMWithFlags({"-slp-vectorize-hor=false"});
+
+  bool vec = true;
+  tsl::ReadBoolFromEnvVar("VECTORIZE", true, &vec);
+  if (vec) {
+    FeedLLVMWithFlags({
+        "-slp-min-reg-size=64",
+        "-slp-max-reg-size=64",
+    });
+  } else {
+    // TODO: sycl-opt disables all LLVM vectorization passes. Evaluate if it is
+    // needed.
+    FeedLLVMWithFlags({"-sycl-opt=1"});
+  }
+
+  llvm_ir::InitializeLLVMCommandLineOptions(
+      debug_options.xla_backend_extra_options());
+
+  llvm::PassRegistry* registry = llvm::PassRegistry::getPassRegistry();
+  InitializePasses(registry);
+}
+}  // namespace
+
+namespace spir {
+absl::StatusOr<std::vector<uint8_t>> CompileToSpir(
+    llvm::Module* module, se::GpuComputeCapability gpu_version,
+    const DebugOptions& debug_options) {
+  std::string libdevice_dir_path;
+  static absl::once_flag backend_init_flag;
+  absl::call_once(backend_init_flag, SPIRBackendInit, debug_options);
+
+  std::string spir;
+  {
+    XLA_SCOPED_LOGGING_TIMER("Compile module " + module->getName().str());
+
+    // If the module has no functions or globals, there's nothing to compile.
+    if (module->empty() && module->global_empty()) {
+      VLOG(2) << "Module '" << module->getName().str()
+              << "' is empty. Skipping compilation.";
+      return std::vector<uint8_t>();
+    }
+
+    // No SPIR target machine?
+    llvm::Triple default_target_triple("spir64-unknown-unknown");
+    std::unique_ptr<llvm::TargetMachine> target_machine =
+        SPIRGetTargetMachine(default_target_triple, gpu_version, debug_options);
+
+    bool opt = true;
+    tsl::ReadBoolFromEnvVar("SYCL_LLVM_OPT", true, &opt);
+    if (opt) {
+      // Link with libdevice, and optimize the LLVM module.
+      TF_RETURN_IF_ERROR(LinkAndOptimizeModule(
+          module, gpu_version, debug_options, libdevice_dir_path,
+          SPIRTargetModuleLinker, default_target_triple, target_machine.get(),
+          kDefaultInlineThreshold));
+    }
+
+#if 0
+    LOG(ERROR) << "Optimized IR before converting to spir\n" << llvm_ir::DumpToString(module);
+#endif
+
+    // Lower optimized LLVM module to SPIR.
+    TF_ASSIGN_OR_RETURN(spir,
+                        EmitModuleToSpir(module, gpu_version, debug_options));
+  }
+  return std::vector<uint8_t>(spir.begin(), spir.end());
+}
+}  // namespace spir
+
 }  // namespace gpu
 }  // namespace xla
diff --git a/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.h b/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.h
index 38a4687987..b97fff7d9b 100644
--- a/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.h
+++ b/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.h
@@ -69,6 +69,12 @@ absl::StatusOr<std::vector<uint8_t>> CompileToHsaco(
     const std::string& module_config_cache_key);
 }  // namespace amdgpu
 
+namespace spir {
+absl::StatusOr<std::vector<uint8_t>> CompileToSpir(
+    llvm::Module* module, se::GpuComputeCapability gpu_version,
+    const DebugOptions& debug_options);
+}  // namespace spir
+
 }  // namespace gpu
 }  // namespace xla
 
diff --git a/xla/service/gpu/make_batch_pointers.cc b/xla/service/gpu/make_batch_pointers.cc
index e7788d652d..bffdbd4e00 100644
--- a/xla/service/gpu/make_batch_pointers.cc
+++ b/xla/service/gpu/make_batch_pointers.cc
@@ -43,6 +43,7 @@ namespace make_batch_pointers {
 void* kernel();  // returns a pointer to a CUDA C++ device function
 }  // namespace make_batch_pointers
 
+#if !TENSORFLOW_USE_SYCL
 absl::Status MakeBatchPointers(se::Stream* stream,
                                se::DeviceMemoryBase base_ptr,
                                size_t stride_bytes, size_t n,
@@ -72,5 +73,14 @@ absl::Status MakeBatchPointers(se::Stream* stream,
 #endif
   return absl::OkStatus();
 }
+#else
+absl::Status MakeBatchPointers(se::Stream* stream,
+                               const se::DeviceMemoryBase& base_ptr,
+                               size_t stride_bytes, size_t n,
+                               se::DeviceMemoryBase& ptrs_out) {
+  ptrs_out = base_ptr;
+  return absl::OkStatus();
+}
+#endif // !TENSORFLOW_USE_SYCL
 
 }  // namespace xla::gpu
diff --git a/xla/service/gpu/make_batch_pointers.h b/xla/service/gpu/make_batch_pointers.h
index 171f33616d..af452781c2 100644
--- a/xla/service/gpu/make_batch_pointers.h
+++ b/xla/service/gpu/make_batch_pointers.h
@@ -49,11 +49,17 @@ namespace xla::gpu {
 //    driver and slow down *all* work on the GPU.  So to do this right, we'd
 //    need to allocate the host memory as pinned, one alloc per stream.  Then
 //    we'd need to manage this memory without leaks.  This becomes complex!
+#if !TENSORFLOW_USE_SYCL
 absl::Status MakeBatchPointers(se::Stream* stream,
                                se::DeviceMemoryBase base_ptr,
                                size_t stride_bytes, size_t n,
                                se::DeviceMemoryBase ptrs_out);
-
+#else
+absl::Status MakeBatchPointers(se::Stream* stream,
+                               const se::DeviceMemoryBase& base_ptr,
+                               size_t stride_bytes, size_t n,
+                               se::DeviceMemoryBase& ptrs_out);
+#endif
 }  // namespace xla::gpu
 
 #endif  // XLA_SERVICE_GPU_MAKE_BATCH_POINTERS_H_
diff --git a/xla/service/gpu/matmul_utils.h b/xla/service/gpu/matmul_utils.h
index 22d7f17813..5d64dfd606 100644
--- a/xla/service/gpu/matmul_utils.h
+++ b/xla/service/gpu/matmul_utils.h
@@ -40,6 +40,23 @@ limitations under the License.
 #include "rocm/rocm_config.h"
 #endif
 
+namespace stream_executor {
+namespace cuda {
+namespace BlasLt {
+enum class Epilogue {
+  kDefault = 1,                   // No special postprocessing
+  kReLU = 2,                      // Apply point-wise ReLU function
+  kBias = 4,                      // Add broadcasted bias vector
+  kBiasThenReLU = kBias | kReLU,  // Apply bias and then ReLU transform
+  kGELU = 32,                // Apply GELU point-wise transform to the results
+  kGELUWithAux = 32 | 1024,  // Apply GELU with auxiliary output.
+  kBiasThenGELU = kBias | kGELU,  // Apply bias and then approximate GELU.
+  kBiasThenGELUWithAux = kBiasThenGELU | 1024,
+};
+}
+}  // namespace cuda
+}  // namespace stream_executor
+
 namespace xla {
 namespace gpu {
 
diff --git a/xla/service/gpu/model/gpu_performance_model_base.cc b/xla/service/gpu/model/gpu_performance_model_base.cc
index 40bb1ff69b..9d91129962 100644
--- a/xla/service/gpu/model/gpu_performance_model_base.cc
+++ b/xla/service/gpu/model/gpu_performance_model_base.cc
@@ -76,7 +76,11 @@ int GetCoalescingWasteFactor(PrimitiveType element_type) {
 // (1830 MHz) to saturate the memory bandwidth (3.35 TB/s).
 float AdjustBandwidth(const se::DeviceDescription& gpu_device_info,
                       float bandwidth, int64_t num_blocks) {
+#if TENSORFLOW_USE_SYCL
+  float per_block_bandwidth = gpu_device_info.clock_rate_ghz() * 1.0e9f * 64;
+#else
   float per_block_bandwidth = gpu_device_info.clock_rate_ghz() * 1.0e9f * 32;
+#endif
   float max_bandwidth = num_blocks * per_block_bandwidth;
 
   return std::min(bandwidth, max_bandwidth);
@@ -157,7 +161,11 @@ LaunchDimensions GpuPerformanceModelBase::EstimateFusionLaunchDimensions(
       return kernel_emitter->launch_dimensions();
     }
   }
-  int64_t block_size = 128;  // Result for default LaunchDimensionsConfig.
+#if TENSORFLOW_USE_SYCL
+  int64_t block_size = RoundUpTo(device_info.threads_per_block_limit(),int64_t{32});
+#else
+  int64_t block_size = 128; // Result for default LaunchDimensionsConfig.
+#endif
   int64_t num_blocks = CeilOfRatio(estimated_num_threads, block_size);
   return LaunchDimensions(num_blocks, block_size);
 }
diff --git a/xla/service/gpu/model/gpu_performance_model_base.h b/xla/service/gpu/model/gpu_performance_model_base.h
index 7d08a0c68a..8c2a4f4e9d 100644
--- a/xla/service/gpu/model/gpu_performance_model_base.h
+++ b/xla/service/gpu/model/gpu_performance_model_base.h
@@ -122,7 +122,11 @@ class GpuPerformanceModelBase {
   };
 
   // Estimated values in the absence of easy ways to query them.
+#if TENSORFLOW_USE_SYCL
+  static constexpr absl::Duration kKernelLaunchOverhead = absl::Microseconds(5);
+#else
   static constexpr absl::Duration kKernelLaunchOverhead = absl::Microseconds(1);
+#endif
   static constexpr absl::Duration kNcclKernelLaunchOverhead =
       absl::Microseconds(5);
   static constexpr float kL2CacheSpeedup = 2.5;
diff --git a/xla/service/gpu/nccl_clique.cc b/xla/service/gpu/nccl_clique.cc
index 059fc88ff5..bbabeef0d0 100644
--- a/xla/service/gpu/nccl_clique.cc
+++ b/xla/service/gpu/nccl_clique.cc
@@ -73,8 +73,11 @@ absl::StatusOr<const NcclCliqueIdCallback*> GetNcclCliqueIdCallback(
       << "If non-local devices are taking part of a collective API on "
          "GPU, the nccl_clique_id_callback must be provided by the client.";
 
-  static auto* local_callback = new NcclCliqueIdCallback(
-      [](const NcclCliqueKey&) { return NcclApi::Default()->GetUniqueId(); });
+  static auto* local_callback =
+      new NcclCliqueIdCallback([](const NcclCliqueKey& key, const RunId& id) {
+        // return NcclApi::Default()->GetUniqueId();
+        return NcclApi::Default()->GetId(key, id);
+      });
   return local_callback;
 }
 
@@ -252,7 +255,7 @@ static absl::StatusOr<std::shared_ptr<NcclClique::Lock>> InitializeNcclClique(
   // gives access to clique communicators.
   auto initialize = [&](absl::Span<const NcclApi::DeviceRank* const> args)
       -> absl::StatusOr<NcclClique::Lock> {
-    TF_ASSIGN_OR_RETURN(auto clique_id, clique_id_callback(clique_key));
+    TF_ASSIGN_OR_RETURN(auto clique_id, clique_id_callback(clique_key, run_id));
 
     std::vector<NcclApi::DeviceRank> ranks;
     ranks.reserve(args.size());
diff --git a/xla/service/gpu/nccl_clique.h b/xla/service/gpu/nccl_clique.h
index 44c7234378..7cf962b50c 100644
--- a/xla/service/gpu/nccl_clique.h
+++ b/xla/service/gpu/nccl_clique.h
@@ -31,6 +31,7 @@ limitations under the License.
 #include "xla/executable_run_options.h"
 #include "xla/service/gpu/nccl_clique_key.h"
 #include "xla/service/gpu/runtime/nccl_api.h"
+#include "xla/service/gpu/runtime/ccl_api.h"
 #include "xla/service/lockable.h"
 #include "xla/stream_executor/stream_executor.h"
 
diff --git a/xla/service/gpu/nccl_clique_key.h b/xla/service/gpu/nccl_clique_key.h
index dbd7ba1200..8c123126f6 100644
--- a/xla/service/gpu/nccl_clique_key.h
+++ b/xla/service/gpu/nccl_clique_key.h
@@ -26,6 +26,7 @@ limitations under the License.
 
 #include "absl/status/statusor.h"
 #include "absl/types/span.h"
+#include "xla/executable_run_options.h"
 #include "xla/service/global_device_id.h"
 
 namespace xla::gpu {
@@ -155,7 +156,8 @@ H AbslHashValue(H h, const NcclCliqueId& id) {
 
 // A callback to get a unique clique id (see `ncclUniqueId` documentation).
 using NcclCliqueIdCallback =  // NOLINT
-    std::function<absl::StatusOr<NcclCliqueId>(const NcclCliqueKey&)>;
+    std::function<absl::StatusOr<NcclCliqueId>(const NcclCliqueKey&,
+                                               const RunId&)>;
 
 }  // namespace xla::gpu
 
diff --git a/xla/service/gpu/reduction_utils.h b/xla/service/gpu/reduction_utils.h
index 8245b34c3d..319d1256e3 100644
--- a/xla/service/gpu/reduction_utils.h
+++ b/xla/service/gpu/reduction_utils.h
@@ -42,6 +42,8 @@ struct ReductionDimensions {
   constexpr static int kColReducedDimension = 1;
   constexpr static int kColMinorKeptDimension = 2;
 
+  constexpr static int kVectorizedDimension = 3;
+
   // Indicates whether the reduction is a row reduction or a column reduction.
   bool is_row_reduction;
 
diff --git a/xla/service/gpu/runtime/BUILD b/xla/service/gpu/runtime/BUILD
index 4efa84b600..09597b8e16 100644
--- a/xla/service/gpu/runtime/BUILD
+++ b/xla/service/gpu/runtime/BUILD
@@ -193,62 +193,86 @@ xla_test(
 # have `if_nccl` and `if_gpu_configured` that do not compose. NCCL header included directly in
 # :nccl_api target and all other targets should use this header to launch collective operations.
 # This allows to minimize the spreading of #ifdef all over the XLA code base.
-alias(
-    name = "nccl_api",
-    actual = if_nccl(":_nccl_api_impl", ":_nccl_api_stub"),
-)
+# alias(
+#     name = "nccl_api",
+#     actual = if_nccl(":_nccl_api_impl", ":_nccl_api_stub"),
+# )
 
-cc_library(
-    name = "_nccl_api_impl",
-    srcs = if_gpu_is_configured(
-        ["nccl_api.cc"],
-        ["nccl_api_stub.cc"],
-    ),
-    hdrs = ["nccl_api.h"],
-    compatible_with = get_compatible_with_portable(),
-    deps = [
-        "//xla:shape_util",
-        "//xla:xla_data_proto_cc",
-        "//xla/service:collective_ops_utils",
-        "//xla/service/gpu:nccl_clique_key",
-        "//xla/stream_executor",
-        "//xla/stream_executor/gpu:gpu_activation",
-        "@com_google_absl//absl/algorithm:container",
-        "@com_google_absl//absl/container:btree",
-        "@com_google_absl//absl/hash",
-        "@com_google_absl//absl/status",
-        "@com_google_absl//absl/status:statusor",
-        "@com_google_absl//absl/strings",
-        "@com_google_absl//absl/strings:str_format",
-        "@com_google_absl//absl/types:span",
-        "@tsl//tsl/concurrency:ref_count",
-        "@tsl//tsl/platform:errors",
-        "@tsl//tsl/platform:logging",
-        "@tsl//tsl/platform:statusor",
-    ] + if_cuda_is_configured([
-        "@local_config_nccl//:nccl",
-        "//xla/stream_executor/cuda:cuda_driver",
-        "//xla/stream_executor/cuda:cuda_executor",
-    ]) + if_rocm_is_configured([
-        "@local_config_rocm//rocm:rccl",
-        "//xla/stream_executor/rocm:rocm_driver",
-        "//xla/stream_executor/rocm:rocm_executor",
-    ]) + if_gpu_is_configured([
-        "//xla/stream_executor/gpu:gpu_stream",
-    ]),
-)
+# cc_library(
+#     name = "_nccl_api_impl",
+#     srcs = if_gpu_is_configured(
+#         ["nccl_api.cc"],
+#         ["nccl_api_stub.cc"],
+#     ),
+#     hdrs = ["nccl_api.h"],
+#     compatible_with = get_compatible_with_portable(),
+#     deps = [
+#         "//xla:shape_util",
+#         "//xla:xla_data_proto_cc",
+#         "//xla/service:collective_ops_utils",
+#         "//xla/service/gpu:nccl_clique_key",
+#         "//xla/stream_executor",
+#         "//xla/stream_executor/gpu:gpu_activation",
+#         "@com_google_absl//absl/algorithm:container",
+#         "@com_google_absl//absl/container:btree",
+#         "@com_google_absl//absl/hash",
+#         "@com_google_absl//absl/status",
+#         "@com_google_absl//absl/status:statusor",
+#         "@com_google_absl//absl/strings",
+#         "@com_google_absl//absl/strings:str_format",
+#         "@com_google_absl//absl/types:span",
+#         "@tsl//tsl/concurrency:ref_count",
+#         "@tsl//tsl/platform:errors",
+#         "@tsl//tsl/platform:logging",
+#         "@tsl//tsl/platform:statusor",
+#     ] + if_cuda_is_configured([
+#         "@local_config_nccl//:nccl",
+#         "//xla/stream_executor/cuda:cuda_driver",
+#         "//xla/stream_executor/cuda:cuda_executor",
+#     ]) + if_rocm_is_configured([
+#         "@local_config_rocm//rocm:rccl",
+#         "//xla/stream_executor/rocm:rocm_driver",
+#         "//xla/stream_executor/rocm:rocm_executor",
+#     ]) + if_gpu_is_configured([
+#         "//xla/stream_executor/gpu:gpu_stream",
+#     ]),
+# )
+
+# cc_library(
+#     name = "_nccl_api_stub",
+#     srcs = ["nccl_api_stub.cc"],
+#     hdrs = ["nccl_api.h"],
+#     compatible_with = get_compatible_with_portable(),
+#     deps = [
+#         "//xla:shape_util",
+#         "//xla:xla_data_proto_cc",
+#         "//xla/service:collective_ops_utils",
+#         "//xla/service/gpu:nccl_clique_key",
+#         "//xla/stream_executor",
+#         "@com_google_absl//absl/status",
+#         "@com_google_absl//absl/status:statusor",
+#         "@com_google_absl//absl/types:span",
+#         "@tsl//tsl/concurrency:ref_count",
+#         "@tsl//tsl/platform:logging",
+#     ],
+# )
 
 cc_library(
-    name = "_nccl_api_stub",
-    srcs = ["nccl_api_stub.cc"],
-    hdrs = ["nccl_api.h"],
+    name = "nccl_api",
+    srcs = ["ccl_api.cc"],
+    hdrs = [
+        "nccl_api.h",
+        "ccl_api.h",
+    ],
     compatible_with = get_compatible_with_portable(),
     deps = [
+        "//xla/service/gpu:nccl_clique_key",
         "//xla:shape_util",
         "//xla:xla_data_proto_cc",
         "//xla/service:collective_ops_utils",
-        "//xla/service/gpu:nccl_clique_key",
         "//xla/stream_executor",
+        "//xla/stream_executor/gpu:gpu_stream",
+        "@intel_extension_for_openxla//xla/service/gpu:ccl_ops",
         "@com_google_absl//absl/status",
         "@com_google_absl//absl/status:statusor",
         "@com_google_absl//absl/types:span",
@@ -593,6 +617,7 @@ cc_library(
         "//xla/stream_executor",
         "@com_google_absl//absl/base:core_headers",
         "@com_google_absl//absl/container:flat_hash_map",
+        "@intel_extension_for_openxla//xla/service/gpu:xetla_gpu_fused_mha_runner",
         "@com_google_absl//absl/status",
         "@com_google_absl//absl/synchronization",
     ],
diff --git a/xla/service/gpu/runtime/ccl_api.cc b/xla/service/gpu/runtime/ccl_api.cc
new file mode 100644
index 0000000000..1f48e37e55
--- /dev/null
+++ b/xla/service/gpu/runtime/ccl_api.cc
@@ -0,0 +1,318 @@
+/* Copyright (c) 2024 Intel Corporation
+
+Copyright 2019 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "xla/service/gpu/runtime/ccl_api.h"
+
+#include <cstddef>
+#include <cstdint>
+
+#include "absl/status/status.h"
+#include "absl/status/statusor.h"
+#include "tsl/concurrency/ref_count.h"
+#include "xla/service/collective_ops_utils.h"
+#include "xla/service/gpu/ccl_ops.h"
+#include "xla/service/gpu/runtime/nccl_api.h"
+#include "xla/service/gpu/nccl_clique_key.h"
+#include "xla/stream_executor/device_memory.h"
+#include "xla/stream_executor/gpu/gpu_stream.h"
+#include "xla/stream_executor/stream.h"
+
+namespace xla::gpu {
+//==-----------------------------------------------------------------------===//
+// NcclApi::PersistentPlanAllocator
+//==-----------------------------------------------------------------------===//
+
+using PersistentPlanAllocator = NcclApi::PersistentPlanAllocator;
+using ScopedPersistentPlanAllocator = NcclApi::ScopedPersistentPlanAllocator;
+
+PersistentPlanAllocator::PersistentPlanAllocator(int64_t,
+                                                 se::DeviceMemoryAllocator*,
+                                                 se::Stream*) {
+  // Suppress clang unused private field warnings.
+  (void)device_ordinal_;
+  (void)allocator_;
+  (void)stream_;
+}
+
+PersistentPlanAllocator::~PersistentPlanAllocator() = default;
+
+absl::StatusOr<se::DeviceMemoryBase>
+PersistentPlanAllocator::AllocateAndInitialize(void*, size_t) {
+  return absl::UnimplementedError("XLA compiled without NCCL support");
+}
+
+absl::Status PersistentPlanAllocator::Deallocate(se::DeviceMemoryBase mem) {
+  return absl::UnimplementedError("XLA compiled without NCCL support");
+}
+
+ScopedPersistentPlanAllocator::ScopedPersistentPlanAllocator(
+    NcclCommHandle, tsl::RCReference<PersistentPlanAllocator>) {
+  // Suppress clang unused private field warnings.
+  (void)comm_;
+  (void)recover_;
+  (void)allocator_;
+}
+
+ScopedPersistentPlanAllocator::~ScopedPersistentPlanAllocator() = default;
+
+//===----------------------------------------------------------------------===//
+// CclApi
+//===----------------------------------------------------------------------===//
+
+static absl::Status UnimplementedError(std::string mes = "") {
+  return absl::UnimplementedError("XLA compiled without CCL support: " + mes);
+}
+
+CclApi::CclApi() {}
+
+absl::StatusOr<NcclCliqueId> CclApi::GetUniqueId() { return NcclCliqueId(); }
+
+absl::StatusOr<NcclCliqueId> CclApi::GetId(const NcclCliqueKey& key,
+                                           const RunId& id) {
+  std::string new_id =
+      id.ToString() + "=" + GlobalDeviceIdsToString(key.devices());
+  TF_RET_CHECK(new_id.size() < NcclCliqueId::kSize)
+      << "Run ID length must < kSize(" << NcclCliqueId::kSize << ").";
+  new_id.resize(NcclCliqueId::kSize);
+
+  return NcclCliqueId().FromString(new_id);
+}
+
+absl::StatusOr<std::vector<CclApi::OwnedNcclComm>> CclApi::CommInitRanks(
+    int32_t nranks, const NcclCliqueId& clique_id,
+    absl::Span<const DeviceRank> ranks, const Config& config){
+  VLOG(1) << "Initialize NCCL communicator for " << ranks.size()
+          << " devices; hash(id)=" << absl::HashOf(clique_id);
+
+  std::vector<OwnedNcclComm> comms;
+  comms.reserve(ranks.size());
+
+  for (size_t i = 0; i < ranks.size(); ++i) {
+    VLOG(1) << "Initialize NCCL communicator for rank #" << ranks[i].rank
+            << " of " << nranks << "; hash(id)=" << absl::HashOf(clique_id);
+
+  NcclCommHandle comm = reinterpret_cast<NcclCommHandle>(
+      new ccl::communicator(nranks, ranks[i].rank, clique_id.ToString()));
+
+  comms.emplace_back(comm, NcclCommDeleter{this});
+}
+
+  return comms;
+}
+
+absl::StatusOr<std::vector<CclApi::OwnedNcclComm>> CclApi::CommSplit(
+    absl::Span<const NcclCommHandle> comms, int32_t color,
+    absl::Span<const int32_t> keys, std::optional<Config> config){
+  // Don't need now
+  return UnimplementedError("CommSplit");
+}
+
+absl::Status CclApi::CommAbort(NcclCommHandle) {
+  // Don't need now
+  return UnimplementedError("CommAbort");
+}
+
+absl::Status CclApi::CommFinalize(NcclCommHandle) {
+  // Don't need now
+  return UnimplementedError("CommFinalize");
+}
+
+absl::Status CclApi::CommDestroy(NcclCommHandle comm) {
+  delete reinterpret_cast<ncclComm_t>(comm);
+  return absl::OkStatus();
+}
+
+absl::StatusOr<int32_t> CclApi::CommCount(NcclCommHandle) {
+  // Don't need now
+  return UnimplementedError("CommCount");
+}
+
+absl::Status CclApi::CommGetAsyncError(NcclCommHandle comm) {
+  return absl::OkStatus();
+}
+
+absl::Status CclApi::GroupStart() {
+  // Don't need now
+  return UnimplementedError("GroupStart");
+}
+
+absl::Status CclApi::GroupEnd() {
+  // Don't need now
+  return UnimplementedError("GroupEnd");
+}
+
+absl::Status CclApi::AllReduce(se::DeviceMemoryBase send_buffer,
+                               se::DeviceMemoryBase recv_buffer,
+                               PrimitiveType dtype, size_t count,
+                               ReductionKind reduction_kind,
+                               NcclCommHandle comm, se::Stream* stream) {
+  se::gpu::GpuStreamHandle gpu_stream = se::gpu::AsGpuStreamValue(stream);
+
+  const void* send_buffer_ = send_buffer.opaque();
+  void* recv_buffer_ = recv_buffer.opaque();
+  size_t element_count = count * (primitive_util::IsComplexType(dtype) ? 2 : 1);
+
+  auto comm_ = reinterpret_cast<ncclComm_t>(comm);
+
+  sycl_allreduce(send_buffer_, recv_buffer_, element_count, dtype,
+                 reduction_kind, gpu_stream, comm_);
+  return absl::OkStatus();
+}
+
+absl::Status CclApi::Broadcast(se::DeviceMemoryBase send_buffer,
+                               se::DeviceMemoryBase recv_buffer,
+                               PrimitiveType dtype, size_t count,
+                               size_t root, NcclCommHandle comm,
+                               se::Stream* stream){
+  se::gpu::GpuStreamHandle gpu_stream = se::gpu::AsGpuStreamValue(stream);
+
+  const void* send_buffer_ = send_buffer.opaque();
+  void* recv_buffer_ = recv_buffer.opaque();
+  size_t element_count = count * (primitive_util::IsComplexType(dtype) ? 2 : 1);
+
+  auto comm_ = reinterpret_cast<ncclComm_t>(comm);
+
+  sycl_broadcast(send_buffer_, recv_buffer_, element_count, dtype,
+                 root, gpu_stream, comm_);
+
+  return absl::OkStatus();
+}
+
+absl::Status CclApi::ReduceScatter(se::DeviceMemoryBase send_buffer,
+                                   se::DeviceMemoryBase recv_buffer,
+                                   PrimitiveType dtype, size_t count,
+                                   ReductionKind reduction_kind,
+                                   NcclCommHandle comm, se::Stream* stream) {
+  se::gpu::GpuStreamHandle gpu_stream = se::gpu::AsGpuStreamValue(stream);
+
+  const void* send_buffer_ = send_buffer.opaque();
+  void* recv_buffer_ = recv_buffer.opaque();
+  size_t element_count = count * (primitive_util::IsComplexType(dtype) ? 2 : 1);
+
+  auto comm_ = reinterpret_cast<ncclComm_t>(comm);
+
+  int num_participants = comm_->nranks;
+  TF_RET_CHECK(element_count % num_participants == 0)
+      << "Source buffer was not an exact multiple of the number of "
+         "participants.";
+  int64_t recv_count = element_count / num_participants;
+  VLOG(3) << absl::StreamFormat(
+      "Calling ncclReduceScatter(send_buffer=%p, recv_buffer=%p, "
+      "recvcount=%d, "
+      "comm=%p, stream=%p)",
+      send_buffer_, recv_buffer_, recv_count, static_cast<const void*>(comm_),
+      gpu_stream);
+
+  sycl_reduce_scatter(send_buffer_, recv_buffer_, recv_count, dtype,
+                      reduction_kind, gpu_stream, comm_);
+  return absl::OkStatus();
+}
+
+absl::Status CclApi::AllGather(se::DeviceMemoryBase send_buffer,
+                               se::DeviceMemoryBase recv_buffer,
+                               PrimitiveType dtype, size_t count,
+                               NcclCommHandle comm, se::Stream* stream) {
+  se::gpu::GpuStreamHandle gpu_stream = se::gpu::AsGpuStreamValue(stream);
+
+  const void* send_buffer_ = send_buffer.opaque();
+  void* recv_buffer_ = recv_buffer.opaque();
+  size_t element_count = count * (primitive_util::IsComplexType(dtype) ? 2 : 1);
+
+  auto comm_ = reinterpret_cast<ncclComm_t>(comm);
+
+  sycl_allgather(send_buffer_, recv_buffer_, element_count, dtype, gpu_stream,
+                 comm_);
+  return absl::OkStatus();
+}
+
+absl::Status CclApi::AllToAll(bool has_split_dimension,
+                              std::vector<const void*>& send_buffers,
+                              std::vector<void*>& recv_buffers,
+                              size_t element_count, PrimitiveType element_type,
+                              NcclCommHandle comm, se::Stream* stream) {
+  auto comm_ = reinterpret_cast<ncclComm_t>(comm);
+
+  se::gpu::GpuStreamHandle gpu_stream = se::gpu::AsGpuStreamValue(stream);
+
+  element_count = element_count * (primitive_util::IsComplexType(element_type) ? 2 : 1);
+
+  if (has_split_dimension) {
+    sycl_alltoall_split(send_buffers, recv_buffers, element_count, element_type,
+                        gpu_stream, comm_);
+  } else {
+    sycl_alltoall(send_buffers, recv_buffers, element_count, element_type,
+                  gpu_stream, comm_);
+  }
+
+  return absl::OkStatus();
+}
+
+absl::Status CclApi::CollectivePermute(se::DeviceMemoryBase src_addr,
+                                       se::DeviceMemoryBase dest_addr,
+                                       size_t element_count,
+                                       PrimitiveType element_type,
+                                       const std::optional<int64_t> source_id,
+                                       const std::optional<int64_t> target_id,
+                                       NcclCommHandle comm,
+                                       se::Stream* stream) {
+  auto comm_ = reinterpret_cast<ncclComm_t>(comm);
+
+  se::gpu::GpuStreamHandle gpu_stream = se::gpu::AsGpuStreamValue(stream);
+
+  element_count = element_count * (primitive_util::IsComplexType(element_type) ? 2 : 1);
+
+  sycl_collective_permute(src_addr.opaque(), dest_addr.opaque(), element_count,
+                          element_type, source_id, target_id, gpu_stream,
+                          comm_);
+
+  return absl::OkStatus();
+}
+
+absl::Status CclApi::Send(se::DeviceMemoryBase, PrimitiveType, size_t, int32_t,
+                          NcclCommHandle, se::Stream*) {
+  // Don't need now
+  return UnimplementedError("Send");
+}
+
+absl::Status CclApi::Recv(se::DeviceMemoryBase, PrimitiveType, size_t, int32_t,
+                          NcclCommHandle, se::Stream*) {
+  // Don't need now
+  return UnimplementedError("Recv");
+}
+
+absl::StatusOr<CclApi::NcclRegisteredBufferHandle> CclApi::RegisterBuffer(
+    NcclCommHandle, se::DeviceMemoryBase) {
+  // Don't need now
+  return UnimplementedError("RegisterBuffer");
+}
+
+absl::StatusOr<CclApi::NcclRegisteredBufferHandle> CclApi::DeregisterBuffer(
+    NcclCommHandle, CclApi::NcclRegisteredBufferHandle) {
+  // Don't need now
+  return UnimplementedError("DeregisterBuffer");
+}
+
+ncclComm_t CastCCLComm(CclApi::NcclCommHandle comm) {
+  return reinterpret_cast<ncclComm_t>(comm);
+}
+
+NcclApi* NcclApi::Default() {
+  static auto* ccl_api = new CclApi();
+  return ccl_api;
+}
+
+}  // namespace xla::gpu
diff --git a/xla/service/gpu/runtime/ccl_api.h b/xla/service/gpu/runtime/ccl_api.h
new file mode 100644
index 0000000000..3acaac4067
--- /dev/null
+++ b/xla/service/gpu/runtime/ccl_api.h
@@ -0,0 +1,120 @@
+/* Copyright (c) 2024 Intel Corporation
+
+Copyright 2019 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+#ifndef XLA_SERVICE_GPU_RUNTIME_CCL_API_H_
+#define XLA_SERVICE_GPU_RUNTIME_CCL_API_H_
+
+#include <cstddef>
+#include <cstdint>
+
+#include "absl/status/status.h"
+#include "absl/status/statusor.h"
+#include "tsl/concurrency/ref_count.h"
+#include "xla/service/collective_ops_utils.h"
+#include "xla/service/gpu/ccl_ops.h"
+#include "xla/service/gpu/runtime/nccl_api.h"
+#include "xla/service/gpu/nccl_clique_key.h"
+#include "xla/stream_executor/device_memory.h"
+#include "xla/stream_executor/gpu/gpu_stream.h"
+#include "xla/stream_executor/stream.h"
+
+namespace xla::gpu {
+//===----------------------------------------------------------------------===//
+// CclApi
+//===----------------------------------------------------------------------===//
+
+class CclApi final : public NcclApi {
+ public:
+  CclApi();
+
+  absl::StatusOr<NcclCliqueId> GetUniqueId() final;
+
+  absl::StatusOr<NcclCliqueId> GetId(const NcclCliqueKey& key,
+                                     const RunId& id) final;
+
+//   absl::StatusOr<OwnedNcclComm> CommInitRank(int32_t nranks,
+//                                              const NcclCliqueId& clique_id,
+//                                              int32_t rank);
+
+  absl::StatusOr<std::vector<OwnedNcclComm>> CommInitRanks(
+      int32_t nranks, const NcclCliqueId& clique_id,
+      absl::Span<const DeviceRank> ranks, const Config& config) final;
+
+  absl::StatusOr<std::vector<OwnedNcclComm>> CommSplit(
+      absl::Span<const NcclCommHandle> comms, int32_t color,
+      absl::Span<const int32_t> keys, std::optional<Config> config) final;
+
+  absl::Status CommAbort(NcclCommHandle) final;
+  absl::Status CommFinalize(NcclCommHandle) final;
+  absl::Status CommDestroy(NcclCommHandle comm) final;
+  absl::StatusOr<int32_t> CommCount(NcclCommHandle) final;
+  absl::Status CommGetAsyncError(NcclCommHandle comm) final;
+
+  absl::Status GroupStart() final;
+
+  absl::Status GroupEnd() final;
+
+  absl::Status AllReduce(se::DeviceMemoryBase send_buffer,
+                         se::DeviceMemoryBase recv_buffer, PrimitiveType dtype,
+                         size_t count, ReductionKind reduction_kind,
+                         NcclCommHandle comm, se::Stream* stream) final;
+
+  absl::Status Broadcast(se::DeviceMemoryBase send_buffer,
+                         se::DeviceMemoryBase recv_buffer,
+                         PrimitiveType dtype, size_t count,
+                         size_t root, NcclCommHandle comm,
+                         se::Stream* stream) final;
+
+  absl::Status ReduceScatter(se::DeviceMemoryBase send_buffer,
+                             se::DeviceMemoryBase recv_buffer,
+                             PrimitiveType dtype, size_t count,
+                             ReductionKind reduction_kind, NcclCommHandle comm,
+                             se::Stream* stream) final;
+
+  absl::Status AllGather(se::DeviceMemoryBase send_buffer,
+                         se::DeviceMemoryBase recv_buffer, PrimitiveType dtype,
+                         size_t count, NcclCommHandle comm,
+                         se::Stream* stream) final;
+
+  absl::Status AllToAll(bool has_split_dimension,
+                        std::vector<const void*>& send_buffers,
+                        std::vector<void*>& recv_buffers, size_t element_count,
+                        PrimitiveType element_type, NcclCommHandle comm,
+                        se::Stream* stream);
+
+  absl::Status CollectivePermute(se::DeviceMemoryBase src_addr,
+                                 se::DeviceMemoryBase dest_addr,
+                                 size_t element_count, PrimitiveType element_type,
+                                 const std::optional<int64_t> source_id,
+                                 const std::optional<int64_t> target_id,
+                                 NcclCommHandle comm, se::Stream* stream);
+
+  absl::Status Send(se::DeviceMemoryBase, PrimitiveType, size_t, int32_t,
+                    NcclCommHandle, se::Stream*) final;
+  absl::Status Recv(se::DeviceMemoryBase, PrimitiveType, size_t, int32_t,
+                    NcclCommHandle, se::Stream*) final;
+
+  absl::StatusOr<NcclRegisteredBufferHandle> RegisterBuffer(
+      NcclCommHandle, se::DeviceMemoryBase) final;
+
+  absl::StatusOr<NcclRegisteredBufferHandle> DeregisterBuffer(
+      NcclCommHandle, NcclRegisteredBufferHandle) final;
+};
+
+ncclComm_t CastCCLComm(CclApi::NcclCommHandle);
+
+}  // namespace xla::gpu
+#endif  // XLA_SERVICE_GPU_RUNTIME_CCL_API_H_
diff --git a/xla/service/gpu/runtime/cholesky_thunk.cc b/xla/service/gpu/runtime/cholesky_thunk.cc
index b91be4449d..0b5beedf4f 100644
--- a/xla/service/gpu/runtime/cholesky_thunk.cc
+++ b/xla/service/gpu/runtime/cholesky_thunk.cc
@@ -54,6 +54,7 @@ absl::Status DoPotrfBatched(const se::GpuAsmOpts& asm_opts,
   se::DeviceMemory<T*> as(params->workspace_buffer);
 #endif
 
+#if !TENSORFLOW_USE_SYCL
   CHECK_GE(as.size(), params->batch_size);
   CHECK_GE(infos.size(), params->batch_size);
 
@@ -66,6 +67,10 @@ absl::Status DoPotrfBatched(const se::GpuAsmOpts& asm_opts,
   // Now that we've set up the `as` array, we can call cusolver.
   return context.PotrfBatched(params->uplo, params->n, as, params->n, infos,
                               params->batch_size);
+#else // !TENSORFLOW_USE_SYCL
+  return context.PotrfBatched(params->uplo, params->n, as, params->n, infos,
+                              params->batch_size, a_base);
+#endif // !TENSORFLOW_USE_SYCL
 }
 
 template <typename T>
diff --git a/xla/service/gpu/runtime/convolution_thunk.cc b/xla/service/gpu/runtime/convolution_thunk.cc
index 6e8158d866..40ddbf86e0 100644
--- a/xla/service/gpu/runtime/convolution_thunk.cc
+++ b/xla/service/gpu/runtime/convolution_thunk.cc
@@ -71,6 +71,9 @@ absl::Status ConvolutionThunk::ExecuteOnStream(const ExecuteParams& params) {
   se::DeviceMemoryBase scratch =
       buffer_allocations.GetDeviceAddress(scratch_buffer_);
 
+#if TENSORFLOW_USE_SYCL
+  return absl::InternalError("After SYCL support FFI convolution, EmitConvolutionThunk should not be use.");
+#else
   bool runner_created = false;
   RunConvOptions opts;
   opts.runner_cache = &GetOrCreateRunner(params.stream, &runner_created);
@@ -106,7 +109,7 @@ absl::Status ConvolutionThunk::ExecuteOnStream(const ExecuteParams& params) {
   TF_RETURN_IF_ERROR(RunGpuConv(config_, absl::MakeSpan(operand_se_buffers),
                                 absl::MakeSpan(result_se_buffers), scratch,
                                 params.stream, opts));
-
+#endif
   // Note: Convolution has a tuple buffer as an output, but we don't need to
   // populate it as no one should be reading from the tuple directly.
   if (!params.stream->ok()) {
diff --git a/xla/service/gpu/runtime/custom_call_thunk.cc b/xla/service/gpu/runtime/custom_call_thunk.cc
index 0eaf0aaf4c..92ed9c7f69 100644
--- a/xla/service/gpu/runtime/custom_call_thunk.cc
+++ b/xla/service/gpu/runtime/custom_call_thunk.cc
@@ -45,7 +45,7 @@ limitations under the License.
 #include "xla/util.h"
 #include "tsl/platform/errors.h"
 
-#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM
+#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM || TENSORFLOW_USE_SYCL
 #include "xla/stream_executor/gpu/gpu_stream.h"
 #endif
 
@@ -98,7 +98,7 @@ absl::Status CustomCallThunk::ExecuteCustomCall(const ExecuteParams& params) {
     }
   }
 
-#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM
+#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM || TENSORFLOW_USE_SYCL
   auto gpu_stream = se::gpu::AsGpuStreamValue(params.stream);
   XlaCustomCallStatus custom_call_status;
   call_target_(gpu_stream, buffers.data(), opaque_.data(), opaque_.size(),
@@ -109,11 +109,11 @@ absl::Status CustomCallThunk::ExecuteCustomCall(const ExecuteParams& params) {
   } else {
     return absl::OkStatus();
   }
-#else   //  GOOGLE_CUDA || TENSORFLOW_USE_ROCM
+#else   //  GOOGLE_CUDA || TENSORFLOW_USE_ROCM || TENSORFLOW_USE_SYCL
   return Unavailable(
       "Custom calls on GPU are not supported in this configuration. Please "
       "build with --config=cuda or --config=rocm");
-#endif  //   GOOGLE_CUDA || TENSORFLOW_USE_ROCM
+#endif  //   GOOGLE_CUDA || TENSORFLOW_USE_ROCM || TENSORFLOW_USE_SYCL
 }
 
 absl::Status CustomCallThunk::ExecuteFfiHandler(const ExecuteParams& params) {
diff --git a/xla/service/gpu/runtime/custom_call_thunk.h b/xla/service/gpu/runtime/custom_call_thunk.h
index 02679d2e0d..1bcb07264c 100644
--- a/xla/service/gpu/runtime/custom_call_thunk.h
+++ b/xla/service/gpu/runtime/custom_call_thunk.h
@@ -32,7 +32,7 @@ limitations under the License.
 #include "xla/shape.h"
 #include "xla/status.h"
 
-#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM
+#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM || TENSORFLOW_USE_SYCL
 #include "xla/stream_executor/gpu/gpu_types.h"
 #endif
 
@@ -52,11 +52,11 @@ namespace gpu {
 // compiler is allowed to create.
 class CustomCallThunk : public Thunk {
  public:
-#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM
+#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM || TENSORFLOW_USE_SYCL
   using Stream = stream_executor::gpu::GpuStreamHandle;
-#else   //  GOOGLE_CUDA || TENSORFLOW_USE_ROCM
+#else   //  GOOGLE_CUDA || TENSORFLOW_USE_ROCM || TENSORFLOW_USE_SYCL
   using Stream = void*;
-#endif  //  GOOGLE_CUDA || TENSORFLOW_USE_ROCM
+#endif  //  GOOGLE_CUDA || TENSORFLOW_USE_ROCM || TENSORFLOW_USE_SYCL
 
   using CustomCallTarget = std::function<void(Stream, void**, const char*,
                                               size_t, XlaCustomCallStatus*)>;
diff --git a/xla/service/gpu/runtime/fused_mha_thunk.cc b/xla/service/gpu/runtime/fused_mha_thunk.cc
index b39ba7a373..0c3f7bab56 100644
--- a/xla/service/gpu/runtime/fused_mha_thunk.cc
+++ b/xla/service/gpu/runtime/fused_mha_thunk.cc
@@ -21,6 +21,10 @@ limitations under the License.
 #include "xla/stream_executor/stream_executor.h"
 #include "xla/util.h"
 
+#if TENSORFLOW_USE_SYCL
+#include "xla/service/gpu/xetla_gpu_fused_mha_runner.h"
+#endif
+
 namespace xla {
 namespace gpu {
 
@@ -85,6 +89,13 @@ absl::Status FusedMHAThunk::ExecuteOnStream(const ExecuteParams& params) {
       AssignBufferIfNotNull(buffer_allocations, bias_buffer_);
   std::optional<se::DeviceMemoryBase> activation_buffer =
       AssignBufferIfNotNull(buffer_allocations, activation_buffer_);
+
+#if TENSORFLOW_USE_SYCL
+  TF_RETURN_IF_ERROR(RunXetlaGpuFMHA(config_, lhs_bmm1_buffer, rhs_bmm1_buffer,
+                                     rhs_bmm2_buffer, output_buffer,
+                                     scratch_buffer, mask_buffer, bias_buffer,
+                                     activation_buffer, params.stream));
+#else 
   std::optional<se::DeviceMemoryBase> seqlen_q_buffer =
       AssignBufferIfNotNull(buffer_allocations, seqlen_q_buffer_);
   std::optional<se::DeviceMemoryBase> seqlen_k_buffer =
@@ -95,7 +106,7 @@ absl::Status FusedMHAThunk::ExecuteOnStream(const ExecuteParams& params) {
       config_, lhs_bmm1_buffer, rhs_bmm1_buffer, rhs_bmm2_buffer, output_buffer,
       scratch_buffer, mask_buffer, bias_buffer, activation_buffer,
       seqlen_q_buffer, seqlen_k_buffer, params.stream, opts));
-
+#endif
   if (!params.stream->ok()) {
     return Internal("FusedMHAThunk::ExecuteOnStream failed.");
   }
@@ -200,6 +211,14 @@ absl::Status FusedMHABackwardThunk::ExecuteOnStream(
       AssignBufferIfNotNull(buffer_allocations, seqlen_k_buffer_);
   RunFusedMHABackwardOptions opts;
 
+#if TENSORFLOW_USE_SYCL
+  TF_RETURN_IF_ERROR(RunXetlaGpuFMHABackward(
+      config_, bmm1_grad_gemm1_rhs_buffer, bmm1_grad_gemm2_rhs_buffer,
+      bmm2_grad_gemm1_lhs_buffer, bmm2_grad_gemm2_rhs_buffer, d_output_buffer,
+      scratch_buffer, d_bmm1_lhs_buffer, d_bmm1_rhs_buffer, d_bmm2_rhs_buffer,
+      d_s_buffer, softmax_sum_buffer, d_Q_accum_buffer, mask_buffer,
+      d_bias_buffer, fwd_output_buffer, bias_buffer, params.stream));
+#else
   opts.runner_cache = &GetOrCreateRunner(params.stream);
 
   TF_RETURN_IF_ERROR(RunGpuFMHABackward(
@@ -209,6 +228,7 @@ absl::Status FusedMHABackwardThunk::ExecuteOnStream(
       d_s_buffer, softmax_sum_buffer, d_Q_accum_buffer, mask_buffer,
       d_bias_buffer, fwd_output_buffer, bias_buffer, seqlen_q_buffer,
       seqlen_k_buffer, params.stream, opts));
+#endif
   if (!params.stream->ok()) {
     return Internal("FusedMHABackwardThunk::ExecuteOnStream failed.");
   }
diff --git a/xla/service/gpu/runtime/gemm_thunk.cc b/xla/service/gpu/runtime/gemm_thunk.cc
index 4d46a78c4a..afee556685 100644
--- a/xla/service/gpu/runtime/gemm_thunk.cc
+++ b/xla/service/gpu/runtime/gemm_thunk.cc
@@ -41,6 +41,7 @@ GemmThunk::GemmThunk(ThunkInfo thunk_info, GemmConfig config,
       workspace_(workspace),
       deterministic_(deterministic) {}
 
+#if !TENSORFLOW_USE_SYCL
 absl::Status GemmThunk::ExecuteOnStream(const ExecuteParams& params) {
   VLOG(3) << "Running GEMM thunk";
   const BufferAllocations& allocs = *params.buffer_allocations;
@@ -57,6 +58,12 @@ absl::Status GemmThunk::ExecuteOnStream(const ExecuteParams& params) {
                  allocs.GetDeviceAddress(output_buffer_), workspace,
                  deterministic_, stream);
 }
+#else // !TENSORFLOW_USE_SYCL
+Status GemmThunk::ExecuteOnStream(const ExecuteParams& params) {
+  return absl::InternalError(
+      "After SYCL support FFI Gemm, EmitGemmThunk should not be use.");
+}
+#endif // !TENSORFLOW_USE_SYCL
 
 absl::Status GemmThunk::Initialize(const InitializeParams& params) {
   if (!params.executor->AsBlas()) {
diff --git a/xla/service/gpu/runtime/gpublas_lt_matmul_thunk.cc b/xla/service/gpu/runtime/gpublas_lt_matmul_thunk.cc
index a6aaabe5b3..e70516e687 100644
--- a/xla/service/gpu/runtime/gpublas_lt_matmul_thunk.cc
+++ b/xla/service/gpu/runtime/gpublas_lt_matmul_thunk.cc
@@ -52,6 +52,13 @@ CublasLtMatmulThunk::CublasLtMatmulThunk(
       d_scale_buffer_(d_scale),
       d_amax_buffer_(d_amax) {}
 
+#if TENSORFLOW_USE_SYCL
+absl::Status CublasLtMatmulThunk::ExecuteOnStream(const ExecuteParams& params) {
+  return absl::InternalError(
+      "After SYCL support FFI Matmul, EmitCublasLtMatmulThunk should not be "
+      "use.");
+}
+#else  // TENSORFLOW_USE_SYCL
 absl::Status CublasLtMatmulThunk::ExecuteOnStream(const ExecuteParams& params) {
   TF_ASSIGN_OR_RETURN(auto plan, GetMatmulPlan(params.stream));
   TF_ASSIGN_OR_RETURN(auto algorithm, GetMatmulAlgorithm(plan));
@@ -124,6 +131,6 @@ CublasLtMatmulThunk::GetMatmulAlgorithm(
       matmul_algorithm_cache_.emplace(plan, algorithms[algorithm_idx_]);
   return it->second;
 }
-
+#endif  // TENSORFLOW_USE_SYCL
 }  // namespace gpu
 }  // namespace xla
diff --git a/xla/service/gpu/runtime/nccl_api.h b/xla/service/gpu/runtime/nccl_api.h
index 05f2300888..b4154971e6 100644
--- a/xla/service/gpu/runtime/nccl_api.h
+++ b/xla/service/gpu/runtime/nccl_api.h
@@ -146,6 +146,10 @@ class NcclApi {
   // https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/api/comms.html#ncclgetuniqueid
   virtual absl::StatusOr<NcclCliqueId> GetUniqueId() = 0;
 
+  // Temporary function to get unique id for non-NCCL backend.
+  virtual absl::StatusOr<NcclCliqueId> GetId(const NcclCliqueKey& key,
+                                             const RunId& id) = 0;
+                                             
   // Creates new communicators for given devices.
   //
   // This API doesn't have a corresponding API in NCCL and implemented as
diff --git a/xla/service/gpu/spir_compiler.cc b/xla/service/gpu/spir_compiler.cc
new file mode 100644
index 0000000000..bffa2d412b
--- /dev/null
+++ b/xla/service/gpu/spir_compiler.cc
@@ -0,0 +1,289 @@
+/* Copyright (c) 2023 Intel Corporation
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "xla/service/gpu/spir_compiler.h"
+
+#include <stdlib.h>
+
+#include <fstream>
+#include <memory>
+#include <string>
+#include <utility>
+#include <vector>
+
+#include "tsl/platform/path.h"
+#include "tsl/platform/status.h"
+#include "xla/tsl/util/env_var.h"
+#include "xla/hlo/ir/hlo_opcode.h"
+#include "xla/service/algebraic_simplifier.h"
+#include "xla/service/call_inliner.h"
+#include "xla/service/convert_mover.h"
+#include "xla/service/dot_dimension_merger.h"
+#include "xla/service/dump.h"
+#include "xla/service/float_normalization.h"
+#include "xla/service/float_support.h"
+#include "xla/service/gpu/backend_configs.pb.h"
+#include "xla/service/gpu/buffer_sharing.h"
+#include "xla/service/gpu/cublas_cudnn.h"
+#include "xla/service/gpu/cudnn_fused_conv_rewriter.h"
+#include "xla/service/gpu/cudnn_fused_mha_rewriter.h"
+#include "xla/service/gpu/cusolver_rewriter.h"
+#include "xla/service/gpu/gemm_impl_picker.h"
+#include "xla/service/gpu/gpu_conv_padding_legalization.h"
+#include "xla/service/gpu/gpu_conv_rewriter.h"
+#include "xla/service/gpu/gpu_layout_assignment.h"
+#include "xla/service/gpu/ir_emission_utils.h"
+#include "xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.h"
+#include "xla/service/gpu/move_copy_to_users.h"
+#include "xla/service/gpu/redundant_convert_mover.h"
+#include "xla/service/gpu/target_constants.h"
+#include "xla/service/gpu/triangular_solve_rewriter.h"
+#include "xla/service/hlo_constant_folding.h"
+#include "xla/service/hlo_cse.h"
+#include "xla/service/hlo_dce.h"
+#include "xla/service/hlo_pass_fix.h"
+#include "xla/service/hlo_pass_pipeline.h"
+#include "xla/service/hlo_verifier.h"
+#include "xla/service/layout_normalization.h"
+#include "xla/service/llvm_ir/llvm_util.h"
+#include "xla/service/reshape_decomposer.h"
+#include "xla/service/reshape_mover.h"
+#include "xla/service/tuple_simplifier.h"
+#include "xla/status_macros.h"
+#include "xla/stream_executor/sycl/hw_info.h"
+#include "xla/stream_executor/sycl/sycl_platform_id.h"
+#include "xla/types.h"
+#include "xla/util.h"
+
+namespace xla {
+namespace gpu {
+namespace {
+
+class ConvBfloat16Support : public FloatSupport {
+ public:
+  explicit ConvBfloat16Support()
+      : FloatSupport(BF16), is_conv_bf16_supported_(true) {}
+
+  bool SupportsLowPrecisionOperand(const HloInstruction& hlo,
+                                   int64_t operand_index) const override {
+    return (hlo.opcode() != HloOpcode::kConvolution) || is_conv_bf16_supported_;
+  }
+
+  bool SupportsLowPrecisionOutput(const HloInstruction& hlo) const override {
+    return (hlo.opcode() != HloOpcode::kConvolution) || is_conv_bf16_supported_;
+  }
+
+  bool SupportsMixedPrecisions(const HloInstruction& hlo) const override {
+    // Skip all HLOs other than convolutions.
+    return (hlo.opcode() != HloOpcode::kConvolution);
+  }
+
+ private:
+  bool is_conv_bf16_supported_;
+};
+
+}  // namespace
+
+absl::Status SPIRCompiler::OptimizeHloConvolutionCanonicalization(
+    HloModule* hlo_module, se::GpuComputeCapability gpu_version,
+    se::dnn::VersionInfo dnn_version,
+    se::DeviceMemoryAllocator* device_allocator) {
+  auto cuda_compute_capability =
+      std::get<se::CudaComputeCapability>(gpu_version);
+  // Convert convolutions into CustomCalls to onednn, then canonicalize them
+  // (GpuConvPaddingLegalization). Also expand cuSolver calls.
+  HloPassPipeline pipeline("conv_canonicalization");
+  pipeline.AddInvariantCheckerDebug<HloVerifier>(
+      /*layout_sensitive=*/false,
+      /*allow_mixed_precision=*/false);
+
+  // Convert upsupported bf16 convolutions to f32.
+  ConvBfloat16Support conv_bf16_support;
+  pipeline.AddPass<FloatNormalization>(&conv_bf16_support);
+
+  pipeline.AddPass<GpusolverRewriter>();
+  pipeline.AddPass<GpuConvRewriter>();
+  pipeline.AddPass<CudnnFusedConvRewriter>(cuda_compute_capability);
+  pipeline.AddPass<GpuConvPaddingLegalization>();
+
+  // The conv padding/vectorization passes which we need to get rid of.  They
+  // also leave behind unnecessary tuple/get-tuple-element pairs that
+  // TupleSimplifier fixes.
+  pipeline.AddPass<CallInliner>();
+  pipeline.AddPass<TupleSimplifier>();
+
+  AlgebraicSimplifierOptions algsimp_options =
+      GetAlgebraicSimplifierOptions(hlo_module->config());
+  algsimp_options.set_enable_conv_operand_swap(false);
+  algsimp_options.set_enable_unconditional_reduce_of_concat_replacement(false);
+  pipeline.AddPass<HloPassFix<AlgebraicSimplifier>>(algsimp_options);
+
+  // tf2xla bridge, DepthwiseConvolutionConverter, GpuConvRewriter, and
+  // CudnnSimplifyPadding introduce reshapes and transposes.  Run ReshapeMover
+  // to a fixed point.  Include algsimp because ReshapeMover relies on it.
+  [&, &pipeline = pipeline.AddPass<HloPassFix<HloPassPipeline>>(
+          "reshape_mover_after_conv_canonicalization")] {
+    ReshapeMoverOptions reshape_mover_options;
+    reshape_mover_options.reshape_of_1d_broadcast_is_cheap = true;
+    pipeline.AddPass<HloPassFix<ReshapeMover>>(reshape_mover_options);
+    pipeline.AddPass<AlgebraicSimplifier>(algsimp_options);
+  }();
+
+  // The reshapes and transposes can possibly be eliminated using
+  // AlgebraicSimplifier. ConvertMover and ReshapeMover fight with each other.
+  // ConvertMover wants to move some converts down the graph, but ReshapeMover
+  // wants to move them up the graph. We run ConvertMover and algsimp to a fixed
+  // point.
+  [&, &pipeline = pipeline.AddPass<HloPassFix<HloPassPipeline>>(
+          "simplify_after_conv_canonicalization")] {
+    pipeline.AddPass<ConvertMover>();
+    pipeline.AddPass<AlgebraicSimplifier>(algsimp_options);
+  }();
+
+  // GpuConvRewriter, GpuConvPaddingLegalization and
+  // CudnnConvPadForTensorCores may add instructions which can be simplified
+  // by constant folding.
+  pipeline.AddPass<HloConstantFolding>();
+  TF_RETURN_IF_ERROR(pipeline.Run(hlo_module).status());
+
+  return absl::OkStatus();
+}
+
+absl::Status SPIRCompiler::OptimizeHloPostLayoutAssignment(
+    HloModule* hlo_module, se::StreamExecutor* stream_exec,
+    const CompileOptions& options, const TargetConfig& gpu_target_config,
+    tsl::thread::ThreadPool* thread_pool) {
+  HloPassPipeline pre_pipeline("spir post-layout_assignment part 1");
+
+  // This needs to run before GemmRewriter, which is part of
+  // OptimizeHloPostLayoutAssignment().
+  auto cuda_compute_capability = std::get<se::CudaComputeCapability>(
+      gpu_target_config.device_description.gpu_compute_capability());
+
+  // For frontend debugging.
+  FrontendAttributes frontend_attributes;
+  bool is_xetla_hardware_support = IsXetlaHardwareSupport();
+  if (is_xetla_hardware_support) {
+    frontend_attributes.mutable_map()->emplace("is_xetla_hardware_support",
+                                               "True");
+  }
+  hlo_module->add_frontend_attributes(frontend_attributes);
+  bool use_mha = true;
+  TF_CHECK_OK(tsl::ReadBoolFromEnvVar("MHA", true, &use_mha));
+  if (use_mha && is_xetla_hardware_support) {
+    HloPassPipeline mha_fusion_pipeline("multi-headed attention fusion");
+    const DebugOptions& debug_options = hlo_module->config().debug_options();
+    // The LayoutAssignment pass may leave behind kCopy instructions which are
+    // duplicate or NOPs, so remove them with algebraic simplification and CSE.
+    AlgebraicSimplifierOptions alg_sim_options;
+    alg_sim_options.set_supports_non_canonical_dots(false);
+    alg_sim_options.set_is_layout_sensitive(true);
+    alg_sim_options.set_enable_conv_operand_swap(false);
+    // "slow" minmax means we propagate nan.
+    alg_sim_options.set_minmax_propagate_nan(
+        !hlo_module->config().debug_options().xla_gpu_enable_fast_min_max());
+    alg_sim_options.set_enable_unconditional_reduce_of_concat_replacement(
+        false);
+    if (debug_options.xla_gpu_normalize_layouts()) {
+      mha_fusion_pipeline.AddPass<ReshapeDecomposer>();
+      mha_fusion_pipeline.AddPass<HloPassFix<MoveCopyToUsers>>();
+      mha_fusion_pipeline.AddPass<LayoutNormalization>();
+    }
+    mha_fusion_pipeline.AddPass<HloCSE>(/*is_layout_sensitive=*/true);
+    mha_fusion_pipeline.AddPass<HloPassFix<AlgebraicSimplifier>>(
+        alg_sim_options);
+    mha_fusion_pipeline.AddPass<HloCSE>(/*is_layout_sensitive=*/true);
+
+    // Rewrite Multi-Headed Attention modules to Fused MHA custom-calls.
+    mha_fusion_pipeline.AddPass<RedundantConvertMover>();
+    mha_fusion_pipeline.AddPass<HloDCE>();
+    mha_fusion_pipeline.AddPass<CudnnFusedMHARewriter>(cuda_compute_capability,
+                                                       stream_exec);
+    mha_fusion_pipeline.AddPass<AlgebraicSimplifier>(alg_sim_options);
+    mha_fusion_pipeline.AddPass<HloDCE>();
+    mha_fusion_pipeline.AddPass<HloCSE>(/*is_layout_sensitive=*/true,
+                                        /*only_fusion_computations*/ false);
+    TF_RETURN_IF_ERROR(mha_fusion_pipeline.Run(hlo_module).status());
+  }
+
+  pre_pipeline.AddPass<DotDimensionMerger>();
+
+  // Padding a gemm operand that's a constant results in pad(constant).  Run
+  // constant-folding to simplify this into a new constant.
+  pre_pipeline.AddPass<HloConstantFolding>();
+  TF_RETURN_IF_ERROR(pre_pipeline.Run(hlo_module).status());
+
+  TF_RETURN_IF_ERROR(GpuCompiler::OptimizeHloPostLayoutAssignment(
+      hlo_module, stream_exec, options, gpu_target_config, thread_pool));
+
+  HloPassPipeline post_pipeline("spir post-layout_assignment part 2");
+
+  // Transform TriangularSolve ops into custom-calls, so we can add temp
+  // memory.
+  post_pipeline.AddPass<TriangularSolveRewriter>();
+
+  TF_RETURN_IF_ERROR(post_pipeline.Run(hlo_module).status());
+
+  return absl::OkStatus();
+}
+
+absl::Status SPIRCompiler::AddConvAndGemmAutotuningPasses(
+    HloPassPipeline* pipeline, HloModule* hlo_module,
+    AutotuneConfig& autotune_config, tsl::thread::ThreadPool* thread_pool) {
+  pipeline->AddPass<GemmAlgorithmPicker>(autotune_config);
+  return OkStatus();
+}
+
+SPIRCompiler::SPIRCompiler()
+    : GpuCompiler(stream_executor::sycl::kSyclPlatformId, spir::TargetTriple(),
+                  spir::DataLayout()) {}
+
+HloDataflowAnalysis::CanShareBuffer SPIRCompiler::GetCanShareBuffer() const {
+  return &CanShareBufferHint;
+}
+
+absl::StatusOr<GpuCompiler::BackendCompileResult>
+SPIRCompiler::CompileTargetBinary(const HloModuleConfig& module_config,
+                                  llvm::Module* llvm_module,
+                                  se::GpuComputeCapability gpu_version,
+                                  bool relocatable,
+                                  const HloModule* debug_module,
+                                  const CompileOptions& options) {
+  if (relocatable) {
+    return Unimplemented("relocatable target binary is not implemented");
+  }
+
+  std::vector<uint8_t> spir;
+  {
+    // This may print multiple lines per HLO compilation because of the
+    // parallelized compilation of LLVM modules.
+    XLA_SCOPED_LOGGING_TIMER_IF(
+        "SPIRCompiler::CompileTargetBinary - CompileToSpir",
+        !options.is_autotuning_compilation);
+    TF_ASSIGN_OR_RETURN(spir,
+                        spir::CompileToSpir(llvm_module, gpu_version,
+                                            module_config.debug_options()));
+  }
+
+  return BackendCompileResult{"", std::move(spir)};
+}
+
+/*static*/ SPIRCompiler* SPIRCompiler::CreateSPIRCompiler() {
+  static auto compiler = absl::make_unique<SPIRCompiler>();
+  return compiler.get();
+}
+
+}  // namespace gpu
+}  // namespace xla
diff --git a/xla/service/gpu/spir_compiler.h b/xla/service/gpu/spir_compiler.h
new file mode 100644
index 0000000000..f620a79047
--- /dev/null
+++ b/xla/service/gpu/spir_compiler.h
@@ -0,0 +1,70 @@
+/* Copyright (c) 2023 Intel Corporation
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef XLA_SERVICE_GPU_SPIR_COMPILER_H_
+#define XLA_SERVICE_GPU_SPIR_COMPILER_H_
+
+#include <string>
+#include <utility>
+#include <vector>
+
+#include "absl/base/call_once.h"
+#include "llvm/IRReader/IRReader.h"
+#include "llvm/Support/SourceMgr.h"
+#include "xla/service/gpu/gpu_compiler.h"
+#include "xla/statusor.h"
+
+namespace xla {
+namespace gpu {
+
+// SPIRCompiler generates efficient GPU executables for NVPTX target.
+class SPIRCompiler : public GpuCompiler {
+ public:
+  SPIRCompiler();
+  ~SPIRCompiler() override {}
+
+  absl::Status OptimizeHloConvolutionCanonicalization(
+      HloModule* hlo_module, se::GpuComputeCapability gpu_version,
+      se::dnn::VersionInfo dnn_version,
+      se::DeviceMemoryAllocator* device_allocator) override;
+
+  absl::Status OptimizeHloPostLayoutAssignment(
+      HloModule* hlo_module, se::StreamExecutor* stream_exec,
+      const CompileOptions& options, const TargetConfig& gpu_target_config,
+      tsl::thread::ThreadPool* thread_pool) override;
+
+  absl::Status AddConvAndGemmAutotuningPasses(
+      HloPassPipeline* pipeline, HloModule* hlo_module,
+      AutotuneConfig& autotune_config,
+      tsl::thread::ThreadPool* thread_pool) override;
+
+  HloDataflowAnalysis::CanShareBuffer GetCanShareBuffer() const override;
+
+  absl::StatusOr<BackendCompileResult> CompileTargetBinary(
+      const HloModuleConfig& module_config, llvm::Module* llvm_module,
+      se::GpuComputeCapability gpu_version, bool relocatable,
+      const HloModule* debug_module, const CompileOptions& options) override;
+
+  static SPIRCompiler* CreateSPIRCompiler();
+
+ private:
+  SPIRCompiler(const SPIRCompiler&) = delete;
+  SPIRCompiler& operator=(const SPIRCompiler&) = delete;
+};
+
+}  // namespace gpu
+}  // namespace xla
+
+#endif  // XLA_SERVICE_GPU_SPIR_COMPILER_H_
diff --git a/xla/service/gpu/spir_compiler_registration.cc b/xla/service/gpu/spir_compiler_registration.cc
new file mode 100644
index 0000000000..a397ae7a17
--- /dev/null
+++ b/xla/service/gpu/spir_compiler_registration.cc
@@ -0,0 +1,27 @@
+/* Copyright (c) 2023 Intel Corporation
+
+Copyright 2019 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "xla/service/gpu/spir_compiler.h"
+#include "xla/stream_executor/sycl/sycl_platform_id.h"
+
+static bool InitCompilerModule() {
+  xla::Compiler::RegisterCompilerFactory(
+      stream_executor::sycl::kSyclPlatformId,
+      []() { return std::make_unique<xla::gpu::SPIRCompiler>(); });
+  return true;
+}
+static bool compiler_module_initialized = InitCompilerModule();
diff --git a/xla/service/gpu/stream_executor_util.cc b/xla/service/gpu/stream_executor_util.cc
index 4827fc251c..171078aeaf 100644
--- a/xla/service/gpu/stream_executor_util.cc
+++ b/xla/service/gpu/stream_executor_util.cc
@@ -176,6 +176,13 @@ StreamExecutorConvLayoutsToXlaLayouts(const ConvolutionDimensionNumbers& dnums,
                            dnums.kernel_spatial_dimensions().end());
       filter_layout.push_back(dnums.kernel_input_feature_dimension());
       break;
+    case FilterLayout::kYXInputOutput:  // HWIO
+      filter_layout.insert(filter_layout.end(),
+                           dnums.kernel_spatial_dimensions().begin(),
+                           dnums.kernel_spatial_dimensions().end());
+      filter_layout.push_back(dnums.kernel_input_feature_dimension());
+      filter_layout.push_back(dnums.kernel_output_feature_dimension());
+      break;
     default:
       return Internal("Invalid filter layout %s for conv with dnums %s,",
                       FilterLayoutString(filter),
@@ -213,7 +220,7 @@ XlaConvShapesToStreamExecutorLayouts(const ConvolutionDimensionNumbers& dnums,
   Layout nhwc_input, nhwc_filter, nhwc_output;
   std::tie(nhwc_input, nhwc_filter, nhwc_output) =
       StreamExecutorConvLayoutsToXlaLayouts(dnums, DataLayout::kBatchYXDepth,
-                                            FilterLayout::kOutputYXInput,
+                                            FilterLayout::kYXInputOutput,
                                             DataLayout::kBatchYXDepth)
           .value();
 
@@ -262,7 +269,7 @@ XlaConvShapesToStreamExecutorLayouts(const ConvolutionDimensionNumbers& dnums,
           ConvolutionDimensionNumbersToString(dnums), vect_size);
     }
   } else if (LayoutUtil::Equal(filter.layout(), nhwc_filter)) {
-    filter_layout = FilterLayout::kOutputYXInput;
+    filter_layout = FilterLayout::kYXInputOutput;
   } else {
     return Internal(
         "Invalid filter layout %s for conv with dnums %s, expected one of (%s, "
diff --git a/xla/service/gpu/target_constants.h b/xla/service/gpu/target_constants.h
index 13190ae690..574a4120d7 100644
--- a/xla/service/gpu/target_constants.h
+++ b/xla/service/gpu/target_constants.h
@@ -67,7 +67,7 @@ inline const char* DataLayout() {
       "e-p:64:64:64-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-f32:"
       "32:32-f64:64:64-v16:16:16-v24:32:32-v32:32:32-v48:64:64-v64:64:64-v96:"
       "128:128-v128:128:128-v192:256:256-v256:256:256-v512:512:512-v1024:1024:"
-      "1024";
+      "1024-n8:16:32:64";
   return kDataLayout;
 }
 }  // namespace spir
diff --git a/xla/service/gpu/tests/dynamic_update_slice_inplace.hlo b/xla/service/gpu/tests/dynamic_update_slice_inplace.hlo
index 3d0af18b08..ae88c58014 100644
--- a/xla/service/gpu/tests/dynamic_update_slice_inplace.hlo
+++ b/xla/service/gpu/tests/dynamic_update_slice_inplace.hlo
@@ -141,13 +141,13 @@ ENTRY main {
 // CHECK:      [[DUS1]].in_bounds-after:
 // CHECK-NEXT:   ret void
 // CHECK:      [[DUS0]].in_bounds-true
-// CHECK-DAG:    getelementptr bfloat, ptr %[[ARG1]], i64 %[[VAL_141:.*]]
-// CHECK-DAG:    getelementptr bfloat, ptr %[[ARG3]], i64 %[[VAL_141]]
-// CHECK-DAG:    getelementptr inbounds [10 x [11 x [12 x bfloat]]], ptr %[[ARG0]], i64 0, i64 %[[VAL_185:.*]], i64 %[[VAL_187:.*]], i64 %[[VAL_189:.*]]
+// CHECK-DAG:    getelementptr i16, ptr %[[ARG1]], i64 %[[VAL_141:.*]]
+// CHECK-DAG:    getelementptr i16, ptr %[[ARG3]], i64 %[[VAL_141]]
+// CHECK-DAG:    getelementptr inbounds [10 x [11 x [12 x i16]]], ptr %[[ARG0]], i64 0, i64 %[[VAL_185:.*]], i64 %[[VAL_187:.*]], i64 %[[VAL_189:.*]]
 // CHECK:      [[DUS1]].in_bounds-true
-// CHECK-DAG:    getelementptr bfloat, ptr %[[ARG1]], i64 %[[VAL_173:.*]]
-// CHECK-DAG:    getelementptr bfloat, ptr %[[ARG3]], i64 %[[VAL_173]]
-// CHECK-DAG:    getelementptr inbounds [8 x [11 x [12 x bfloat]]], ptr %[[ARG2]], i64 0, i64 %[[VAL_208:.*]], i64 %[[VAL_210:.*]], i64 %[[VAL_212:.*]]
+// CHECK-DAG:    getelementptr i16, ptr %[[ARG1]], i64 %[[VAL_173:.*]]
+// CHECK-DAG:    getelementptr i16, ptr %[[ARG3]], i64 %[[VAL_173]]
+// CHECK-DAG:    getelementptr inbounds [8 x [11 x [12 x i16]]], ptr %[[ARG2]], i64 0, i64 %[[VAL_208:.*]], i64 %[[VAL_210:.*]], i64 %[[VAL_212:.*]]
 
 HloModule MultipleInplaceDus, is_scheduled=true, input_output_alias={ {0}: (0, {}), {1}: (2, {}) }
 
@@ -191,13 +191,13 @@ ENTRY main {
 // CHECK:      [[DUS1]].in_bounds-after:
 // CHECK-NEXT:   ret void
 // CHECK:      [[DUS0]].in_bounds-true
-// CHECK-DAG:    getelementptr bfloat, ptr %[[ARG1]], i64 %[[VAL_247:.*]]
-// CHECK-DAG:    getelementptr bfloat, ptr %[[ARG3]], i64 %[[VAL_247]]
-// CHECK-DAG:    getelementptr inbounds [10 x [11 x [12 x bfloat]]], ptr %[[ARG0]], i64 0, i64 %[[VAL_291:.*]], i64 %[[VAL_293:.*]], i64 %[[VAL_295:.*]]
+// CHECK-DAG:    getelementptr i16, ptr %[[ARG1]], i64 %[[VAL_247:.*]]
+// CHECK-DAG:    getelementptr i16, ptr %[[ARG3]], i64 %[[VAL_247]]
+// CHECK-DAG:    getelementptr inbounds [10 x [11 x [12 x i16]]], ptr %[[ARG0]], i64 0, i64 %[[VAL_291:.*]], i64 %[[VAL_293:.*]], i64 %[[VAL_295:.*]]
 // CHECK:      [[DUS1]].in_bounds-true
-// CHECK-DAG:    getelementptr bfloat, ptr %[[ARG1]], i64 %[[VAL_279:.*]]
-// CHECK-DAG:    getelementptr bfloat, ptr %[[ARG3]], i64 %[[VAL_279]]
-// CHECK-DAG:    getelementptr inbounds [8 x [11 x [12 x bfloat]]], ptr %[[ARG2]], i64 0, i64 %[[VAL_314:.*]], i64 %[[VAL_316:.*]], i64 %[[VAL_318:.*]]
+// CHECK-DAG:    getelementptr i16, ptr %[[ARG1]], i64 %[[VAL_279:.*]]
+// CHECK-DAG:    getelementptr i16, ptr %[[ARG3]], i64 %[[VAL_279]]
+// CHECK-DAG:    getelementptr inbounds [8 x [11 x [12 x i16]]], ptr %[[ARG2]], i64 0, i64 %[[VAL_314:.*]], i64 %[[VAL_316:.*]], i64 %[[VAL_318:.*]]
 
 HloModule MultipleInplaceDusWithTransposeBitcastToTheRoot, is_scheduled=true, input_output_alias={ {0}: (0, {}), {1}: (2, {}) }
 
@@ -238,9 +238,9 @@ ENTRY main {
 // CHECK:      [[DUS0]].in_bounds-after:
 // CHECK-NEXT:   ret void
 // CHECK:      [[DUS0]].in_bounds-true
-// CHECK-DAG:    getelementptr bfloat, ptr %[[ARG1]], i64 %[[VAL_353:.*]]
-// CHECK-DAG:    getelementptr bfloat, ptr %[[ARG2]], i64 %[[VAL_353]]
-// CHECK-DAG:    getelementptr inbounds [10 x [11 x [12 x bfloat]]], ptr %[[ARG0]], i64 0, i64 %[[VAL_366:.*]], i64 %[[VAL_368:.*]], i64 %[[VAL_370:.*]]
+// CHECK-DAG:    getelementptr i16, ptr %[[ARG1]], i64 %[[VAL_353:.*]]
+// CHECK-DAG:    getelementptr i16, ptr %[[ARG2]], i64 %[[VAL_353]]
+// CHECK-DAG:    getelementptr inbounds [10 x [11 x [12 x i16]]], ptr %[[ARG0]], i64 0, i64 %[[VAL_366:.*]], i64 %[[VAL_368:.*]], i64 %[[VAL_370:.*]]
 
 HloModule SingleInplaceDusWithTransposeBitcastToTheRoot, is_scheduled=true, input_output_alias={ {}: (0, {}) }
 
@@ -277,9 +277,9 @@ ENTRY main {
 // CHECK:      [[DUS0]].in_bounds-after:
 // CHECK-NEXT:   ret void
 // CHECK:      [[DUS0]].in_bounds-true
-// CHECK-DAG:    getelementptr bfloat, ptr %[[ARG1]], i64 %[[VAL_408:.*]]
-// CHECK-DAG:    getelementptr bfloat, ptr %[[ARG2]], i64 %[[VAL_408:.*]]
-// CHECK-DAG:    getelementptr inbounds [10 x [11 x [12 x bfloat]]], ptr %[[ARG0]], i64 0, i64 %[[VAL_421:.*]], i64 %[[VAL_423:.*]], i64 %[[VAL_425:.*]]
+// CHECK-DAG:    getelementptr i16, ptr %[[ARG1]], i64 %[[VAL_408:.*]]
+// CHECK-DAG:    getelementptr i16, ptr %[[ARG2]], i64 %[[VAL_408:.*]]
+// CHECK-DAG:    getelementptr inbounds [10 x [11 x [12 x i16]]], ptr %[[ARG0]], i64 0, i64 %[[VAL_421:.*]], i64 %[[VAL_423:.*]], i64 %[[VAL_425:.*]]
 
 HloModule SingleInplaceDusWithReshapeBitcastToTheRoot, is_scheduled=true, input_output_alias={ {}: (0, {}) }
 
@@ -316,9 +316,9 @@ ENTRY main {
 // CHECK:      [[DUS0]].in_bounds-after:
 // CHECK-NEXT:   ret void
 // CHECK:      [[DUS0]].in_bounds-true
-// CHECK-DAG:    getelementptr bfloat, ptr %[[ARG1]], i64 %[[VAL_468:.*]]
-// CHECK-DAG:    getelementptr bfloat, ptr %[[ARG2]], i64 %[[VAL_468]]
-// CHECK-DAG:    getelementptr inbounds [10 x [6 x [2 x [11 x bfloat]]]], ptr %[[ARG0]], i64 0, i64 %[[VAL_483:.*]], i64 %[[VAL_485:.*]], i64 %[[VAL_487:.*]], i64 %[[VAL_489:.*]]
+// CHECK-DAG:    getelementptr i16, ptr %[[ARG1]], i64 %[[VAL_468:.*]]
+// CHECK-DAG:    getelementptr i16, ptr %[[ARG2]], i64 %[[VAL_468]]
+// CHECK-DAG:    getelementptr inbounds [10 x [6 x [2 x [11 x i16]]]], ptr %[[ARG0]], i64 0, i64 %[[VAL_483:.*]], i64 %[[VAL_485:.*]], i64 %[[VAL_487:.*]], i64 %[[VAL_489:.*]]
 
 HloModule SingleInplaceDusWithBitcastToTheRootAndFromTheParameter, is_scheduled=true, input_output_alias={ {}: (0, {}) }
 
diff --git a/xla/service/hlo_dataflow_analysis.cc b/xla/service/hlo_dataflow_analysis.cc
index a60e0211ca..96ff0bacd9 100644
--- a/xla/service/hlo_dataflow_analysis.cc
+++ b/xla/service/hlo_dataflow_analysis.cc
@@ -1815,37 +1815,6 @@ bool HloDataflowAnalysis::DoesNotUseOperandBuffer(
 
 namespace {
 
-// Removes layers of tuple indirection introduced via 'tuple' and
-// 'get-tuple-element' instructions to more directly identify the source of the
-// given HLO value (identified by the given `ShapeIndex` into the output of the
-// given `HloInstruction`).
-//
-// e.g. for the following:
-//    %x = some-op(...)
-//    %foo = get-tuple-element(%x), index=0
-//    %bar = tuple(%y, %foo)
-//
-// ... FollowTupleIndirection(%bar, {1}) == {%x, {0}} (output 1 of 'bar' comes
-// from output 0 of %x).
-//
-// Note that all 'tuple' instructions are followed before all
-// 'get-tuple-element' instructions are followed. This is because it is assumed
-// that tupling a value and then extracting it from the tuple again will not
-// occur in properly-optimized IR.
-std::pair<const HloInstruction*, ShapeIndex> FollowTupleIndirection(
-    const HloInstruction* instruction, ShapeIndex operand_index) {
-  while (instruction->opcode() == HloOpcode::kTuple && !operand_index.empty()) {
-    instruction = instruction->operand(operand_index.front());
-    operand_index.pop_front();
-  }
-  while (instruction->opcode() == HloOpcode::kGetTupleElement) {
-    operand_index.push_front(instruction->tuple_index());
-    instruction = instruction->operand(0);
-  }
-
-  return {instruction, operand_index};
-}
-
 // Returns in-place input/output pairs for the given fusion instruction,
 // according to the aliasing rules for the corresponding fusion computation.
 //
@@ -2180,4 +2149,18 @@ bool HloDataflowAnalysis::CanShareOperandBufferWithUser(
   return user->IsElementwiseOnOperand(user->operand_index(operand));
 }
 
+std::pair<const HloInstruction*, ShapeIndex> FollowTupleIndirection(
+    const HloInstruction* instruction, ShapeIndex operand_index) {
+  while (instruction->opcode() == HloOpcode::kTuple && !operand_index.empty()) {
+    instruction = instruction->operand(operand_index.front());
+    operand_index.pop_front();
+  }
+  while (instruction->opcode() == HloOpcode::kGetTupleElement) {
+    operand_index.push_front(instruction->tuple_index());
+    instruction = instruction->operand(0);
+  }
+
+  return {instruction, operand_index};
+}
+
 }  // namespace xla
diff --git a/xla/service/hlo_dataflow_analysis.h b/xla/service/hlo_dataflow_analysis.h
index a5cf84409a..e381771927 100644
--- a/xla/service/hlo_dataflow_analysis.h
+++ b/xla/service/hlo_dataflow_analysis.h
@@ -369,6 +369,26 @@ class HloDataflowAnalysis {
   ForwardsValue forwards_value_ = nullptr;
 };
 
+// Removes layers of tuple indirection introduced via 'tuple' and
+// 'get-tuple-element' instructions to more directly identify the source of the
+// given HLO value (identified by the given `ShapeIndex` into the output of the
+// given `HloInstruction`).
+//
+// e.g. for the following:
+//    %x = some-op(...)
+//    %foo = get-tuple-element(%x), index=0
+//    %bar = tuple(%y, %foo)
+//
+// ... FollowTupleIndirection(%bar, {1}) == {%x, {0}} (output 1 of 'bar' comes
+// from output 0 of %x).
+//
+// Note that all 'tuple' instructions are followed before all
+// 'get-tuple-element' instructions are followed. This is because it is assumed
+// that tupling a value and then extracting it from the tuple again will not
+// occur in properly-optimized IR.
+std::pair<const HloInstruction*, ShapeIndex> FollowTupleIndirection(
+    const HloInstruction* instruction, ShapeIndex operand_index);
+
 }  // namespace xla
 
 #endif  // XLA_SERVICE_HLO_DATAFLOW_ANALYSIS_H_
diff --git a/xla/service/hlo_verifier.cc b/xla/service/hlo_verifier.cc
index 751756ccda..60017f3fb9 100644
--- a/xla/service/hlo_verifier.cc
+++ b/xla/service/hlo_verifier.cc
@@ -1332,7 +1332,8 @@ Status ShapeVerifier::HandleCustomCall(HloInstruction* instruction) {
     } else {
       TF_RET_CHECK(ShapeUtil::Compatible(output_subshape, operand_subshape))
           << "Different aliasing shapes: " << operand_subshape.ToString()
-          << " vs " << output_subshape.ToString();
+          << " vs " << output_subshape.ToString() << ". Inst: "
+          << custom_call->ToString();
     }
   }
   return OkStatus();
diff --git a/xla/service/instruction_fusion.cc b/xla/service/instruction_fusion.cc
index bcc9d2e454..99f48a9d17 100644
--- a/xla/service/instruction_fusion.cc
+++ b/xla/service/instruction_fusion.cc
@@ -901,12 +901,10 @@ bool IsSafeToFuseSliceIntoDusFusion(const HloInstruction* producer,
                "operand that has the same value as the in-place buffer";
       }
     }
-    if (!producer->IsElementwise() &&
-        (consumer->operand(operand_number) == producer ||
-         absl::c_find(producer->operands(),
-                      consumer->operand(operand_number)) !=
-             producer->operands().end())) {
-      VLOG(4) << "Found non-elementwise producer that uses the same operand of "
+    if (consumer->operand(operand_number) == producer ||
+        absl::c_find(producer->operands(), consumer->operand(operand_number)) !=
+            producer->operands().end()) {
+      VLOG(4) << "Found producer that uses the same operand of "
                  "an in-place consumer";
       auto get_real_operand = [](const HloInstruction* op,
                                  const HloInstruction* operand) {
@@ -926,20 +924,51 @@ bool IsSafeToFuseSliceIntoDusFusion(const HloInstruction* producer,
       };
       // A common special case is a slice or dynamic-slice and a
       // dynamic-update-slice that use the same indices. This pattern is safe.
-
+      auto is_nonelementwise_op = [](const HloInstruction* inst) {
+        return inst->opcode() != HloOpcode::kFusion && !inst->IsElementwise() &&
+               inst->opcode() != HloOpcode::kBitcast &&
+               inst->opcode() != HloOpcode::kParameter;
+      };
       auto producer_nonelementwise_ops =
-          ExtractInstructions(producer, [](const HloInstruction* inst) {
-            return inst->opcode() != HloOpcode::kFusion &&
-                   !inst->IsElementwise() &&
-                   inst->opcode() != HloOpcode::kBitcast &&
-                   inst->opcode() != HloOpcode::kParameter;
+          ExtractInstructions(producer, [&](const HloInstruction* inst) {
+            return is_nonelementwise_op(inst);
           });
       if (producer_nonelementwise_ops.size() > 1) {
         return "Producer fusion has multiple non-elementwise ops, bailing.";
       }
       // If the producer has only elementwise ops or bitcasts, we can fuse.
       if (producer_nonelementwise_ops.empty()) {
-        return {};
+        if (consumer->opcode() != HloOpcode::kFusion) {
+          return {};
+        }
+        // If consumer fusion have inplace ops and non-elementwise ops and all
+        // of them access the same buffer of producer, there exists inplace
+        // conflict also even though producer has no non-elementwise ops.
+        auto inplace_instr_and_index = FollowTupleIndirection(
+            consumer->fused_expression_root(), pair.second);
+        auto consumer_nonelementwise_ops =
+            ExtractInstructions(consumer, [&](const HloInstruction* inst) {
+              // Exclude instructions which are same as inplace fusion roots.
+              return is_nonelementwise_op(inst) &&
+                     inst != inplace_instr_and_index.first;
+            });
+        auto* fused_computation = consumer->fused_instructions_computation();
+        std::unique_ptr<HloReachabilityMap> reachability =
+            HloReachabilityMap::Build(fused_computation);
+        auto inplace_consumer_parameter =
+            fused_computation->parameter_instruction(
+                (consumer->operand_index(producer)));
+        bool inplace_conflict_after_fusion = false;
+        absl::c_for_each(
+            consumer_nonelementwise_ops, [&](const HloInstruction* inst) {
+              if (reachability->IsReachable(inplace_consumer_parameter, inst)) {
+                inplace_conflict_after_fusion = true;
+              }
+            });
+        return inplace_conflict_after_fusion
+                   ? "Non-elementwise ops in consumer lead to inplace conflict "
+                     "after fusion."
+                   : FusionDecision();
       }
       auto dus_ops =
           ExtractInstructions(consumer, HloOpcode::kDynamicUpdateSlice);
diff --git a/xla/service/llvm_ir/kernel_support_library.cc b/xla/service/llvm_ir/kernel_support_library.cc
index 7d72be76a0..568b7e3ecf 100644
--- a/xla/service/llvm_ir/kernel_support_library.cc
+++ b/xla/service/llvm_ir/kernel_support_library.cc
@@ -38,7 +38,7 @@ Status KernelSupportLibrary::ForWithStatus(
         for_body_generator) {
   if (peel_first_iteration) {
     return ForWithStatus(
-        name, start, end, step, true,
+        name, start, end, step,
         [&](llvm::Value* indvar, bool is_first_iteration) -> Status {
           return for_body_generator(indvar, b_->getInt1(is_first_iteration));
         });
diff --git a/xla/service/llvm_ir/llvm_util.cc b/xla/service/llvm_ir/llvm_util.cc
index 332c36ddd1..bc59b39916 100644
--- a/xla/service/llvm_ir/llvm_util.cc
+++ b/xla/service/llvm_ir/llvm_util.cc
@@ -223,16 +223,24 @@ llvm::Type* PrimitiveTypeToIrType(PrimitiveType element_type,
       return llvm::Type::getInt8Ty(module->getContext());
     case S16:
     case U16:
+    case BF16:
+      // For BF16 we just need some type that is 16 bits wide so that it will
+      // take up the right amount of space in memory. LLVM does not have a BF16
+      // type (the LLVM half type is IEEE 16 bit floating point, not bfloat), so
+      // we can't map it directly to an LLVM type. We will not map a BF16
+      // addition to an addition on this type (int16_t) - this is just the type
+      // used for storage.
+      // SYCL: revert a883b54870bc7ce24e510f941d458a354de67d66 to avoid 
+      // Unexpected llvm intrinsic like llvm.fabs.bf16 & llvm.floor.bf16
       return llvm::Type::getInt16Ty(module->getContext());
     case F8E5M2:
     case F8E5M2FNUZ:
     case F8E4M3FN:
     case F8E4M3B11FNUZ:
     case F8E4M3FNUZ:
-      // We represent F8 as an int since there is no LLVM F8 dtype.
+      // Similarly as with BF16, we represent F8 as an int since there is no
+      // LLVM F8 dtype.
       return llvm::Type::getInt8Ty(module->getContext());
-    case BF16:
-      return llvm::Type::getBFloatTy(module->getContext());
     case F16:
       return llvm::Type::getHalfTy(module->getContext());
     case S32:
diff --git a/xla/service/scatter_promotion.cc b/xla/service/scatter_promotion.cc
new file mode 100644
index 0000000000..200845ff17
--- /dev/null
+++ b/xla/service/scatter_promotion.cc
@@ -0,0 +1,142 @@
+/* Copyright 2022 The OpenXLA Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "xla/service/scatter_promotion.h"
+
+#include <memory>
+#include <optional>
+#include <string>
+#include <utility>
+
+#include "xla/service/hlo_creation_utils.h"
+
+namespace xla {
+namespace {
+
+std::optional<PrimitiveType> Get16bitFloatOperandType(
+    const HloInstruction* instr) {
+  std::optional<PrimitiveType> type;
+  for (const HloInstruction* operand : instr->operands()) {
+    if (!type.has_value() && (operand->shape().element_type() == F16 ||
+                              operand->shape().element_type() == BF16)) {
+      type = operand->shape().element_type();
+    }
+  }
+  if (!type.has_value()) {
+    return std::nullopt;
+  }
+  return type;
+}
+
+bool IsScatter(const HloInstruction* inst) {
+  return inst->opcode() == HloOpcode::kScatter;
+}
+
+std::unique_ptr<HloInstruction> Clone16bitScatter(
+    const HloInstruction* inst, const Shape& shape,
+    absl::Span<HloInstruction* const> operands) {
+  // clone an scatter and also clone the attached computation to match the type.
+  std::unique_ptr<HloInstruction> new_inst =
+      inst->CloneWithNewOperands(shape, operands);
+  HloComputation* to_apply = new_inst->to_apply();
+  HloComputation* to_apply_promoted = [&]() {
+    PrimitiveType type = shape.element_type();
+    std::string name = absl::StrCat(to_apply->name(), "_promoted");
+    HloComputation::Builder promoted(name);
+    auto x = promoted.AddInstruction(HloInstruction::CreateParameter(
+        /*parameter_number=*/0, ShapeUtil::MakeShape(type, {}), "x"));
+    auto y = promoted.AddInstruction(HloInstruction::CreateParameter(
+        /*parameter_number=*/1, ShapeUtil::MakeShape(type, {}), "y"));
+    promoted.AddInstruction(HloInstruction::CreateBinary(
+        ShapeUtil::MakeShape(type, {}), to_apply->root_instruction()->opcode(),
+        x, y));
+    return inst->GetModule()->AddEmbeddedComputation(promoted.Build());
+  }();
+  new_inst->set_to_apply(to_apply_promoted);
+  return new_inst;
+}
+
+}  // namespace
+
+StatusOr<bool> ChangeMultiInputOpDataType::Run(
+    HloModule* module,
+    const absl::flat_hash_set<absl::string_view>& execution_threads) {
+  bool changed = false;
+  HloCloner default_cloner = [](const HloInstruction* inst, const Shape& shape,
+                                absl::Span<HloInstruction* const> operands) {
+    return inst->CloneWithNewOperands(shape, operands);
+  };
+  HloCloner cloner = cloner_ ? cloner_ : default_cloner;
+
+  for (HloComputation* comp :
+       module->MakeNonfusionComputations(execution_threads)) {
+    for (HloInstruction* instr : comp->MakeInstructionPostOrder()) {
+      std::optional<PrimitiveType> operand_type =
+          Get16bitFloatOperandType(instr);
+      if (!op_matcher_(instr) || !operand_type.has_value() ||
+          !instr->shape().IsArray() ||
+          instr->opcode() == HloOpcode::kParameter ||
+          (op_matcher_(instr) &&
+           instr->to_apply()->root_instruction()->operand_count() != 2)) {
+        continue;
+      }
+      const PrimitiveType from_type = *operand_type;
+      auto it = to_type_map_.find(from_type);
+      if (it == to_type_map_.end()) {
+        continue;
+      }
+
+      const PrimitiveType to_type = it->second;
+      absl::InlinedVector<HloInstruction*, 8> new_operands;
+
+      for (HloInstruction* operand : instr->mutable_operands()) {
+        const PrimitiveType cur_operand_type = operand->shape().element_type();
+        auto it = to_type_map_.find(cur_operand_type);
+        if (it == to_type_map_.end()) {
+          // Remain datatype of the operands which mismatch the types in
+          // from_ty.
+          new_operands.push_back(operand);
+        } else {
+          // Change datatype of the operands which match one of the types in
+          // from_ty.
+          new_operands.push_back(MakeConvertToHlo(operand, to_type));
+        }
+      }
+
+      Shape new_shape = instr->shape();
+      new_shape.set_element_type(to_type);
+      HloInstruction* new_instr =
+          comp->AddInstruction(cloner(instr, new_shape, new_operands));
+      TF_RETURN_IF_ERROR(comp->ReplaceInstruction(
+          instr, MakeConvertToHlo(new_instr, from_type)));
+      changed = true;
+    }
+  }
+  return changed;
+}
+
+// Promote 16-bit float scatter to 32-bit float types.
+// {{F16, F32}, {BF16, F32}}
+ScatterPromotion::ScatterPromotion(
+    absl::Span<std::pair<PrimitiveType, PrimitiveType> const> from_to_types)
+    : pass_(from_to_types, IsScatter, Clone16bitScatter) {}
+
+StatusOr<bool> ScatterPromotion::Run(
+    HloModule* module,
+    const absl::flat_hash_set<absl::string_view>& execution_threads) {
+  return pass_.Run(module, execution_threads);
+}
+
+}  // namespace xla
diff --git a/xla/service/scatter_promotion.h b/xla/service/scatter_promotion.h
new file mode 100644
index 0000000000..abb13b74f8
--- /dev/null
+++ b/xla/service/scatter_promotion.h
@@ -0,0 +1,106 @@
+/* Copyright 2024 The OpenXLA Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef XLA_SERVICE_SCATTER_PROMOTION_H_
+#define XLA_SERVICE_SCATTER_PROMOTION_H_
+
+#include <functional>
+#include <memory>
+#include <utility>
+
+#include "xla/service/hlo_pass_interface.h"
+
+namespace xla {
+
+// Changes `from_ty op(from_ty a, from_ty b)` into
+// `from_ty convert(op(to_ty convert(a), to_ty convert(b)))`.
+//
+// This path is different from "xla/service/change_op_data_type.h"
+// ChangeOpDataType. ChangeOpDataType only considers ops that match `op_matcher`
+// and where all operands have type `from_ty`.  ChangeOpDataType will not do the
+// correct thing for ops like dynamic-slice where only some of the arguments
+// should be converted; it's up to you to avoid matching such ops with
+// `op_matcher`.
+//
+// The ChangeMultiInputOpDataType pass support multiple input which not all
+// operands have type `from_ty. It will apply the transform only to the operands
+// match one of the types in from_ty.
+//
+// It uses provided `cloner` to clone an instruction with shape and converted
+// operands. If the cloner is not provided, it will uses `CloneWithNewOperands`.
+class ChangeMultiInputOpDataType : public HloModulePass {
+ public:
+  using HloCloner = std::function<std::unique_ptr<HloInstruction>(
+      const HloInstruction*, const Shape&, absl::Span<HloInstruction* const>)>;
+  ChangeMultiInputOpDataType(
+      absl::Span<std::pair<PrimitiveType, PrimitiveType> const> from_to_types,
+      HloPredicate op_matcher, HloCloner cloner = nullptr)
+      : op_matcher_(op_matcher), cloner_(cloner) {
+    for (const std::pair<PrimitiveType, PrimitiveType>& pair : from_to_types) {
+      to_type_map_[pair.first] = pair.second;
+    }
+  }
+
+  ChangeMultiInputOpDataType(PrimitiveType from_ty, PrimitiveType to_ty,
+                             HloPredicate op_matcher,
+                             HloCloner cloner = nullptr)
+      : op_matcher_(op_matcher), cloner_(cloner) {
+    to_type_map_[from_ty] = to_ty;
+  }
+
+  absl::string_view name() const override {
+    return "change-multi-input-op-data-type";
+  }
+  absl::StatusOr<bool> Run(
+      HloModule* module,
+      const absl::flat_hash_set<absl::string_view>& execution_threads) override;
+
+ private:
+  // map with key = from_type and value = to_type.
+  absl::flat_hash_map<PrimitiveType, PrimitiveType> to_type_map_;
+  HloPredicate op_matcher_;
+  HloCloner cloner_;
+};
+
+// This pass rewrites scatter operations to promote 16 bit integer scatter
+// to 32-bit to avoid slow 16-bit atomic.
+//
+// Scatter invoke `EmitAtomicOperationUsingCAS` function for atomicCAS.
+// The function is in "xla/service/gpu/ir_emitter_nested.cc".
+// if the element type is smaller than 32 bits, XLA will use int32_t for the
+// atomicCAS operation. A 32-bit location holds two 16-bit data.
+// In this case, atomicCAS reads and writes 32 bit values from the memory.
+// When competition intensifies, it will be really slow.
+//
+// This pass lets us run the fp16/bf16 scatter as "convert to fp32,
+// run in fp32, then convert back to fp16/bf16".
+class ScatterPromotion : public HloModulePass {
+ public:
+  explicit ScatterPromotion(
+      absl::Span<std::pair<PrimitiveType, PrimitiveType> const> from_to_types);
+  absl::string_view name() const override { return "scatter-promotion"; }
+
+  using HloPassInterface::Run;
+  absl::StatusOr<bool> Run(
+      HloModule* module,
+      const absl::flat_hash_set<absl::string_view>& execution_threads) override;
+
+ private:
+  ChangeMultiInputOpDataType pass_;
+};
+
+}  // namespace xla
+
+#endif  // XLA_SERVICE_SCATTER_PROMOTION_H_
diff --git a/xla/stream_executor/blas.h b/xla/stream_executor/blas.h
index 9834e03197..fbcd5d7d62 100644
--- a/xla/stream_executor/blas.h
+++ b/xla/stream_executor/blas.h
@@ -139,6 +139,8 @@ constexpr AlgorithmType kDefaultBlasGemm = -2;
 constexpr AlgorithmType kDefaultBlasGemv = -3;
 constexpr AlgorithmType kNoAlgorithm = -4;
 constexpr AlgorithmType kRuntimeAutotuning = -5;
+constexpr AlgorithmType kXetlaGemm = -6;
+constexpr AlgorithmType kOneDnnGemm = -7;
 
 // blas uses -1 to represent the default algorithm. This happens to match up
 // with the CUBLAS_GEMM_DFALT constant, so cuda_blas.cc is using static_cast
diff --git a/xla/stream_executor/build_defs.bzl b/xla/stream_executor/build_defs.bzl
index a937a57edc..57544af795 100644
--- a/xla/stream_executor/build_defs.bzl
+++ b/xla/stream_executor/build_defs.bzl
@@ -1,7 +1,8 @@
 """Configurations for StreamExecutor builds"""
 
 load("@local_config_cuda//cuda:build_defs.bzl", "if_cuda_is_configured")
-load("@local_config_rocm//rocm:build_defs.bzl", _if_gpu_is_configured = "if_gpu_is_configured")
+load("@local_config_sycl//sycl:build_defs.bzl", "if_sycl_is_configured")
+load("@local_config_rocm//rocm:build_defs.bzl", "if_rocm_is_configured", _if_gpu_is_configured = "if_gpu_is_configured")
 load(
     "@tsl//tsl/platform:rules_cc.bzl",
     "cc_library",
@@ -19,12 +20,12 @@ def tf_additional_cuda_platform_deps():
 def tf_additional_cudnn_plugin_copts():
     return ["-DNV_CUDNN_DISABLE_EXCEPTION"]
 
-# Returns whether any GPU backend is configured.
+# Returns whether any GPU backend is configuered.
 def if_gpu_is_configured(if_true, if_false = []):
-    return _if_gpu_is_configured(if_true, if_false)
+    return if_sycl_is_configured(if_true, if_false)
 
 def if_cuda_or_rocm(x):
-    return if_gpu_is_configured(x)
+    return if_cuda_is_configured(x) + if_rocm_is_configured(x)
 
 # nvlink is not available via the pip wheels, disable it since it will create
 # unnecessary dependency
diff --git a/xla/stream_executor/cuda/cuda_driver.cc b/xla/stream_executor/cuda/cuda_driver.cc
index 9d9054f09a..f1281bd2b5 100644
--- a/xla/stream_executor/cuda/cuda_driver.cc
+++ b/xla/stream_executor/cuda/cuda_driver.cc
@@ -1410,6 +1410,14 @@ struct BitPatternToValue {
       "Feature not supported on CUDA platform (LoadHsaco)");
 }
 
+/* static */ absl::Status GpuDriver::LoadLevelzero(GpuContext* context,
+                                                  const char* spir_contents,
+                                                  const size_t size,
+                                                  CUmodule* module) {
+  return absl::InternalError(
+      "Feature not supported on CUDA platform (LoadLevelzero)");
+}
+
 /* static */ absl::Status GpuDriver::SynchronousMemsetUint8(
     GpuContext* context, CUdeviceptr location, uint8_t value, size_t size) {
   ScopedActivateContext activation(context);
diff --git a/xla/stream_executor/cuda/cuda_executor.cc b/xla/stream_executor/cuda/cuda_executor.cc
index d0b239cf06..bbe9b91d72 100644
--- a/xla/stream_executor/cuda/cuda_executor.cc
+++ b/xla/stream_executor/cuda/cuda_executor.cc
@@ -207,6 +207,12 @@ absl::Status GpuExecutor::LoadModuleFromHsaco(const char* hsaco,
       "Feature not supported on CUDA platform (LoadModuleFromHsaco)");
 }
 
+absl::Status GpuExecutor::LoadModuleFromSpir(const char* spir, const size_t size,
+                                             CUmodule* module) {
+  return absl::InternalError(
+      "Feature not supported on CUDA platform (LoadModuleFromSpir)");
+}
+
 absl::Status GpuExecutor::GetKernel(const MultiKernelLoaderSpec& spec,
                                     Kernel* kernel) {
   GpuKernel* cuda_kernel = AsGpuKernel(kernel);
diff --git a/xla/stream_executor/gpu/BUILD b/xla/stream_executor/gpu/BUILD
index b4fabb3050..73be6e20b0 100644
--- a/xla/stream_executor/gpu/BUILD
+++ b/xla/stream_executor/gpu/BUILD
@@ -10,6 +10,10 @@ load(
     "if_rocm",
     "if_rocm_is_configured",
 )
+load(
+    "@local_config_sycl//sycl:build_defs.bzl",
+    "if_sycl_is_configured",
+)
 load(
     "@tsl//tsl:tsl.bzl",
     "if_libtpu",
@@ -173,6 +177,8 @@ gpu_only_cc_library(
         "//xla/stream_executor/cuda:cuda_conditional_kernels",
     ]) + if_rocm_is_configured([
         "//xla/stream_executor/rocm:hip_conditional_kernels",
+    ]) + if_sycl_is_configured([
+        "@intel_extension_for_openxla//xla/stream_executor/sycl:sycl_conditional_kernels",
     ]),
 )
 
@@ -364,6 +370,8 @@ gpu_only_cc_library(
         "@local_config_cuda//cuda:cuda_headers",
     ]) + if_rocm_is_configured([
         "@local_config_rocm//rocm:rocm_headers",
+    ]) + if_sycl_is_configured([
+        "@local_config_sycl//sycl:sycl_headers",
     ]),
 )
 
@@ -602,6 +610,38 @@ tsl_gpu_library(
     ],
 )
 
+tsl_gpu_library(
+    name = "gpu_malloc_allocator_header",
+    hdrs = ["gpu_malloc_allocator.h"],
+    deps = [
+        "//xla/stream_executor:stream_executor_headers",
+        "@com_google_absl//absl/base:core_headers",
+        "@com_google_absl//absl/container:flat_hash_map",
+        "@tsl//tsl/framework:allocator",
+        "@tsl//tsl/framework:device_id",
+        "@tsl//tsl/platform:mutex",
+    ],
+)
+
+tsl_gpu_library(
+    name = "gpu_malloc_allocator",
+    srcs = [
+        "gpu_malloc_allocator.cc",
+    ],
+    hdrs = ["gpu_malloc_allocator.h"],
+    deps = [
+        ":gpu_init_impl",
+        "//xla/stream_executor:stream_executor_headers",
+        "@com_google_absl//absl/base:core_headers",
+        "@com_google_absl//absl/container:flat_hash_map",
+        "@com_google_absl//absl/strings",
+        "@tsl//tsl/framework:allocator",
+        "@tsl//tsl/framework:device_id",
+        "@tsl//tsl/platform:logging",
+        "@tsl//tsl/platform:mutex",
+    ],
+)
+
 cc_library(
     name = "gpu_blas_lt",
     srcs = ["gpu_blas_lt.cc"],
diff --git a/xla/stream_executor/gpu/gpu_driver.h b/xla/stream_executor/gpu/gpu_driver.h
index 91690f6964..a5c9f4e27b 100644
--- a/xla/stream_executor/gpu/gpu_driver.h
+++ b/xla/stream_executor/gpu/gpu_driver.h
@@ -628,6 +628,9 @@ class GpuDriver {
   // (supported on ROCm only)
   static absl::Status LoadHsaco(GpuContext* context, const char* hsaco_contents,
                                 GpuModuleHandle* module);
+  static absl::Status LoadLevelzero(GpuContext* context,
+                                    const char* spir_contents, const size_t size,
+                                    GpuModuleHandle* module);
 
   // Retrieves a named kernel from a loaded module, and places the resulting
   // handle into function (outparam) on success. Neither kernel_name nor
diff --git a/xla/stream_executor/gpu/gpu_executor.h b/xla/stream_executor/gpu/gpu_executor.h
index 3c67875754..00c16758fd 100644
--- a/xla/stream_executor/gpu/gpu_executor.h
+++ b/xla/stream_executor/gpu/gpu_executor.h
@@ -351,6 +351,11 @@ class GpuExecutor : public internal::StreamExecutorInterface {
   absl::Status LoadModuleFromHsaco(const char* hsaco, GpuModuleHandle* module)
       TF_EXCLUSIVE_LOCKS_REQUIRED(in_memory_modules_mu_);
 
+  // (supported on SYCL only)
+  absl::Status LoadModuleFromSpir(const char* spir, const size_t size,
+                                  GpuModuleHandle* module)
+      TF_EXCLUSIVE_LOCKS_REQUIRED(in_memory_modules_mu_);
+
   absl::Status Launch(Stream* stream, const ThreadDim& thread_dims,
                       const BlockDim& block_dims,
                       const std::optional<ClusterDim>& cluster_dims,
diff --git a/xla/stream_executor/gpu/gpu_helpers.h b/xla/stream_executor/gpu/gpu_helpers.h
index c86f49140a..5560795dd4 100644
--- a/xla/stream_executor/gpu/gpu_helpers.h
+++ b/xla/stream_executor/gpu/gpu_helpers.h
@@ -53,14 +53,18 @@ T* GpuMemoryMutable(DeviceMemory<T>* mem) {
 static_assert(
     sizeof(std::complex<float>) == sizeof(GpuComplexType),
     "std::complex<float> and GpuComplexType should have the same size");
+#if !TENSORFLOW_USE_SYCL
 static_assert(offsetof(GpuComplexType, x) == 0,
               "The real part of GpuComplexType should appear first.");
+#endif // !TENSORFLOW_USE_SYCL
 static_assert(
     sizeof(std::complex<double>) == sizeof(GpuDoubleComplexType),
     "std::complex<double> and GpuDoubleComplexType should have the same "
     "size");
+#if !TENSORFLOW_USE_SYCL
 static_assert(offsetof(GpuDoubleComplexType, x) == 0,
               "The real part of GpuDoubleComplexType should appear first.");
+#endif // !TENSORFLOW_USE_SYCL
 
 // Type traits to get CUDA complex types from std::complex<>.
 
diff --git a/xla/stream_executor/gpu/gpu_malloc_allocator.cc b/xla/stream_executor/gpu/gpu_malloc_allocator.cc
new file mode 100644
index 0000000000..fdd6d59612
--- /dev/null
+++ b/xla/stream_executor/gpu/gpu_malloc_allocator.cc
@@ -0,0 +1,154 @@
+/* Copyright (c) 2024 Intel Corporation
+
+Copyright 2021 The OpenXLA Authors.
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+    http://www.apache.org/licenses/LICENSE-2.0
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "xla/stream_executor/gpu/gpu_malloc_allocator.h"
+
+#include <atomic>
+#include <cstddef>
+#include <map>
+#include <memory>
+#include <optional>
+#include <string>
+#include <vector>
+
+#include "absl/strings/str_cat.h"
+#include "absl/strings/str_join.h"
+#include "xla/stream_executor/gpu/gpu_init.h"     // IWYU pragma: keep
+#include "xla/stream_executor/stream_executor.h"  // IWYU pragma: keep
+#include "tsl/framework/allocator.h"
+#include "tsl/framework/device_id.h"
+#include "tsl/platform/logging.h"
+#include "tsl/platform/mutex.h"
+
+namespace stream_executor {
+
+std::atomic<int> GpuMallocAllocator::number_instantiated_(0);
+
+GpuMallocAllocator::GpuMallocAllocator(StreamExecutor* executor,
+                                       tsl::PlatformDeviceId platform_device_id,
+                                       bool compute_stats)
+    : stream_exec_(executor),
+      name_(absl::StrCat("gpu", platform_device_id.value())) {
+  ++number_instantiated_;
+
+  if (compute_stats) {
+    stats_ = std::make_unique<tsl::AllocatorStats>();
+  }  // If not set, it means we do not compute stats.
+
+  VLOG(1) << Name() << " GPUMallocAsync initialized on platform: "
+          << platform_device_id.value();
+}
+
+GpuMallocAllocator::~GpuMallocAllocator() {}
+
+void* GpuMallocAllocator::AllocateRaw(size_t alignment, size_t num_bytes) {
+  CHECK(stream_exec_ != nullptr)
+      << "A stream executor must be added to the default gpu allocator";
+  // The lock is only needed when stats are enabled, but it must be around
+  // the cuMemAllocFromPoolAsync call as well to ensure consistency of the stats
+  // update.
+  std::unique_lock<tsl::mutex> lock(lock_, std::defer_lock);
+  if (stats_) {
+    lock.lock();
+  }
+  int64_t count = num_bytes / sizeof(uint8_t);
+  void* ptr = (stream_exec_->AllocateArray<uint8_t>(count, 0)).opaque();
+
+  if (ptr == nullptr) {
+    LOG(ERROR) << Name() << " failed to allocate " << num_bytes << " bytes: ";
+    if (stats_) {
+      LOG(ERROR) << "Stats: " << stats_->DebugString();
+    }
+
+    return nullptr;
+  }
+
+  // Update stats.
+  if (stats_) {
+    ++(stats_->num_allocs);
+    stats_->bytes_in_use += num_bytes;
+    if (stats_->bytes_in_use > stats_->peak_bytes_in_use) {
+      VLOG(9) << "New Peak memory usage of " << stats_->bytes_in_use
+              << " bytes.";
+    }
+    stats_->peak_bytes_in_use =
+        std::max(stats_->peak_bytes_in_use, stats_->bytes_in_use);
+    stats_->largest_alloc_size =
+        std::max<std::size_t>(stats_->largest_alloc_size, num_bytes);
+    bool ptr_inserted = size_map_.emplace(ptr, num_bytes).second;
+    DCHECK(ptr_inserted);
+  }
+  VLOG(1) << Name() << " Allocated " << num_bytes << " at " << ptr;
+  return ptr;
+}
+
+void GpuMallocAllocator::DeallocateRaw(void* ptr) {
+  if (ptr == nullptr) return;
+  // The lock is only needed when stats are enabled, but it must be around
+  // the cuMemFreeAsync call as well to ensure consistency of the stats update.
+  std::unique_lock<tsl::mutex> lock(lock_, std::defer_lock);
+  if (stats_) {
+    lock.lock();
+  }
+  auto device_memory_base = DeviceMemoryBase(ptr);
+  stream_exec_->Deallocate(&device_memory_base);
+
+  // Updates the stats.
+  if (stats_) {
+    DCHECK(size_map_.contains(ptr));
+    size_t size = size_map_[ptr];
+    stats_->bytes_in_use -= size;
+    size_map_.erase(ptr);
+  }
+
+  VLOG(1) << Name() << " Freed ptr: " << ptr;
+}
+
+bool GpuMallocAllocator::TracksAllocationSizes() const {
+  return static_cast<bool>(stats_);
+}
+
+size_t GpuMallocAllocator::RequestedSize(const void* ptr) const {
+  if (!stats_ || !ptr) return 0;
+  tsl::mutex_lock l(lock_);
+  return size_map_.at(ptr);
+}
+
+size_t GpuMallocAllocator::AllocatedSize(const void* ptr) const {
+  if (!stats_ || !ptr) return 0;
+  tsl::mutex_lock l(lock_);
+  return size_map_.at(ptr);
+}
+
+std::optional<tsl::AllocatorStats> GpuMallocAllocator::GetStats() {
+  if (!stats_) return std::nullopt;
+  tsl::mutex_lock l(lock_);
+  return *stats_;
+}
+
+bool GpuMallocAllocator::ClearStats() {
+  if (!stats_) return false;
+  tsl::mutex_lock l(lock_);
+  stats_->num_allocs = 0;
+  stats_->peak_bytes_in_use = stats_->bytes_in_use;
+  stats_->largest_alloc_size = 0;
+  return true;
+}
+
+void GpuMallocAllocator::SetStreamAndPreallocateMemory(void* stream) {
+  LOG(FATAL) <<  // Crash OK.
+      "Unimplemented. ";
+}
+
+}  // namespace stream_executor
diff --git a/xla/stream_executor/gpu/gpu_malloc_allocator.h b/xla/stream_executor/gpu/gpu_malloc_allocator.h
new file mode 100644
index 0000000000..ce441527d5
--- /dev/null
+++ b/xla/stream_executor/gpu/gpu_malloc_allocator.h
@@ -0,0 +1,83 @@
+/* Copyright (c) 2024 Intel Corporation
+
+Copyright 2021 The OpenXLA Authors.
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+    http://www.apache.org/licenses/LICENSE-2.0
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef XLA_STREAM_EXECUTOR_GPU_GPU_MALLOC_ALLOCATOR_H_
+#define XLA_STREAM_EXECUTOR_GPU_GPU_MALLOC_ALLOCATOR_H_
+
+#include <atomic>
+#include <cstddef>
+#include <memory>
+#include <optional>
+#include <string>
+
+#include "absl/base/thread_annotations.h"
+#include "absl/container/flat_hash_map.h"
+#include "xla/stream_executor/stream_executor.h"  // IWYU pragma: keep
+#include "tsl/framework/allocator.h"
+#include "tsl/framework/device_id.h"
+#include "tsl/platform/mutex.h"
+
+namespace stream_executor {
+
+class GpuMallocAllocator : public tsl::Allocator {
+ public:
+  explicit GpuMallocAllocator(StreamExecutor* executor,
+                              tsl::PlatformDeviceId platform_device_id,
+                              bool compute_stats = true);
+  ~GpuMallocAllocator() override;
+  std::string Name() override { return name_; }
+  void* AllocateRaw(size_t alignment,
+                    size_t num_bytes) override ABSL_NO_THREAD_SAFETY_ANALYSIS;
+  void DeallocateRaw(void* ptr) override ABSL_NO_THREAD_SAFETY_ANALYSIS;
+
+  bool TracksAllocationSizes() const override;
+
+  size_t RequestedSize(const void* ptr) const override;
+
+  size_t AllocatedSize(const void* ptr) const override;
+
+  std::optional<tsl::AllocatorStats> GetStats() override;
+
+  bool ClearStats() override;
+
+  void SetStreamAndPreallocateMemory(void* stream) override;
+
+  static int GetInstantiatedCountTestOnly() { return number_instantiated_; }
+
+  tsl::AllocatorMemoryType GetMemoryType() const override {
+    return tsl::AllocatorMemoryType::kDevice;
+  }
+
+ private:
+  StreamExecutor* stream_exec_;
+
+  // Just a counter for the number of time this class is instantiated.
+  // Only useful for tests.
+  static std::atomic<int> number_instantiated_;
+
+  std::string name_;
+
+  GpuMallocAllocator(const GpuMallocAllocator&) = delete;
+  void operator=(const GpuMallocAllocator&) = delete;
+
+  // Stats.
+  // Structures mutable after construction
+  mutable tsl::mutex lock_;
+  std::unique_ptr<tsl::AllocatorStats> stats_ ABSL_PT_GUARDED_BY(lock_);
+  absl::flat_hash_map<const void*, size_t> size_map_ ABSL_GUARDED_BY(lock_);
+};
+
+}  // namespace stream_executor
+
+#endif  // XLA_STREAM_EXECUTOR_GPU_GPU_MALLOC_ALLOCATOR_H_
diff --git a/xla/stream_executor/gpu/gpu_types.h b/xla/stream_executor/gpu/gpu_types.h
index c8d6266b35..a6b4afa2bd 100644
--- a/xla/stream_executor/gpu/gpu_types.h
+++ b/xla/stream_executor/gpu/gpu_types.h
@@ -18,7 +18,18 @@ limitations under the License.
 #ifndef XLA_STREAM_EXECUTOR_GPU_GPU_TYPES_H_
 #define XLA_STREAM_EXECUTOR_GPU_GPU_TYPES_H_
 
-#if TENSORFLOW_USE_ROCM
+#if TENSORFLOW_USE_SYCL
+
+#if __has_include(<sycl/sycl.hpp>)
+#include <sycl/sycl.hpp>
+#elif __has_include(<CL/sycl.hpp>)
+#include <CL/sycl.hpp>
+#else
+#error "Unsupported compiler"
+#endif
+#include <complex>
+
+#elif TENSORFLOW_USE_ROCM
 
 #define __HIP_DISABLE_CPP_FUNCTIONS__
 
@@ -40,7 +51,34 @@ namespace gpu {
 // current CUDA/HIP version.
 struct UnsupportedGpuFeature {};
 
-#if TENSORFLOW_USE_ROCM
+#if TENSORFLOW_USE_SYCL
+typedef struct SYCLEventWrapper {
+  ::sycl::event* event;
+  ::sycl::queue* queue;
+} EventWrapper;
+
+using GpuContextHandle = const void*;
+using GpuStreamHandle = ::sycl::queue*;
+using GpuEventHandle = EventWrapper*;
+using GpuFunctionHandle = ::sycl::kernel*;
+using GpuFunctionAttribute = const void*;
+using GpuDeviceHandle = ::sycl::device*;
+using GpuDevicePtr = void*;
+using GpuDeviceAttribute = const void*;
+using GpuDeviceProperty = const void*;
+using GpuModuleHandle = ze_module_handle_t;
+using GpuStatus = const void*;
+using GpuFuncCachePreference = const void*;
+using GpuSharedMemConfig = const void*;
+using GpuComplexType = std::complex<float>;
+using GpuDoubleComplexType = std::complex<double>;
+using GpuRngHandle = const void*;
+using GpuGraphHandle = const void*;
+using GpuGraphExecHandle = const void*;
+using GpuGraphNodeHandle = const void*;
+using GpuGraphConditionalHandle = UnsupportedGpuFeature;
+
+#elif TENSORFLOW_USE_ROCM
 
 using GpuContextHandle = hipCtx_t;
 using GpuStreamHandle = hipStream_t;
diff --git a/xla/stream_executor/gpu/redzone_allocator.cc b/xla/stream_executor/gpu/redzone_allocator.cc
index 5f08f84c2d..6ea38951d1 100644
--- a/xla/stream_executor/gpu/redzone_allocator.cc
+++ b/xla/stream_executor/gpu/redzone_allocator.cc
@@ -317,6 +317,7 @@ static absl::StatusOr<RedzoneCheckStatus> CheckRedzonesForBuffer(
   return RedzoneCheckStatus::OK();
 }
 
+#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM
 absl::StatusOr<RedzoneCheckStatus> RedzoneAllocator::CheckRedzones() const {
   StreamExecutor* executor = stream_->parent();
 
@@ -368,6 +369,7 @@ absl::StatusOr<RedzoneCheckStatus> RedzoneAllocator::CheckRedzones() const {
 
   return RedzoneCheckStatus::OK();
 }
+#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM
 
 std::string RedzoneCheckStatus::RedzoneFailureMsg() const {
   return absl::StrFormat(
diff --git a/xla/stream_executor/kernel_spec.h b/xla/stream_executor/kernel_spec.h
index d50ac23713..ec67384ba6 100644
--- a/xla/stream_executor/kernel_spec.h
+++ b/xla/stream_executor/kernel_spec.h
@@ -167,9 +167,12 @@ class CudaCubinInMemory : public KernelLoaderSpec {
                     absl::string_view kernel_name);
 
   absl::Span<const uint8_t> cubin_bytes() const { return cubin_bytes_; }
+  const int size() const { return size_; }
 
  private:
   absl::Span<const uint8_t> cubin_bytes_;
+  // SYCL: this is needed only for SPIRV
+  int size_;
 
   CudaCubinInMemory(const CudaCubinInMemory &) = delete;
   void operator=(const CudaCubinInMemory &) = delete;
diff --git a/xla/stream_executor/rocm/rocm_driver.cc b/xla/stream_executor/rocm/rocm_driver.cc
index 409546aafd..10b199a515 100644
--- a/xla/stream_executor/rocm/rocm_driver.cc
+++ b/xla/stream_executor/rocm/rocm_driver.cc
@@ -1163,6 +1163,14 @@ struct BitPatternToValue {
   return ret;
 }
 
+/* static */ absl::Status GpuDriver::LoadLevelzero(GpuContext* context,
+                                                  const char* spir_contents,
+                                                  const size_t size,
+                                                  hipModule_t* module) {
+  return absl::InternalError(
+      "Feature not supported on ROCm platform (LoadLevelzero)");
+}
+
 /* static */ absl::Status GpuDriver::SynchronousMemsetUint8(
     GpuContext* context, hipDeviceptr_t location, uint8 value, size_t size) {
   ScopedActivateContext activation{context};
diff --git a/xla/stream_executor/rocm/rocm_executor.cc b/xla/stream_executor/rocm/rocm_executor.cc
index 722f2c800a..0dca7a6801 100644
--- a/xla/stream_executor/rocm/rocm_executor.cc
+++ b/xla/stream_executor/rocm/rocm_executor.cc
@@ -461,6 +461,11 @@ absl::Status GpuExecutor::LoadModuleFromHsaco(const char* hsaco,
   return absl::OkStatus();
 }
 
+tsl::Status GpuExecutor::LoadModuleFromSpir(const char* spir, const size_t size,
+                                            hipModule_t* module) {
+  LOG(FATAL) << "Feature not supported on ROCM platform (LoadModuleFromSpir)";
+}
+
 // This is a non-essential operation; if there's a failure, proceed without
 // logging an error. It's nearly certain that in case of failures, we'd never
 // get here in the first place; these are very low-impact routines.
